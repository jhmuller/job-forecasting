{
  
    
        "post0": {
            "title": "Part 4 - New data on Weekly Claims",
            "content": "I was scanning a WSJ article today on jobless claims, https://www.wsj.com/articles/weekly-jobless-claims-coronavirus-10-29-2020-11603921724?modtag=djemBestOfTheWeb The article started by claiming that initial claims filings had fallen to their lowest level since the pandemic began. This didn&#39;t seem interesting because according to FRED you could have been saying that since May. Seems like saying I am as old as I&#39;ve ever been. But then I noticed a chart showing claims under Regular state programs plus claims under special pandemic programs ... and for continued claims the special pandemic numbers were about the same as the regular ones. Uh oh, so the claims could be 2x what I would get from FRED and the normal BLS data. I could not find any data on these new plans on FRED but did find a spreadsheet on the Department of Labor website, see https://oui.doleta.gov/unemploy/DataDashboard.asp . The data might be available on the DOL or BLS site via some API. Not sure yet. In any case, in this post I will read in that data and plot it. Later I will figure out how to integreate it with the rest. . import os import sys import inspect import datetime import pandas as pd import numpy as np import time from plotnine import ggplot import matplotlib as mpl import xlrd import matplotlib.pyplot as plt import selenium from selenium import webdriver from selenium.webdriver.support.ui import Select import pathlib import shutil . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd version 1.19.3 of np version 3.3.3 of mpl version 1.2.0 of xlrd version 3.141.0 of selenium version 3.14.1 of webdriver . Selenium . Setup temp directory for download . temp_dir = &quot;./data/temp&quot; if not os.path.isdir(temp_dir): os.mkdir(temp_dir) prefs = {&quot;download.default_directory&quot; : os.path.abspath(temp_dir)} options = webdriver.ChromeOptions() options.add_experimental_option(&quot;prefs&quot;,prefs) options.add_argument(&quot;download.default_directory=&quot;+os.path.abspath(temp_dir)) . chromedriver_path = os.path.join(&#39;chromedriver.exe&#39;) driver = webdriver.Chrome(executable_path=chromedriver_path, options=options) url = &quot;https://oui.doleta.gov/unemploy/DataDashboard.asp&quot; driver.get(url) . def get_downloaded_fpath(data_dir=None, files_before=None, file_ext=&quot;.csv&quot;, max_wait = 10, verbosity=0): import time done = False start_time = datetime.datetime.now() while not done: files_after = set(os.listdir(data_dir)) new_files = files_after.difference(set(files_before)) if verbosity &gt; 0: print(&quot;Files Before: {0}&quot;.format(files_before)) print(&quot;Files After: {0}&quot;.format(files_after)) print(&quot;New Files: {0}&quot;.format(new_files)) if verbosity &gt; 0: print(new_files) for fname in new_files: if os.path.splitext(fname)[1] == file_ext: return (os.path.join(data_dir, fname)) cur_time = datetime.datetime.now() if (cur_time - start_time).seconds &gt; max_wait: return None time.sleep(0.5) . files_before = set(os.listdir(temp_dir)) time.sleep(1) el = driver.find_element_by_link_text(&quot;NEW: Weekly Pandemic Claims Data&quot;) el.click() downloaded_fpath = get_downloaded_fpath(data_dir=temp_dir, files_before=files_before, file_ext=&quot;.xlsx&quot;, max_wait=3, verbosity=0) if not downloaded_fpath: raise Exception(&quot;No downloaded file found!&quot;) print(&quot;new file: {0}&quot;.format(downloaded_fpath)) def clean_filename(fpath): import re head, tail = os.path.split(downloaded_fpath) base, ext = os.path.splitext(tail) pattern = &quot;(.*)( ( d* ))&quot; match = re.match(pattern, base) if match: base = match.group(1).strip() new_fname = base + ext return new_fname temp_df = pd.read_excel(downloaded_fpath) data_dir = &quot;./data&quot; new_fname = clean_filename(downloaded_fpath) new_fpath = os.path.join(data_dir, new_fname) temp_df.to_csv(new_fpath, index=False) . new file: ./data/temp weekly_pandemic_claims.xlsx . import re head, tail = os.path.split(downloaded_fpath) base, ext = os.path.splitext(tail) pattern = &quot;(.*)( ( d* ))&quot; match = re.match(pattern, base) if match: base = match.group(1).strip() new_fname = base + ext . Clean up . if os.path.isfile(new_fpath): # stop driver and remove old directory driver.quit() shutil.rmtree(temp_dir) . Read . Read in the spreadsheet and get rid of rows with no data for Rptdate . data_dir = &quot;./data&quot; new_fpath = os.path.join(data_dir, new_fname) states_df = pd.read_csv(new_fpath) states_df = states_df[states_df[&#39;Rptdate&#39;].notna()] states_df.rename(columns={&quot;Rptdate&quot;:&quot;date&quot;}, inplace=True) states_df.head(3) . State date PUA IC Reflect Date PUA CC PEUC CC . 0 State | Rptdate | PUA IC | Reflect Date | PUA CC | PEUC CC | . 1 AL | 2020-04-04 00:00:00 | 0 | 2020-03-28 00:00:00 | 0 | 0 | . 2 AK | 2020-04-04 00:00:00 | 0 | 2020-03-28 00:00:00 | 0 | 0 | . print(states_df.dtypes) data_cols = [c for c in states_df.columns if re.search(&quot;CC|IC&quot;, c)] ocols = [] for col in data_cols: if states_df[col].dtype == &#39;object&#39;: ocols.append(col) if not ocols: print(f&quot;all the data cols [{data_cols}] are numeric&quot;) else: # get rid of non numeric rows numeric_rows = states_df[data_cols].apply(lambda x: pd.Series(x).str.isnumeric(), axis=0).all(axis=1) print(f&quot;found {0} non numeric rows&quot;.format(states_df.shape[0]-numeric_rows)) states_df = states_df.loc[numeric_rows] print(f&quot;{ocols} are object, should be numeric&quot;) for col in ocols: states_df[col] = states_df[col].astype(float) print(states_df.dtypes) . State object date object PUA IC object Reflect Date object PUA CC object PEUC CC object dtype: object found 0 non numeric rows [&#39;PUA IC&#39;, &#39;PUA CC&#39;, &#39;PEUC CC&#39;] are object, should be numeric State object date object PUA IC float64 Reflect Date object PUA CC float64 PEUC CC float64 dtype: object . Plot . For now I&#39;m not interested in the state breakdown so I&#39;ll aggregate to US level and also separte out Initial Claims, IC, from Continued Claims, CC. Partly because the CC values are about 10x the IC values so easier to see and check the IC data if plotted alone. Also because there is data on 2 different special programs for CC. . us_df = states_df.drop(labels=[&quot;State&quot;], axis=1).groupby(by=&#39;date&#39;) us_df = us_df.sum(min_count=1) us_df.head() #us_df.reset_index(inplace=True) . PUA IC PUA CC PEUC CC . date . 2020-04-04 00:00:00 31949.0 | 52494.0 | 0.0 | . 2020-04-11 00:00:00 78726.0 | 69537.0 | 3802.0 | . 2020-04-18 00:00:00 218399.0 | 216481.0 | 31426.0 | . 2020-04-25 00:00:00 681487.0 | 1172238.0 | 63720.0 | . 2020-05-02 00:00:00 1048982.0 | 3629986.0 | 91724.0 | . us_df = states_df.drop(labels=[&quot;State&quot;], axis=1).groupby(by=&#39;date&#39;) us_df = us_df.sum(min_count=1) us_df.reset_index(inplace=True) print(us_df.tail()) us_df.to_csv(os.path.join(&quot;./data&quot;, &quot;us_pau_claims.csv&quot;), index=False) . date PUA IC PUA CC PEUC CC 28 2020-10-17 00:00:00 345440.0 10622725.0 3711089.0 29 2020-10-24 00:00:00 359044.0 9332610.0 3983613.0 30 2020-10-31 00:00:00 361959.0 9433127.0 4143389.0 31 2020-11-07 00:00:00 296374.0 8681647.0 4376847.0 32 2020-11-14 00:00:00 320237.0 NaN NaN . ic_df = us_df[[&quot;date&quot;,&#39;PUA IC&#39;]] cc_df = us_df.drop(&#39;PUA IC&#39;, axis=1) cc_df[&#39;CC Tot&#39;] = cc_df[[&quot;PUA CC&quot;,&quot;PEUC CC&quot;]].sum(axis=1, skipna=False) cc_df.tail() dfs = {&quot;IC&quot;:ic_df, &quot;CC&quot;:cc_df} . dfs[&quot;IC&quot;].head() . date PUA IC . 0 2020-04-04 00:00:00 | 31949.0 | . 1 2020-04-11 00:00:00 | 78726.0 | . 2 2020-04-18 00:00:00 | 218399.0 | . 3 2020-04-25 00:00:00 | 681487.0 | . 4 2020-05-02 00:00:00 | 1048982.0 | . figwd = 12 fight= 6 fig, axs = plt.subplots(nrows=2, ncols=1, figsize=[figwd, fight], sharex=True) for i, key in enumerate(dfs.keys()): print(f&quot;Plotting {key}&quot;) ax = axs[i] dfs[key].plot(ax=ax, x=&quot;date&quot;, kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, title=key, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, legend=True, label=None, grid=True) . Plotting IC Plotting CC . The regular IC values have been about 750K so this new program is about half that. The regular CC values have been about 7.5 million so this new data is about the same. Later I might combine the regular and special data, but for now it&#39;s enough just to have the special data. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/weekly_claims.html",
            "relUrl": "/jupyter/2021/03/21/weekly_claims.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Part 5 - University of Michigan Consumer Sentiment data",
            "content": "I think they do a great job of documentation at the site. . Also have a look at the siki page about the survey and the data. In particular the references to this and indirectly this about Thomson Reuters and others giving investors early access for a fee. . In any case, you can see the question asking sheet and more. I am interested in the responses about expectations. There seems like a lot of great information there but I am going to focus on the data on the components of the index and in particular the 3 questions related to expectations . x2= PEXP_R = &quot;Now looking ahead--do you think that a year from now you (and your family living there) will be better off financially, or worse off, or just about the same as now?&quot; | x3= BUS12_R = &quot;Now turning to business conditions in the country as a whole--do you think that during the next twelve months we&#39;ll have good times financially, or bad times, or what?&quot; | x4= BUS5_R = &quot;Looking ahead, which would you say is more likely--that in the country as a whole we&#39;ll have continuous good times during the next five years or so, or that we will have periods of widespread unemployment or depression, or what?&quot; | . They compute a weighted average called the Index of Consumer Expectations, or ICE, as follows . ICE = ((x2 + x3 + x4)/4.1134) + 2 [1](#myfootnote1) . I&#39;m sure somewhere in the documentation they explain why they divide by 4.1134 and add 2 but I&#39;ll probably just use the 3 individual variables. We&#39;ll see. . 1: Notice I put in the parentesis to avoid issues like this. If the link doesn&#39;t work search for &quot;The Math equation that stumped the internet&quot;) . First the bolerplate Python I use for most notebooks. It&#39;s evolving. . import os import sys import datetime import time import re import inspect import pandas as pd from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt import selenium from selenium import webdriver from selenium.webdriver.support.ui import Select import pathlib . I like to see version numbers for modules ... and for Python. . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 2.2.1 of re version 1.1.3 of pd version 3.3.3 of mpl version 3.141.0 of selenium version 3.14.1 of webdriver . Selenium to automate downloading . It&#39;s easy to get the data from the site, can be done via the following steps. . Navigate to https://data.sca.isr.umich.edu/data-archive/mine.php [2](#myfootnote1) | For Table select Table 5: Components of the Index of Consumer Sentiment. | Click Comma-Separated(CSV) under format and it should start downloading. | But I want to automate the process and we will using Selenium. Selenium allows us to control a web browser from Pyhon so I can use Python to execute all the steps above. . 2: Note that if you start at the main site you might need to click on Data and select Time Series in the dropdown. . Download Directory . I want to have the data downloaded into a subdirectory below here rather than the default Downloads directory. We can arrange that using ChromeOptions. . temp_dir = &quot;./data/temp&quot; if not os.path.isdir(temp_dir): os.mkdir(temp_dir) prefs = {&quot;download.default_directory&quot; : os.path.abspath(temp_dir)} options = webdriver.ChromeOptions() options.add_experimental_option(&quot;prefs&quot;,prefs) options.add_argument(&quot;download.default_directory=&quot;+os.path.abspath(temp_dir)) . Step 1, going to the site . You can run Selenium in what is called headless mode where you don&#39;t get a window for the browser. But I still find it cool to see a new browser pop up so I won&#39;t use that now. And it&#39;s nice in the early stages to see what is actually happening. Probably will change once the novelty wears off. . chromedriver . With Selenium you have a choice of which browser to use, e.g. Chrome, Firefox et cetera. I&#39;ll use Chrome. Whatever the choice we&#39;ll need a driver. Google it if you are interested. I downloaded the chromedriver and put it in a sibling directory under chromedriver_win32 so it is easy to find for others projects using Selenium. . Here I am doing step 1 from above. . chromedriver_path = os.path.join(&#39;chromedriver.exe&#39;) driver = webdriver.Chrome(executable_path=chromedriver_path, options=options) url = &quot;https://data.sca.isr.umich.edu/data-archive/mine.php&quot; driver.get(url) . Step 2: selecting Table 5 . There are plenty of tutorials on using Selenium to make selections and click buttons. I can&#39;t remember which ones I used. Most likely I just googled what I wanted to do. . In any case, here I am finding the selection section on the page and choosing option 5. . select_element = Select(driver.find_element_by_css_selector(&quot;select&quot;)) # this will print out strings available for selection on select_element, used in visible text below select_element.select_by_index(5) . Identifying the newly downloaded file . In the final step I will download a file with the data into the ./data directory. But I don&#39;t know exactly what the file name will be yet, and also if I pull it down multiple times the file will get names lik &quot;file.csv&quot;, &quot;file(2).csv&quot; et cetera. I want to make sure I can idendify the the latest one I downloaded. I wrote the function below to help with that. It is pobably way more complicated than it needs to be. . In any case, the idea is to start with a list of all the files in the directory before I asked for the download. Then, I look for new files with the appropriate extension, in this case &quot;.csv&quot;. Once I find a file that was not there before, that is the one that just got donloaded. In case the file takes some time to download I will loop looking for a new file, but I have a limit on how long I will do this looping, My default wait limit is 10 seconds. . def get_downloaded_fpath(dir=None, files_before=None, file_ext=&quot;.csv&quot;, max_wait = 10, verbose=True): import time done = False start_time = datetime.datetime.now() while not done: files_after = set(os.listdir(dir)) new_files = files_after.difference(set(files_before)) print(new_files) for fname in new_files: if os.path.splitext(fname)[1] == file_ext: return (os.path.join(dir, fname)) cur_time = datetime.datetime.now() if (cur_time - start_time).seconds &gt; 10: return None time.sleep(0.5) . Step 3: Selecting Comma-Separated(CSV) . Now I look for the &quot;Comma-Separated(CSV)&quot; button on the web page and click it. Then I use the above function to find the path to the new file. . elements = driver.find_elements_by_name(&quot;format&quot;) button = None for e in elements: if e.get_property(&quot;value&quot;) == &#39;Comma-Separated (CSV)&#39;: button = e break if not button: raise RuntimeError(&quot;Error downloading Consumer Sentiment data from {0}&quot;.format(url)) files_before = set(os.listdir(temp_dir)) button.click() fpath = get_downloaded_fpath(dir=temp_dir, files_before=files_before, file_ext=&quot;.csv&quot;, max_wait=10) if not fpath: raise Exception(&quot;No downloaded file found!&quot;) print(&quot;new file: {0}&quot;.format(fpath)) . set() set() {&#39;6cdd26b7-050f-47a5-b417-90b22b5c0fe5.tmp&#39;} {&#39;sca-table5-on-2020-Nov-12.csv&#39;} new file: ./data/temp sca-table5-on-2020-Nov-12.csv . Shotdown the browser . driver.quit() . Read in the new file | print the volumns | Drop the last column, don&#39;t know what it is | Make a datetime column from the month and year and drop the month and year columns. | make the datetime column the index | pick out th expectation columns as the ones that have either &quot;expected&quot; or &quot;Business Condition&quot; in the name. drop all but the expected columns | . | . df = pd.read_csv(fpath, skiprows=1) print(df.columns) df.drop(df.columns[-1], inplace=True, axis=1) df[&#39;Datetime&#39;] = (100*100*df[&#39;Year&#39;] + 100*df[&quot;Month&quot;] + 15).astype(str) df[&#39;Datetime&#39;] = pd.to_datetime(df[&#39;Datetime&#39;]) df.drop([&quot;Month&quot;, &quot;Year&quot;], inplace=True, axis=1) df.set_index(&quot;Datetime&quot;, inplace=True) exp_columns = [c for c in df.columns if re.search(&quot;Expected|Business Condition&quot;, c)] df = df[exp_columns] df.head() . Index([&#39;Month&#39;, &#39;Year&#39;, &#39;Personal Finance Current&#39;, &#39;Personal Finance Expected&#39;, &#39;Business Condition 12 Months&#39;, &#39;Business Condition 5 Years&#39;, &#39;Buying Conditions&#39;, &#39;Current Index&#39;, &#39;Expected Index&#39;, &#39;Unnamed: 9&#39;], dtype=&#39;object&#39;) . Personal Finance Expected Business Condition 12 Months Business Condition 5 Years Expected Index . Datetime . 2008-01-15 116 | 68 | 88 | 68.1 | . 2008-02-15 112 | 54 | 83 | 62.4 | . 2008-03-15 112 | 46 | 81 | 60.1 | . 2008-04-15 100 | 40 | 71 | 53.3 | . 2008-05-15 98 | 36 | 68 | 51.1 | . Plot all the expected series . df.plot(figsize=[16,4], marker=&#39;.&#39;, grid=True) plt.gca().set_ylim(bottom=0) . (0.0, 142.3) . Save the dataframe as &quot;umich_exp.csv&quot; . try: out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) df.to_csv(os.path.join(out_dir, &quot;umich_exp.csv&quot;)) except Exception as exc: print(exc) raise Exception(exc) . delete all the files in the temp directory | delete the temp directory | . try: if os.path.isdir(temp_dir): temp_files = os.listdir(temp_dir) # remove all the temp files for tfile in temp_files: os.remove(os.path.join(temp_dir, tfile)) # remove the temp_dir if os.path.isdir(temp_dir): pathlib.Path.rmdir(pathlib.Path(temp_dir)) except Exception as exc: print(exc) raise Exception(exc) . import datetime print(datetime.datetime.now()) . 2020-11-12 07:08:31.238145 .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/umich_cs.html",
            "relUrl": "/jupyter/2021/03/21/umich_cs.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Potential issues - in recreating my environment",
            "content": "import os import sys import inspect import pandas as pd . numpy issue . version 1.19.4 seems to create problems uninstall 1.19.4 and install version 1.19.3 . zmq issue . fatal error C1083: Cannot open include file: &#39;zmq.h&#39;: No such file or directory . uninstall and reinstall pyzmq . pip uninstall pyzmq pip install pyzmq . ssl issue . If pip complains about ssl, eg. WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available. . following this . Copy the following files . libcrypto-1_1-x64.dll | libcrypto-1_1-x64.pdb | libssl-1_1-x64.dll | libssl-1_1-x64.pdb | . From : C: Users MyUser Miniconda3 Library bin To: C: Users MyUser Miniconda3 DLLs . conda create -n jobs python=3.8 . pip install ipykernel . python -m ipykernel install --user --name=jobs . pip install pandas pytrends . mods = !pip freeze ptmods = [m for m in mods if m.startswith(&#39;pytrends&#39;)] if ptmods : pt = ptmods[0] ptname, ptversion = pt.split(&quot;==&quot;) print(ptname) else: print(&quot;no pytrends&quot;) . pytrends . os.environ[&quot;CONDA_DEFAULT_ENV&quot;] . &#39;jobs&#39; . stmt = f&quot;import {ptname}&quot; print(stmt) exec(stmt, globals(), locals()) . import pytrends . import os import pandas as pd import pytz import pytrends mpath = pytrends.__file__ mpath = pd.__file__ site_packages_path = os.path.split(os.path.split(mpath)[0])[0] print(site_packages_path) sps = os.listdir(site_packages_path) #sps . C: Users jmull anaconda3 lib site-packages . [k for k in os.environ.keys() if k.startswith(&quot;COND&quot;)] . [&#39;CONDA_DEFAULT_ENV&#39;, &#39;CONDA_EXE&#39;, &#39;CONDA_PREFIX&#39;, &#39;CONDA_PREFIX_1&#39;, &#39;CONDA_PROMPT_MODIFIER&#39;, &#39;CONDA_PYTHON_EXE&#39;, &#39;CONDA_SHLVL&#39;] .",
            "url": "https://jhmuller.github.io/job-forecasting/2021/03/21/setup_issues.html",
            "relUrl": "/2021/03/21/setup_issues.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Part 6 - Google Trends data",
            "content": "In this notebook I will describe my code to download data from Google trends as well as code to pull keyword suggestions&quot; based on some seed* keywords. . I start in the usual way I do with all my notebooks with . imports | print version for Python and non standar modules | . This boilerplate code is evolving and hopefully getting better . import os import sys import datetime import inspect import pandas as pd import plotnine as pn import pytrends as pt from pytrends.request import TrendReq . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: mname = name if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): mname = name if hasattr(mod, &quot;__path__&quot;): mname = os.path.split(mod.__path__[0])[1] print(&quot;version {1} of {0} as {2} &quot;.format(mname, name, mod.__version__)) elif hasattr(mod, &quot;__file__&quot;) and &quot;site-packages&quot; in mod.__file__: print(&quot;No __version__ for {0} as {1}&quot;.format(mname, name)) del mod del name . version 3.8.5 of Python version pd of pandas as 1.1.3 version pn of plotnine as 0.7.1 No __version__ for pt as pt . Pytrends api . I&#39;m new to pytrends but I&#39;ll make a go of using it. . Seems like the steps are . create a TrendReq object which is usually called pytrends. | configure the TrendReq object, called pytrends, by calling build_payload with pytrends with parameters for a keyword list | a timeframe | a geography reference | . | call interest_over_time to get the data, returns a dataframe | . It will be useful to wrap some of the code that sets up the paramaters for build_payload into a function . I&#39;ll call the function set_payload . def set_payload(trend_req, kw_list, nweeks=100, geo=&#39;US&#39;, cat=60, grop=&#39;&#39;): end_date = datetime.date.today() start_date = datetime.date.today() - datetime.timedelta(weeks=nweeks) timeframe = start_date.strftime(&quot;%Y-%m-%d&quot;) + &quot; &quot; + end_date.strftime(&quot;%Y-%m-%d&quot;) trend_req.build_payload(kw_list, cat=cat, timeframe=timeframe, geo=geo, gprop=grop) return trend_req . tr = TrendReq(hl=&#39;en-US&#39;, tz=360) tr = set_payload(trend_req=tr, kw_list=[&#39;work&#39;], cat=60, nweeks=20, geo=&#39;US&#39;, grop=&#39;&#39;) df = tr.interest_over_time() print(df.head(2)) print(df.tail(2)) . work isPartial date 2020-07-13 87 False 2020-07-14 88 False work isPartial date 2020-11-27 50 False 2020-11-28 49 False . I don&#39;t know what isPartial means yet. The module documentation does not say much about it. I read somewere You can ignore isPartial for now: that field lets you know if the data point is complete for that particular date, so I plan to remove an ignore it. Now let&#39;s try with more keywords and plot the results . kw_list = [ &quot;work from home&quot;, &quot;unemployment&quot;, &quot;indeed jobs&quot;, &quot;work&quot;] nweeks = 52 tr = set_payload(trend_req=tr, kw_list=kw_list, nweeks=nweeks, ) df = tr.interest_over_time().drop(&quot;isPartial&quot;, axis=1) df.plot(figsize=[12,6], grid=True, marker=&#39;o&#39;) . &lt;AxesSubplot:xlabel=&#39;date&#39;&gt; . Notes . the resulting data is weekly. I have read that that is the standard when the time span is over 90 days. | There is a get_daily_data method available in the pytrends module but I ok with weekly for now. | . Getting More keywords . OK, so far so good. Maybe using more keywords would give us better preictor variables, but what other keywords might be useful. The trend_requst object gives 2 methods that seem like they might help here . relate_queries | suggestions I&#39;m not sure the difference but for now I am usig related_queries. It returns a score which I belive is some measure of how related the other query is. Below I have some code to do the following | start with some seed queries, push then into a queue | pop an item off the queue, an item includes a keyword and a score | if the score is above a threshold add the keyword to the keyword list | call related queries on the keyword and push all results above a cutoff into the queue | . | . from collections import deque import time sleeptime = 0.1 verbosity = 1 max_keywords = 100 min_score = 98 keywords2try = deque() keyword_tups = deque() keyword_set = set() # seed the queue with a few terms keywords2try.appendleft((&quot;jobs&quot;, 101, 0, &#39;root&#39;, 101)) keywords2try.appendleft((&quot;unemployment&quot;, 101, 0, &#39;root&#39;, 101)) print(&quot;Start {0}&quot;.format(datetime.datetime.now())) while len(keywords2try) &gt; 0: if len(keyword_tups) &gt; max_keywords: break kw, score, level, parent, parent_score = keywords2try.pop() if kw in keyword_set: if verbosity &gt; 1: print(&quot;{0} already in set&quot;.format(kw)) continue if verbosity &gt; 0: print(&quot;&lt;kw#{1}=&#39;{0}&#39;&#39;&gt;, &quot;.format(kw, len(keyword_tups)+1), end=&#39;&#39;) # add to keyword_tups and keyword_set keyword_tups.appendleft((kw, score, level, parent, parent_score)) keyword_set.add(kw) trend_req = TrendReq(hl=&#39;en-US&#39;, tz=360) trend_req = set_payload(trend_req, kw_list=[kw]) # do I need to insert some wait time here? time.sleep(sleeptime) try: related = trend_req.related_queries() except Exception as e: last_exception = e exc_type, exc_obj, exc_tb = sys.exc_info() related = &quot;error&quot; break if not related: if verbosity &gt; 1: print(&quot; t No related&quot;) continue if &#39;top&#39; not in related[kw].keys(): if verbosity &gt; 1: print(&quot; tNo top&quot;) continue df = related[kw][&#39;top&#39;] if df is None: if verbosity &gt; 1: print(&quot; ttop is empty&quot;) continue for tup in df.itertuples(): child_score = score * tup.value/float(100) if child_score &gt; min_score and tup.query not in keyword_set: if verbosity &gt; 1: print(&quot; tpushing {0}&quot;.format(tup.query)) keywords2try.appendleft((tup.query, child_score, level+1, kw, score)) if related == &quot;error&quot;: import traceback traceback.print_tb(exc_tb) fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1] print(exc_type, fname, exc_tb.tb_lineno) raise Exception(last_exception) print(&quot;Done {0}&quot;.format(datetime.datetime.now())) kw_df = pd.DataFrame(data=list(keyword_tups), columns=[&#39;keyword&#39;, &#39;score&#39;, &#39;level&#39;,&#39;parent&#39;, &#39;parent_score&#39;]).sort_values(by=&quot;score&quot;, ascending=False) kw_df.sort_values(by=&#39;score&#39;, ascending=True) kw_df.tail() . Start 2020-11-30 11:21:58.099593 &lt;kw#1=&#39;jobs&#39;&#39;&gt;, &lt;kw#2=&#39;unemployment&#39;&#39;&gt;, &lt;kw#3=&#39;jobs near me&#39;&#39;&gt;, &lt;kw#4=&#39;unemployment jobs&#39;&#39;&gt;, &lt;kw#5=&#39;hiring jobs near me&#39;&#39;&gt;, &lt;kw#6=&#39;florida unemployment&#39;&#39;&gt;, &lt;kw#7=&#39;unemployment benefits&#39;&#39;&gt;, &lt;kw#8=&#39;jobs hiring near me part time&#39;&#39;&gt;, &lt;kw#9=&#39;careersource&#39;&#39;&gt;, &lt;kw#10=&#39;part time unemployment benefits&#39;&#39;&gt;, &lt;kw#11=&#39;job hiring near me&#39;&#39;&gt;, &lt;kw#12=&#39;careersource florida&#39;&#39;&gt;, &lt;kw#13=&#39;jobs near me hiring&#39;&#39;&gt;, &lt;kw#14=&#39;careersource central florida&#39;&#39;&gt;, &lt;kw#15=&#39;careersource north central florida&#39;&#39;&gt;, Done 2020-11-30 11:22:27.236610 . keyword score level parent parent_score . 12 jobs near me | 101.00 | 1 | jobs | 101.00 | . 13 unemployment | 101.00 | 0 | root | 101.00 | . 14 jobs | 101.00 | 0 | root | 101.00 | . 5 part time unemployment benefits | 98.98 | 3 | unemployment benefits | 98.98 | . 8 unemployment benefits | 98.98 | 2 | unemployment jobs | 101.00 | . print(&quot;Found {0} queries&quot;.format(kw_df.shape[0])) kw_df.sort_values(by=&quot;score&quot;, inplace=True) out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) try: kw_df.to_csv(os.path.join(out_dir, &quot;google_keywords.csv&quot;)) except Exception as exc: print(exc) raise(exc) . trend_req = TrendReq(hl=&#39;en-US&#39;, tz=360) nyears = 2 trends_df = pd.DataFrame() for tup in kw_df.itertuples(): kw_list = [tup.keyword] nweeks = 52*nyears trend_req = set_payload(trend_req, kw_list, nweeks=nweeks, ) temp_df = trend_req.interest_over_time() temp_df.drop(&#39;isPartial&#39;, axis=1, inplace=True) if trends_df.shape[0] == 0: trends_df = temp_df else: trends_df = trends_df.join(temp_df) print(trends_df.columns) . trends_df.columns trends_df.head() . import matplotlib.pyplot as plt trends_df.plot(figsize=(20, 10)) plt.legend(loc=&#39;lower left&#39;) plt.show() . import datetime print(datetime.datetime.now()) .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/google.html",
            "relUrl": "/jupyter/2021/03/21/google.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Part 3 - Predicting the Jobs number - Course correction",
            "content": "I am making some changes in course due to new discoveries about the data and also refocusing effort. . The first changes should be clear, I am going to start using Jupyter notebooks as the format for my posts. The whole point is to show some of the methods and Jupyter notebooks provide a great way to do that. Thanks to the folks at fastai for their tools and tutorials to help me get started with this. . As with any project, I learn new things as I go along, especiall about the data. . I had said in an earlier post that I would pull the U. of Michigan survey data from the U. Mich site because there was a delay of a month for the data to be available on FRED. Well, it&#39;s not just FRED but overall. I believe there is some arrnagement with Bloomberg to not publish the data elsewhere for a month. I also believe that the overall consumer score can be scrapped from the U. Mich site, but not the results for the 5 questions that make up the overall score. So, the forward looking question results will have a delay of 1 month. Later I will give some notebook code to pull the data from the U. Michigan site automatically using Selenium with Python. And I guess I will use the overall score and get that from the U. Michigan site. . Another change is that I will add the OECD Consumer and Business confidence indexes to the predictor variables. I found how to get them via FRED. There are API&#39;s that should work directly with the OECD but if I can get it from FRED then that is much easier for me since I already know how to use one of the Python APIs for that. . Speaking of data in general, I found this note from the SF Fed, https://www.frbsf.org/education/publications/doctor-econ/2013/october/labor-market-indicators-monetary-policy-unemployment-rate/ Seems that I am using a lot of the data sources that the fed uses. I am going to add a few of the series mentioned in that document. . One other change not so much related to data but to priorities. I will spend less time on developing a standard time series regression model. I am more interested in what I can do with a machine learning ensemble method as well as with a neural net model. . I still have to deal with data at different frequencies. A big shout out to Tom Stark, https://www.philadelphiafed.org/research-and-data/research-contacts/stark, at the Philadelphia Fed for sending me some advice on how to deal with this. He says to use a Kalman filter. And Tom was nice enough to send me his notes on the subject, comprising of 4 PDF files and over 350 pages. Very nice Tom, what a real gem you are. . With that as an intro to Part 3, let&#39;s get started getting data from FRED. . import os import sys import datetime import inspect import pandas as pd import numpy as np from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.9.2 of Python version 1.2.3 of pd version 1.20.1 of np version 3.3.4 of mpl . Getting data from FRED using a Python API . The FRED website itself has a pretty nice GUI for plotting data. But I need to download the data so I can use Python or R tools to do the forecasting. . The FRED website gives links to projects you can use to access the FRED data from a variety of languages. See https://fred.stlouisfed.org/docs/api/fred/ I have used the python one called fredapi, https://github.com/mortada/fredapi, before and will use it here with one modification. I will add arguments to limit the series start and end dates. . To use any of the API tools you will need an API Key. See the instructions here, https://fred.stlouisfed.org/docs/api/api_key.html, to get one and get started. I keep mine in a file called &quot;fred_api_key&quot;. The fredapi module is in a sibling directory so I add that to my path before importing the module. . print(os.getcwd()) os.listdir(&quot;..&quot;) #os.listdir(&quot;.. fredapi&quot;) . C: Users jmull Documents Github job-forecasting _notebooks . [&#39;.devcontainer.json&#39;, &#39;.git&#39;, &#39;.gitattributes&#39;, &#39;.github&#39;, &#39;.gitignore&#39;, &#39;.gitmodules&#39;, &#39;assets&#39;, &#39;docker-compose.yml&#39;, &#39;fredapi&#39;, &#39;Gemfile&#39;, &#39;Gemfile.lock&#39;, &#39;images&#39;, &#39;index.html&#39;, &#39;LICENSE&#39;, &#39;Makefile&#39;, &#39;README.md&#39;, &#39;settings.ini&#39;, &#39;_action_files&#39;, &#39;_config.yml&#39;, &#39;_fastpages_docs&#39;, &#39;_includes&#39;, &#39;_layouts&#39;, &#39;_misc_needed_files&#39;, &#39;_notebooks&#39;, &#39;_pages&#39;, &#39;_plugins&#39;, &#39;_posts&#39;, &#39;_sass&#39;, &#39;_word&#39;] . sys.path.append(&#39;../../fredapi/fredapi&#39;) from fredapi import fred import fredapi Fred = fred.Fred(api_key_file=&quot;fred_api.key&quot;) print(fredapi.__version__) . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-5-85e61bdd8a6f&gt; in &lt;module&gt; 1 sys.path.append(&#39;../../fredapi/fredapi&#39;) -&gt; 2 from fredapi import fred 3 import fredapi 4 Fred = fred.Fred(api_key_file=&#34;fred_api.key&#34;) 5 print(fredapi.__version__) ModuleNotFoundError: No module named &#39;fredapi&#39; . How to identify the data on FRED . To acces the data for a series from FRED you need to know the series id. You can use the fredapi to search for a series given keywords. I already know the series ids I want to use and have them in a csv file, so let&#39;s read that in and see the ids and a brief description of the data for each one. . fred_ids = pd.read_csv(&#39;fred_ids.csv&#39;, index_col=None, sep=&#39;|&#39;) fred_ids . Now we call the fredapi to get the observations for each series. I&#39;m only getting the data back to the start of 2007. I&#39;ll print out the number of observations we get of reach series and we will notice the different frequencies. . FRED and ALFRED . Have I not mentioned ALFRED yet? The goverment releases, like Nonfarm payrolls, usually get updated in subsequent months, that is, the initial estimates might be revised up or down. FRED just has the latest update. You probably want to get the value as released or perhaps the value that was known on a given date. You get that from ALFRED. Notice in the call below I get all_releases and then manipulate the data to only keep the earliest one, or the original. The API is the same for FRED and ALFRED, but if you get other than the most current data you will be getting it from ALFRED. . dfs = [] obs_start = &quot;2007-01-01&quot; obs_end = datetime.date.today() for ser in fred_ids.itertuples(): print(ser.series_id, end=&#39;, &#39;) try: all_df = Fred.get_series_all_releases(series_id=ser.series_id, observation_start=obs_start, observation_end=obs_end) tdf = all_df.sort_values(by=&quot;realtime_start&quot;, ascending=True).groupby(by=&quot;date&quot;).head(1) tdf[&#39;series_id&#39;] = ser.series_id tdf[&#39;date&#39;] = tdf[&#39;date&#39;].dt.date print(&quot;rows= {0}&quot;.format(tdf.shape[0])) dfs.append(tdf) except Exception as exc: exc_type, exc_obj, exc_tb = sys.exc_info() fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1] print(exc_type, fname, exc_tb.tb_lineno) obs_df = pd.concat(dfs) obs_df = obs_df.merge(fred_ids, on=&#39;series_id&#39;) . Merge in the descriptions and have a look at the last few rows. . obs_df.sort_values(by=&quot;date&quot;, inplace=True) obs_df[&#39;dtime&#39;] = pd.to_datetime(obs_df[&#39;date&#39;]) print(obs_df.tail(4)) . Not for HFT . A quick aside about how quickly the data is available on FRED. The Claims data was released at 8:30 ET on Thursday and I first saw it on FRED and in my download about 10-15 minutes later. While not fast enough for high frequency trading, still very good. Also let me say how helpful the folks at FRED have been. I&#39;ve emailed them a number of questions and they typically get back to me in a day or so. FRED and ALFRED are great resources. . Plot of all the data . Let&#39;s have a look at what the series look like over time. What a shock to the employment situation we&#39;ve had this year. Note that in many ways it dwarfs the impact from the 2008 financial crisis. Note how the business confidence index has bounced back. Not so much for consumer confidence. Our target is BLS private. Looks like the ADP data is tracking it really well and it comes out the week before. . def plot_observations(data_df, xcol, ycol, idcol, ncols, figwd, fight, ylim=(None, None), xtick_rot=0, sharex=True): fig = None ids = list(data_df[idcol].unique()) nrows = int(np.ceil(len(ids)/float(ncols))) fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=[figwd, fight], sharex=sharex) gdf = data_df.groupby(by=&#39;description&#39;) for i, (key, group) in enumerate(gdf): row = i // ncols col = i % ncols ax = axs[row][col] group.plot(ax=ax, kind=&#39;line&#39;, x=xcol, y=ycol,linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, ylim=ylim, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, title=key, legend=None, label=None, grid=True) ax.xaxis.set_tick_params(rotation=xtick_rot) return fig . ylim=(None, None) fig = plot_observations(data_df=obs_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None)) . Plot of the last year . Let&#39;s focus in on data from 2020. Notice how some of the series are almost current while others are lagging more than a month. . temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date.today() - datetime.timedelta(weeks=52)] temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date(year=2020, month=1, day=1)] ylim=(None, None) fig = plot_observations(data_df=temp_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None), xtick_rot=90) . save the dataframe into the output directory . out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) obs_df.to_csv(os.path.join(out_dir, &quot;fred.csv&quot;)) . Tidy Fred . cols = [&quot;date&quot;, &quot;value&quot;] series_ids = obs_df[&quot;series_id&quot;].unique() dfs = [] tidy_df = pd.DataFrame() for sid in series_ids: x = obs_df.loc[obs_df[&quot;series_id&quot;]==sid,cols] x.columns = [&quot;date&quot;, sid] if tidy_df.shape[0] == 0: tidy_df = x else: tidy_df = tidy_df.merge(x, how=&quot;outer&quot;, on=&quot;date&quot;) tidy_df.sort_values(by=&quot;date&quot;, inplace=True) scols = [&quot;date&quot;, &quot;USPRIV&quot;, &quot;ICSA&quot;, &quot;JTS1000JOL&quot;] print(tidy_df[scols].head(10)) print(tidy_df[scols].tail(10)) tidy_df.to_csv(&quot;./data/tidy_fred.csv&quot;) . That&#39;s all for now on FRED data. Next up, U. Michigan data and Google Trends data. . import datetime print(datetime.datetime.now()) .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/fred.html",
            "relUrl": "/jupyter/2021/03/21/fred.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Sandbox - Ignore this",
            "content": "import os import sys import time import datetime import touch import inspect import datetime # don&#39;t really need these below, just for testing import numpy from PIL import Image import plotnine as pn import pandas as pd print(sys.version_info) . sys.version_info(major=3, minor=8, micro=5, releaselevel=&#39;final&#39;, serial=0) . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: mname = name if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): mname = name if hasattr(mod, &quot;__path__&quot;): mname = os.path.split(mod.__path__[0])[1] print(&quot;version {1} of {0} as {2} &quot;.format(mname, name, mod.__version__)) elif hasattr(mod, &quot;__file__&quot;) and &quot;site-packages&quot; in mod.__file__: print(&quot;No __version__ for {0} as {1}&quot;.format(mname, name)) del mod del name . version 3.8.5 of Python No __version__ for touch as touch version numpy of numpy as 1.19.3 version Image of Image as 8.0.1 version pn of plotnine as 0.7.1 version pd of pandas as 1.1.3 . import inspect import nbformat fname = &quot;nbformat.convert&quot; lines = inspect.getsource(eval(fname)) #print(lines) . def get_project_notebooks(): from nbformat import read, NO_CONVERT import os import datetime proj_nbs = [] nb_fnames = [f for f in os.listdir() if f.endswith(&quot;.ipynb&quot;)] for fn in nb_fnames: with open(fn) as fp: notebook = read(fp, as_version=NO_CONVERT) cells = notebook[&#39;cells&#39;] if not cells: continue src =cells[0][&quot;source&quot;] if not src: continue lines = src.split(&quot; n&quot;) if lines[0].startswith(&quot;# &quot;): proj_nbs.append(fn) return(proj_nbs) # print the source code for a cell def print_source(cell): print(&quot;type cell= {0}&quot;.format(type(cell))) src = cell[&quot;source&quot;] lines = src.split(&quot; n&quot;) print(&quot;{0} lines&quot;.format(len(lines))) for i,line in enumerate(lines): print(&quot;({0}){1}&quot;.format(i, line)) # run notebook code def run_nb_code(nbfile): from nbformat import read, NO_CONVERT import os import datetime res = &quot;Start time: &quot; + str(datetime.datetime.now()) with open(nbfile) as fp: notebook = read(fp, NO_CONVERT) cells = notebook[&#39;cells&#39;] code_cells = [c for c in cells if c[&#39;cell_type&#39;] == &#39;code&#39;] for i, cell in enumerate(code_cells): src = cell[&#39;source&#39;] #print(&quot;{0}&lt;{1}&gt;&quot;.format(li, line)) try: exec(src, globals(), locals()) except Exception as e: print(&quot;Error on cell {0} n&quot;.format(i)) print(src) print_source(cell) exc_type, exc_obj, exc_tb = sys.exc_info() import traceback traceback.print_tb(exc_tb) fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1] print(exc_type, fname, exc_tb.tb_lineno) return((exc_type, exc_obj, exc_tb, fname)) res += &quot;end: &quot; + str(datetime.datetime.now()) return res # run notebook code def get_first_cell_line(nbfile): from nbformat import read, NO_CONVERT import os import datetime res = &quot;Start time: &quot; + str(datetime.datetime.now()) with open(nbfile) as fp: notebook = read(fp, NO_CONVERT) cells = notebook[&#39;cells&#39;] if not cells: return &quot;&quot; lines = cells[0][&quot;source&quot;].split(&quot; n&quot;) if not lines: return &quot;&quot; return lines[0] def ordered_notebooks(): nbs = get_project_notebooks() nbord = [None]*len(nbs) end_index = len(nbs)-1 for nb in nbs: line0 = get_first_cell_line(nb) parts = line0.split(&#39; &#39;) index = -1 if len(parts) &gt;= 3 and parts[0]==&#39;#&#39; and parts[1]==&#39;Part&#39;: try: index = int(parts[2]) - 1 except Exception as e: pass if index == -1: continue nbord[index] = nb nbord = [nb for nb in nbord if nb] return (nbord) print(get_project_notebooks()) print(ordered_notebooks()) . [&#39;combining_claims.ipynb&#39;, &#39;DataSources.ipynb&#39;, &#39;fastpages-howto.ipynb&#39;, &#39;foo.ipynb&#39;, &#39;fred.ipynb&#39;, &#39;google.ipynb&#39;, &#39;Intro_Overview.ipynb&#39;, &#39;setup_issues.ipynb&#39;, &#39;umich_cs.ipynb&#39;, &#39;weekly_claims.ipynb&#39;] [&#39;Intro_Overview.ipynb&#39;, &#39;DataSources.ipynb&#39;, &#39;fred.ipynb&#39;, &#39;weekly_claims.ipynb&#39;, &#39;umich_cs.ipynb&#39;, &#39;google.ipynb&#39;, &#39;combining_claims.ipynb&#39;] . from IPython.utils.capture import CapturedIO from IPython.utils.capture import capture_output nbs = ordered_notebooks() print(nbs) ofname = &#39;foo_out.txt&#39; with open(ofname, mode=&#39;w&#39;) as fp: fp.write(&quot;Datetime= {0}&quot;.format(str(datetime.datetime.now()))) outstr = &quot;&quot; x = None for nb in nbs: print(&quot; nRunnning {0}&quot;.format(nb)) with capture_output() as co: %run $nb ofname = os.path.splitext(nb)[0] + &quot;.out&quot; with open(ofname, mode=&quot;w&quot;) as fp: fp.write(str(datetime.datetime.now())) fp.write(&quot; stdout n{0} ns stderr{1}&quot;.format(co.stdout, co.stderr)) for o in co.outputs: fp.write(str(o)) . [&#39;Intro_Overview.ipynb&#39;, &#39;DataSources.ipynb&#39;, &#39;fred.ipynb&#39;, &#39;weekly_claims.ipynb&#39;, &#39;umich_cs.ipynb&#39;, &#39;google.ipynb&#39;, &#39;combining_claims.ipynb&#39;] Runnning Intro_Overview.ipynb Runnning DataSources.ipynb Runnning fred.ipynb Runnning weekly_claims.ipynb Runnning umich_cs.ipynb Runnning google.ipynb Runnning combining_claims.ipynb . import time nbs = get_project_notebooks() last = len(nbs) - 1 nblist = [&quot;&quot;]*len(nbs) for nb in nbs: line = get_first_cell_line(nb) line_parts = line.split(&quot; &quot;) if len(line_parts) &gt; 3 and line_parts[1] == &quot;Part&quot;: try: index = int(line_parts[2]) - 1 except: index = last last -= 1 else: index = last last -= 1 nblist[index] = nb #print(f&quot;{nb}, index={index}, {line}&quot;) print(nblist) def add_str_to_file(fname, str=&quot; n&quot;): with open(fname, mode=&#39;a&#39;) as fp: fp.write(str) return if True: for nb in nblist: try: print(&quot;{0} {1}&quot;.format(nb, datetime.datetime.now())) touch.touch(nb) add_str_to_file(nb, str=&quot; n&quot;) time.sleep(61) except Exception as e: print(e) . [&#39;Intro_Overview.ipynb&#39;, &#39;DataSources.ipynb&#39;, &#39;fred.ipynb&#39;, &#39;weekly_claims.ipynb&#39;, &#39;umich_cs.ipynb&#39;, &#39;google.ipynb&#39;, &#39;combining_claims.ipynb&#39;, &#39;setup_issues.ipynb&#39;, &#39;foo.ipynb&#39;, &#39;fastpages-howto.ipynb&#39;] Intro_Overview.ipynb 2020-12-01 00:59:52.851844 DataSources.ipynb 2020-12-01 01:00:53.867450 fred.ipynb 2020-12-01 01:01:54.881359 weekly_claims.ipynb 2020-12-01 01:02:55.900071 umich_cs.ipynb 2020-12-01 01:03:56.905095 google.ipynb 2020-12-01 01:04:57.914338 combining_claims.ipynb 2020-12-01 01:05:58.923362 setup_issues.ipynb 2020-12-01 01:06:59.935349 foo.ipynb 2020-12-01 01:08:00.946764 fastpages-howto.ipynb 2020-12-01 01:09:01.957830 . def print_vars(globs=True, locs=True, filters={&quot;exclude&quot;:(&quot;^_&quot;,)}): import re dicts = {&quot;globs&quot;:globals().keys(), &quot;locs&quot;:locals().keys()} for k,v in dicts.items(): print(&quot;k={0}, eval(k)= {1}&quot;.format(k, eval(k))) if eval(k): y = [x for x in dicts[k]] if filters: if filters[&quot;exclude&quot;]: for filt in filters[&quot;exclude&quot;]: print(&quot;excluding: {0}&quot;.format(filt)) y = [x for x in y if not re.search(filt, x)] print(&quot;{0}= {1}&quot;.format(k,y)) del y print_vars() .",
            "url": "https://jhmuller.github.io/job-forecasting/2021/03/21/foo.html",
            "relUrl": "/2021/03/21/foo.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages - Brief How-to guide",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-4-88b922e30289&gt; in &lt;module&gt; 1 #collapse-hide 2 import pandas as pd -&gt; 3 import altair as alt ModuleNotFoundError: No module named &#39;altair&#39; . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . NameError Traceback (most recent call last) &lt;ipython-input-8-85b8363709cc&gt; in &lt;module&gt; 1 # single-value selection over [Major_Genre, MPAA_Rating] pairs 2 # use specific hard-wired values as the initial selected values -&gt; 3 selection = alt.selection_single( 4 name=&#39;Select&#39;, 5 fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], NameError: name &#39;alt&#39; is not defined . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/fastpages-howto.html",
            "relUrl": "/jupyter/2021/03/21/fastpages-howto.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Part 7 - Combining input data",
            "content": "import os import sys import inspect import datetime import pandas as pd import numpy as np from plotnine import ggplot import matplotlib.pyplot as plt . data_dir = &quot;./data&quot; fnames = os.listdir(data_dir) dfs = {} for fname in fnames: fpath = os.path.join(data_dir, fname) head, tail = os.path.split(fpath) fbase,ext = os.path.splitext(tail) if os.path.isfile(fpath) and ext == &quot;.csv&quot;: df = pd.read_csv(fpath) print(&quot;{0} shape:{1}&quot;.format(fbase,df.shape)) dfs[fbase] = df . fred shape:(3604, 7) google_keywords shape:(24, 6) tidy_fred shape:(868, 17) umich_exp shape:(153, 5) us_pau_claims shape:(33, 4) . fred_claims = dfs[&quot;tidy_fred&quot;][[&quot;date&quot;, &quot;ICSA&quot;, &quot;CCSA&quot;]] fred_claims[&quot;date&quot;] = pd.to_datetime(fred_claims[&quot;date&quot;]) pau_claims = dfs[&#39;us_pau_claims&#39;] pau_claims[&quot;date&quot;] = pd.to_datetime(pau_claims[&quot;date&quot;]) print(&quot;Fred n{0} n&quot;.format(fred_claims.tail(2))) print(&quot;pau n{0} n&quot;.format(pau_claims.tail(2))) claims_df = fred_claims.merge(pau_claims, how=&quot;outer&quot;, on=&quot;date&quot;) claims_df.set_index(keys=&quot;date&quot;, inplace=True) print(&quot;claims n{0} n&quot;.format(claims_df.tail(2))) #claims_df.index = pd.to_datetime(claims_df.index) #print(claims_df.tail(2)) . Fred date ICSA CCSA 866 2020-11-14 742000.0 6071000.0 867 2020-11-21 778000.0 NaN pau date PUA IC PUA CC PEUC CC 31 2020-11-07 296374.0 8681647.0 4376847.0 32 2020-11-14 320237.0 NaN NaN claims ICSA CCSA PUA IC PUA CC PEUC CC date 2020-11-14 742000.0 6071000.0 320237.0 NaN NaN 2020-11-21 778000.0 NaN NaN NaN NaN . &lt;ipython-input-20-d61b5487242c&gt;:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . claims_df.sort_index( inplace=True) print(claims_df.columns) print(claims_df.tail(10)) ic_df = claims_df[[ &quot;ICSA&quot;, &#39;PUA IC&#39;]].dropna(how=&quot;all&quot;) ic_df[&quot;IC_all&quot;] = ic_df.sum(axis=1, skipna=False) cc_df = claims_df[[ &quot;CCSA&quot;, &#39;PUA CC&#39;, &#39;PEUC CC&#39;]].dropna(how=&quot;all&quot;) cc_df[&quot;CC_all&quot;] = cc_df.sum(axis=1, skipna=False) print(cc_df.tail()) print(ic_df.tail()) . Index([&#39;ICSA&#39;, &#39;CCSA&#39;, &#39;PUA IC&#39;, &#39;PUA CC&#39;, &#39;PEUC CC&#39;], dtype=&#39;object&#39;) ICSA CCSA PUA IC PUA CC PEUC CC date 2020-09-26 837000.0 10976000.0 450696.0 10686922.0 1989499.0 2020-10-01 NaN NaN NaN NaN NaN 2020-10-03 840000.0 10018000.0 378964.0 10978217.0 2824685.0 2020-10-10 898000.0 8373000.0 337380.0 10450384.0 3334878.0 2020-10-17 787000.0 7756000.0 345440.0 10622725.0 3711089.0 2020-10-24 751000.0 7285000.0 359044.0 9332610.0 3983613.0 2020-10-31 751000.0 6786000.0 361959.0 9433127.0 4143389.0 2020-11-07 709000.0 6372000.0 296374.0 8681647.0 4376847.0 2020-11-14 742000.0 6071000.0 320237.0 NaN NaN 2020-11-21 778000.0 NaN NaN NaN NaN CCSA PUA CC PEUC CC CC_all date 2020-10-17 7756000.0 10622725.0 3711089.0 22089814.0 2020-10-24 7285000.0 9332610.0 3983613.0 20601223.0 2020-10-31 6786000.0 9433127.0 4143389.0 20362516.0 2020-11-07 6372000.0 8681647.0 4376847.0 19430494.0 2020-11-14 6071000.0 NaN NaN NaN ICSA PUA IC IC_all date 2020-10-24 751000.0 359044.0 1110044.0 2020-10-31 751000.0 361959.0 1112959.0 2020-11-07 709000.0 296374.0 1005374.0 2020-11-14 742000.0 320237.0 1062237.0 2020-11-21 778000.0 NaN NaN . import matplotlib.dates as mdates figwd = 14 fight= 8 fig, axs = plt.subplots(nrows=2, ncols=1, figsize=[figwd, fight], sharex=&#39;row&#39;) ic_df.tail(10).plot(ax=axs[0], kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;x&#39;, lw=2, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=2.2, legend=True, label=None, grid=True) cc_df.tail(10).plot(ax=axs[1], kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;x&#39;, lw=2, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=2.2, legend=True, label=None, grid=True) loc = mdates.DayLocator([0,31]) axs[0].xaxis.set_major_locator(loc) axs[0].xaxis.set_major_formatter(mdates.AutoDateFormatter(loc)) axs[1].xaxis.set_major_locator(loc) axs[1].xaxis.set_major_formatter(mdates.AutoDateFormatter(loc)) plt.show() . if False: from plotnine import ggplot, geom_line ggplot(data=ic_df) + geom_line() .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/combining_claims.html",
            "relUrl": "/jupyter/2021/03/21/combining_claims.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Part 1 - Introduction and Overview",
            "content": "This is the first of a series of posts in which I plan to talk about a real world prediction problem, predicting the change in one of the monthly government macro economic values. In particular, I will look at predicting the change in the number of people working for US private employers in a given month. The number of people working in the US is compiled by the U.S Bureau of Labor Statistics, BLS, and is published monthly on the first Friday of each month. Of interest is not so much the value, or level as it is often call, but the change in the value. . The value published is an estimate of people employed in the prior month. For example, the value published on Friday 2020-10-02 gives an estimate of the number of people employed in September 2020. The number are published in a report currently called the Employment Situation. . There are actually many numbers in the report the BLS publishes with two main types, unemployment and jobs. The unemployment numbers come from a survey of households and the jobs numbers come from a survey of businesses. For the jobs number there is an aggregate value, i.e. for the entire US, and then breakdowns into private versus government and then many further breakdowns by industry type and other things. Suffice it to say that there is a lot of data in the report but I am only going to focus on the number of private jobs added in a month. The reason to focus on private and not the sum of private and government is becuase of some other data I want to use and it only covers private jobs. Why do this? For me it&#39;s to practice data science with real data on an important problem. The jobs number is one of the measures of the health of the economy. I&#39;m writing this in October 2020 with a presidential election coming next month. The report released on October 2nd got a lot of attention in the press. Both the stock and bond markets pay attention to the releases too. Back in 2007 I worked in the investment group of a large bank and I recall that when the numbers came out, usually around 8:30 AM New York time, they were announced for all the traders to hear. As evidence of the potential impact on the markets see here, here, here, or better still just google it. . You might ask WHy not just predict the S&amp;P 500 or something like that if we are interested in the markets. Well, we might get to that later. Some potential methods are described in the papers listed below. . Generative Adversarial Network for Stock Market price Prediction | EMPIRICAL ASSET PRICING VIA MACHINE LEARNING | . I plan to use a variety of techniques including Time Series Regression, Ensemble methods such as Random Forests, and hopefully Neural Networks. I&#39;m interested not only in how the perform out of sample but also how hard it is to build the models and how interpretable the model is. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/Intro_Overview.html",
            "relUrl": "/jupyter/2021/03/21/Intro_Overview.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Part 2 - Data Sources",
            "content": "Fred - the one stop shop . You can get the current value and some historical values from the BLS site, but I am going to use a different source, the St. Federal Reserve Economic Data site referred to as FRED . FRED is a great resource for all kinds of economic data and they have an API you can use to get the data. So I will get both the target variable, i.e. the private jobs number, as well as many of the predictor variables from FRED. . So what to use for predictor variables. One obvious choice is past values of the private jobs number. Past values of the unemployment numbers might also be helpful. There is another publication from the BLS called the JOLTS report for Job Openings and Labor Turnover Survey. This should get you to the latest release. I will use 3 values from that report, job openings, hirings and separations. We have to be careful with this one since it is published with a longer lag than the main jobs report. For example, the JOLTS report for August comes out in October whereas the Employment Situation report for August comes out in September. We can get the JOLTS data from FRED. . The BLS is part of the US Department of Labor. Another division of the Department of Labor produces weekly reports on initial and continued unemployment insurance claims. The claims data is published weekly so we will have to deal with data at different frequencies. And guess what ... we can get those from FRED. . There is a payroll processing company called ADP that produces something similar to the jobs numbers based on their proprietary data, but they don&#39;t have estimates for government jobs which is why I am only targeting the private number. More about the ADP estimates here. The ADP report comes out near the end of the month often just days before the BLS report. It is intended to be a predictor of the BLS values ... so we will see. And you can get the ADP values from FRED. . University of Michigan Consumer Sentiment . Note that most of the predictor variables I have listed so far are backward looking. They tell us about the sate of the world at some point in the past. It would be nice to have some forward looking indicators, like expectations. One source of that is the University of Michigan survey of consumer sentiment. The Michigan survey includes 3 questions that are forward looking, see the survey methodology document here. I think questions 2 through 4 look like they might be good to use so I will. Now we can get the aggregate value on Fred although with a one month lag. Anyway, I don&#39;t think the components are on FRED so I will download them from the Michigan website here. . Google Trends . And now for something completely different, Google Trends Part of the reason my interest in this project was revived was that I found a paper titled In Search of a Job: Forecasting Employment Growth Using Google Trends about using Google Trends to predict the employment numbers and I wanted to try it out myself. The SSRN version of the paper is here . The basic idea is that the relative demand for search terms such as &quot;jobs&quot; might be an indicator of how many people are looking for work. But what other terms should we use other than &quot;jobs&quot;. The papers suggests using Google&#39;s Keyword Planner to find other related terms. It should be fun. . Other Sources . I&#39;m sure there are many other variables I could use. A researcher from the The St. Louis Fed maintains a special set of over 100 monthly economic variables mainly for researchers to use. Read about it here. I don&#39;t want to use that since I believe it has the updated values of releases, not the original values. I want to use the values that first came out in case there were revisions. . The Conference Board is the source of some widely used economic data including the Leading Economic Indicators. I am not going to use those since I belive you have to subscribe to get them. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2021/03/21/DataSources.html",
            "relUrl": "/jupyter/2021/03/21/DataSources.html",
            "date": " • Mar 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a computer scientist, a data scientist and a finance quant. . I like to solve practical problems and create useful applications. I love doing analytics, machine learning, algorithms and data science, so I am thrilled that these disciplines have all become very popular and that just about every industry is using their methods to solve problems. . This website is powered by fastpages [^1]. [^1]:a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://jhmuller.github.io/job-forecasting/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jhmuller.github.io/job-forecasting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}