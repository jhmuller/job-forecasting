{
  
    
        "post0": {
            "title": "Part 4 - New data on Weekly Claims",
            "content": "I was scanning a WSJ article today on jobless claims, https://www.wsj.com/articles/weekly-jobless-claims-coronavirus-10-29-2020-11603921724?modtag=djemBestOfTheWeb The article started by claiming that initial claims filings had fallen to their lowest level since the pandemic began. This didn&#39;t seem interesting because according to FRED you could have been saying that since May. Seems like saying I am as old as I&#39;ve ever been. But then I noticed a chart showing claims under Regular state programs plus claims under special pandemic programs ... and for continued claims the special pandemic numbers were about the same as the regular ones. Uh oh, so the claims could be 2x what I would get from FRED and the normal BLS data. I could not find any data on these new plans on FRED but did find a spreadsheet on the Department of Labor website, see https://oui.doleta.gov/unemploy/DataDashboard.asp . The data might be available on the DOL or BLS site via some API. Not sure yet. In any case, in this post I will read in that data and plot it. Later I will figure out how to integreate it with the rest. . import os import sys import inspect import datetime import pandas as pd import numpy as np from plotnine import ggplot import matplotlib as mpl import xlrd import matplotlib.pyplot as plt . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd version 1.19.1 of np version 3.3.1 of mpl version 1.2.0 of xlrd . Read . Read in the spreadsheet and get rid of rows with no data for Rptdate . states_df = pd.read_excel(&quot;weekly_pandemic_claims.xlsx&quot;) states_df = states_df[states_df[&#39;Rptdate&#39;].notna()] states_df.tail(3) . State Rptdate PUA IC Reflect Date PUA CC PEUC CC . 1534 WV | 2020-10-17 | 4821.0 | NaT | NaN | NaN | . 1535 WI | 2020-10-17 | 2254.0 | NaT | NaN | NaN | . 1536 WY | 2020-10-17 | 519.0 | NaT | NaN | NaN | . Plot . For now I&#39;m not interested in the state breakdown so I&#39;ll aggregate to US level and also separte out Initial Claims, IC, from Continued Claims, CC. Partly because the CC values are about 10x the IC values so easier to see and check the IC data if plotted alone. Also because there is data on 2 different special programs for CC. . us_df = states_df.drop(labels=[&quot;State&quot;], axis=1).groupby(by=&#39;Rptdate&#39;) us_df = us_df.sum(min_count=1) us_df.index = pd.to_datetime(us_df.index).rename(&quot;date&quot;) print(us_df.head()) ic_df = us_df[&#39;PUA IC&#39;] cc_df = us_df.drop(&#39;PUA IC&#39;, axis=1) cc_df[&#39;CC Tot&#39;] = cc_df.sum(axis=1, skipna=False) cc_df.tail() dfs = {&quot;IC&quot;:ic_df, &quot;CC&quot;:cc_df} . PUA IC PUA CC PEUC CC date 2020-04-04 31949.0 52494.0 0.0 2020-04-11 52126.0 68897.0 3802.0 2020-04-18 224990.0 210939.0 31392.0 2020-04-25 833083.0 1088281.0 59760.0 2020-05-02 1051345.0 3498790.0 86972.0 . figwd = 12 fight= 6 fig, axs = plt.subplots(nrows=2, ncols=1, figsize=[figwd, fight], sharex=True) for i, key in enumerate(dfs.keys()): ax = axs[i] dfs[key].plot(ax=ax, kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, title=key, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, legend=True, label=None, grid=True) . The regular IC values have been about 750K so this new program is about half that. The regular CC values have been about 7.5 million so this new data is about the same. Later I might combine the regular and special data, but for now it&#39;s enough just to have the special data. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/29/weekly_claims.html",
            "relUrl": "/jupyter/2020/10/29/weekly_claims.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Part 5 - University of Michigan Consumer Sentiment data",
            "content": "import os import sys import datetime import time import re import inspect import pandas as pd from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 2.2.1 of re version 1.1.3 of pd version 3.3.1 of mpl . data_dir = &quot;./data&quot; chromedriver_dir = &#39;../../chromedriver_win32&#39; chromedriver_path = os.path.join(chromedriver_dir,&quot;chromedriver.exe&quot;) . from selenium import webdriver from selenium.webdriver.support.ui import Select options = webdriver.ChromeOptions() if not os.path.isdir(data_dir): os.mkdir(data_dir) prefs = {&quot;download.default_directory&quot; : os.path.abspath(data_dir)} options.add_experimental_option(&quot;prefs&quot;,prefs) options.add_argument(&quot;download.default_directory=&quot;+os.path.abspath(data_dir)) driver = webdriver.Chrome(executable_path=chromedriver_path, options=options) url = &quot;https://data.sca.isr.umich.edu/data-archive/mine.php&quot; driver.get(url) print(options) . &lt;selenium.webdriver.chrome.options.Options object at 0x00000293EF036220&gt; . select_element = Select(driver.find_element_by_css_selector(&quot;select&quot;)) # this will print out strings available for selection on select_element, used in visible text below select_element.select_by_index(5) . elements = driver.find_elements_by_name(&quot;format&quot;) button = None for e in elements: if e.get_property(&quot;value&quot;) == &#39;Comma-Separated (CSV)&#39;: button = e break if not button: raise RuntimeError(&quot;Error downloading Consumer Sentiment data from {0}&quot;.format(url)) . def get_downloaded_fpath(dir=None, files_before=None, file_ext=&quot;.csv&quot;, max_wait = 10, verbose=True): import time done = False start_time = datetime.datetime.now() while not done: files_after = set(os.listdir(dir)) new_files = files_after.difference(set(files_before)) print(new_files) for fname in new_files: if os.path.splitext(fname)[1] == file_ext: return (os.path.join(dir, fname)) cur_time = datetime.datetime.now() if (cur_time - start_time).seconds &gt; 10: return None time.sleep(0.5) files_before = set(os.listdir(data_dir)) button.click() fpath = get_downloaded_fpath(dir=data_dir, files_before=files_before, file_ext=&quot;.csv&quot;, max_wait=10) print(fpath) . set() {&#39;sca-table5-on-2020-Oct-29.csv&#39;} ./data sca-table5-on-2020-Oct-29.csv . df = pd.read_csv(fpath, skiprows=1) df.drop(df.columns[-1], inplace=True, axis=1) df[&#39;Datetime&#39;] = (100*100*df[&#39;Year&#39;] + 100*df[&quot;Month&quot;] + 15).astype(str) df[&#39;Datetime&#39;] = pd.to_datetime(df[&#39;Datetime&#39;]) df.drop([&quot;Month&quot;, &quot;Year&quot;], inplace=True, axis=1) print(df.agg([&#39;min&#39;, &#39;max&#39;])) df.set_index(&quot;Datetime&quot;, inplace=True) df.head() exp_columns = [c for c in df.columns if re.search(&quot;xpected|12 Months&quot;, c)] . Personal Finance Current Personal Finance Expected min 58 96 max 142 137 Business Condition 12 Months Business Condition 5 Years min 31 48 max 132 112 Buying Conditions Current Index Expected Index Datetime min 86 57.5 47.6 2008-01-15 max 173 121.2 93.5 2020-08-15 . df[exp_columns].plot(figsize=[16,4], marker=&#39;.&#39;, grid=True) plt.gca().set_ylim(bottom=0) . (0.0, 142.3) .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/29/umich_cs.html",
            "relUrl": "/jupyter/2020/10/29/umich_cs.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Part 6 - Google Trends data",
            "content": "import os import sys import datetime import inspect import pandas as pd import pytrends from pytrends.request import TrendReq . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd . pytrends = TrendReq(hl=&#39;en-US&#39;, tz=360) kw_list = [ &quot;work from home&quot;, &quot;unemployment&quot;, &quot;indeed jobs&quot;, &quot;work&quot;] # &quot;unemployment&quot;, &quot;job&quot; &quot; ui online&quot; #,&#39;hire&#39;, &quot;Jobs&quot;, &quot;vacancy&quot;, &quot;employment&quot;, &quot;unemployment&quot;, &quot;benefits&quot;] &quot;njuifile&quot; nweeks = 200 end_date = datetime.date.today().strftime(&quot;%Y-%m-%d&quot;) start_date = (datetime.date.today() - datetime.timedelta(weeks=nweeks)).strftime(&quot;%Y-%m-%d&quot;) timeframe = start_date+ &quot; &quot; + end_date print(timeframe) geo = &#39;US&#39; pytrends.build_payload(kw_list, cat=60, timeframe=timeframe, geo=geo, gprop=&#39;&#39;) . 2016-12-29 2020-10-29 . df = pytrends.interest_over_time() . df.plot(figsize=[12,6], grid=True, marker=&#39;o&#39;) . &lt;AxesSubplot:xlabel=&#39;date&#39;&gt; . pytrends.suggestions(&quot;job&quot;) . [{&#39;mid&#39;: &#39;/m/04115t2&#39;, &#39;title&#39;: &#39;Job&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/m/07s_c&#39;, &#39;title&#39;: &#39;Unemployment&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/m/01rbb&#39;, &#39;title&#39;: &#39;Crime&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/m/019323&#39;, &#39;title&#39;: &#39;Duty&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/g/1211q_qf&#39;, &#39;title&#39;: &#39;job&#39;, &#39;type&#39;: &#39;Topic&#39;}] . dir(pytrends) related = pytrends.related_queries() related.keys() . dict_keys([&#39;work from home&#39;, &#39;unemployment&#39;, &#39;indeed jobs&#39;, &#39;work&#39;]) . from collections import deque import time def build_payload(pytrends, kw, nweeks=100, geo=&#39;US&#39;, cat=60): end_date = datetime.date.today() start_date = datetime.date.today() - datetime.timedelta(weeks=nweeks) timeframe = start_date.strftime(&quot;%Y-%m-%d&quot;) + &quot; &quot; + end_date.strftime(&quot;%Y-%m-%d&quot;) pytrends.build_payload([kw], cat=cat, timeframe=timeframe, geo=geo, gprop=&#39;&#39;) return pytrends sleeptime = 0.05 verbosity=0 max_keywords = 100 min_score = 70 keywords2try = deque() keyword_tups = deque() keyword_set = set() # seed the queue with a few terms keywords2try.appendleft((&quot;jobs&quot;, 101, 0, &#39;root&#39;, 101)) keywords2try.appendleft((&quot;unemployment&quot;, 101, 0, &#39;root&#39;, 101)) print(&quot;Start {0}&quot;.format(datetime.datetime.now())) while len(keywords2try) &gt; 0: if len(keyword_tups) &gt; max_keywords: break kw, score, level, parent, parent_score = keywords2try.pop() if kw in keyword_set: if verbosity &gt; 1: print(&quot;{0} already in set&quot;.format(kw)) continue if verbosity &gt; 0: print(&quot;Adding kw: {0}&quot;.format(kw)) # add to keyword_tups and keyword_set keyword_tups.appendleft((kw, score, level, parent, parent_score)) keyword_set.add(kw) pytrends = TrendReq(hl=&#39;en-US&#39;, tz=360) pytrends = build_payload(pytrends, kw) time.sleep(.5) related = pytrends.related_queries() if not related: if verbosity &gt; 1: print(&quot; t No related&quot;) continue if &#39;top&#39; not in related[kw].keys(): if verbosity &gt; 1: print(&quot; tNo top&quot;) continue df = related[kw][&#39;top&#39;] if df is None: if verbosity &gt; 1: print(&quot; ttop is empty&quot;) continue for tup in df.itertuples(): child_score = score * tup.value/float(100) if child_score &gt; min_score and tup.query not in keyword_set: if verbosity &gt; 1: print(&quot; tpushing {0}&quot;.format(tup.query)) keywords2try.appendleft((tup.query, child_score, level+1, kw, score)) print(&quot;Done {0}&quot;.format(datetime.datetime.now())) kw_df = pd.DataFrame(data=list(keyword_tups), columns=[&#39;keyword&#39;, &#39;score&#39;, &#39;level&#39;,&#39;parent&#39;, &#39;parent_score&#39;]).sort_values(by=&quot;score&quot;, ascending=False) kw_df.head() . Start 2020-10-29 21:12:27.838226 Done 2020-10-29 21:15:31.713361 . keyword score level parent parent_score . 82 jobs | 101.0 | 0 | root | 101.0 | . 54 hiring part time near me | 101.0 | 4 | part time jobs hiring | 101.0 | . 80 jobs near me | 101.0 | 1 | jobs | 101.0 | . 76 unemployment benefits | 101.0 | 2 | unemployment jobs | 101.0 | . 38 jobs hiring | 101.0 | 5 | job hiring near me | 101.0 | . print(&quot;Found {0} queries&quot;.format(kw_df.shape[0])) kw_df.sort_values(by=&quot;score&quot;, inplace=True) kw_df.to_csv(&quot;google_keywords.csv&quot;) . Found 83 queries . kw_df.head() . keyword score level parent parent_score . 22 temporary employment agencies near me | 70.192980 | 6 | employment agencies near me | 89.991000 | . 31 temporary employment agencies | 70.992900 | 5 | employment agencies | 99.990000 | . 16 staffing agency | 73.142685 | 7 | staffing near me | 94.990500 | . 0 express employment staffing | 73.952604 | 10 | express staffing | 73.952604 | . 4 express staffing | 73.952604 | 9 | employment express professionals | 85.991400 | .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/29/google.html",
            "relUrl": "/jupyter/2020/10/29/google.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Part 3 - Predicting the Jobs number - Course correction",
            "content": "I am making some changes in course due to new discoveries about the data and also refocusing effort. . The first changes should be clear, I am going to start using Jupyter notebooks as the format for my posts. The whole point is to show some of the methods and Jupyter notebooks provide a great way to do that. Thanks to the folks at fastai for their tools and tutorials to help me get started with this. . As with any project, I learn new things as I go along, especiall about the data. . I had said in an earlier post that I would pull the U. of Michigan survey data from the U. Mich site because there was a delay of a month for the data to be available on FRED. Well, it&#39;s not just FRED but overall. I believe there is some arrnagement with Bloomberg to not publish the data elsewhere for a month. I also believe that the overall consumer score can be scrapped from the U. Mich site, but not the results for the 5 questions that make up the overall score. So, the forward looking question results will have a delay of 1 month. Later I will give some notebook code to pull the data from the U. Michigan site automatically using Selenium with Python. And I guess I will use the overall score and get that from the U. Michigan site. . Another change is that I will add the OECD Consumer and Business confidence indexes to the predictor variables. I found how to get them via FRED. There are API&#39;s that should work directly with the OECD but if I can get it from FRED then that is much easier for me since I already know how to use one of the Python APIs for that. . Speaking of data in general, I found this note from the SF Fed, https://www.frbsf.org/education/publications/doctor-econ/2013/october/labor-market-indicators-monetary-policy-unemployment-rate/ Seems that I am using a lot of the data sources that the fed uses. I am going to add a few of the series mentioned in that document. . One other change not so much related to data but to priorities. I will spend less time on developing a standard time series regression model. I am more interested in what I can do with a machine learning ensemble method as well as with a neural net model. . I still have to deal with data at different frequencies. A big shout out to Tom Stark, https://www.philadelphiafed.org/research-and-data/research-contacts/stark, at the Philadelphia Fed for sending me some advice on how to deal with this. He says to use a Kalman filter. And Tom was nice enough to send me his notes on the subject, comprising of 4 PDF files and over 350 pages. Very nice Tom, what a real gem you are. . With that as an intro to Part 3, let&#39;s get started getting data from FRED. . import os import sys import datetime import inspect import pandas as pd import numpy as np from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd version 1.19.1 of np version 3.3.1 of mpl . Getting data from FRED using a Python API . The FRED website itself has a pretty nice GUI for plotting data. But I need to download the data so I can use Python or R tools to do the forecasting. . The FRED website gives links to projects you can use to access the FRED data from a variety of languages. See https://fred.stlouisfed.org/docs/api/fred/ I have used the python one called fredapi, https://github.com/mortada/fredapi, before and will use it here with one modification. I will add arguments to limit the series start and end dates. . To use any of the API tools you will need an API Key. See the instructions here, https://fred.stlouisfed.org/docs/api/api_key.html, to get one and get started. I keep mine in a file called &quot;fred_api_key&quot;. The fredapi module is in a sibling directory so I add that to my path before importing the module. . sys.path.append(&#39;../../fredapi&#39;) from fredapi import fred import fredapi Fred = fred.Fred(api_key_file=&quot;fred_api.key&quot;) print(fredapi.__version__) . 0.4.2 . How to identify the data on FRED . To acces the data for a series from FRED you need to know the series id. You can use the fredapi to search for a series given keywords. I already know the series ids I want to use and have them in a csv file, so let&#39;s read that in and see the ids and a brief description of the data for each one. . fred_ids = pd.read_csv(&#39;fred_ids.csv&#39;, index_col=None, sep=&#39;|&#39;) fred_ids . series_id description . 0 USPRIV | BLS private | . 1 NPPTTL | ADP | . 2 ICSA | Initial Claims | . 3 CCSA | Continued Claims | . 4 JTS1000JOL | Job Openings: Total Private | . 5 JTS1000HIL | Hires: Total Private | . 6 JTS1000TSL | Total Separations: Total Private | . 7 CSCICP03USM665S | OECD US Consumer Confidence | . 8 BSCICP03USM665S | OECD Business Confidence | . 9 UNRATE | Unemployment Rate | . 10 UNEMPLOY | Unemployment Level | . 11 CLF16OV | Civilian Labor Force | . 12 UEMP27OV | Unemployed for 27 Weeks | . 13 U6Rate | Unemployed plus Marginally Attached | . 14 CIVPART | Civilian Participation Rate | . 15 LNS12032194 | Part-Time for Econ Reasons | . Now we call the fredapi to get the observations for each series. I&#39;m only getting the data back to the start of 2007. I&#39;ll print out the number of observations we get of reach series and we will notice the different frequencies. . dfs = [] obs_start = &quot;2007-01-01&quot; obs_end = datetime.date.today() for ser in fred_ids.itertuples(): print(ser.series_id, end=&#39;, &#39;) try: all_df = Fred.get_series_all_releases(series_id=ser.series_id, observation_start=obs_start, observation_end=obs_end) tdf = all_df.sort_values(by=&quot;realtime_start&quot;, ascending=True).groupby(by=&quot;date&quot;).head(1) tdf[&#39;series_id&#39;] = ser.series_id tdf[&#39;date&#39;] = tdf[&#39;date&#39;].dt.date print(&quot;rows= {0}&quot;.format(tdf.shape[0])) dfs.append(tdf) except Exception as exc: print(exc) obs_df = pd.concat(dfs) obs_df = obs_df.merge(fred_ids, on=&#39;series_id&#39;) . USPRIV, rows= 165 NPPTTL, rows= 165 ICSA, rows= 721 CCSA, rows= 720 JTS1000JOL, rows= 164 JTS1000HIL, rows= 164 JTS1000TSL, rows= 164 CSCICP03USM665S, rows= 165 BSCICP03USM665S, rows= 165 UNRATE, rows= 165 UNEMPLOY, rows= 165 CLF16OV, rows= 165 UEMP27OV, rows= 165 U6Rate, rows= 165 CIVPART, rows= 165 LNS12032194, rows= 165 . Merge in the descriptions and have a look at the last few rows. . obs_df.sort_values(by=&quot;date&quot;, inplace=True) obs_df[&#39;dtime&#39;] = pd.to_datetime(obs_df[&#39;date&#39;]) print(obs_df.tail(4)) . realtime_start date value series_id description 1048 2020-10-15 2020-10-10 898000 ICSA Initial Claims 1049 2020-10-22 2020-10-17 787000 ICSA Initial Claims 1770 2020-10-29 2020-10-17 7.756e+06 CCSA Continued Claims 1050 2020-10-29 2020-10-24 751000 ICSA Initial Claims dtime 1048 2020-10-10 1049 2020-10-17 1770 2020-10-17 1050 2020-10-24 . Let&#39;s have a look at what the series look like over time. What a shock to the employment situation we&#39;ve had this year. Note that in many ways it dwarfs the impact from the 2008 financial crisis. Note how the business confidence index has bounced back. Not so much for consumer confidence. Our target is BLS private. Looks like the ADP data is tracking it really well and it comes out the week before. . def plot_observations(data_df, xcol, ycol, idcol, ncols, figwd, fight, ylim=(None, None), xtick_rot=0, sharex=True): ids = list(data_df[idcol].unique()) nrows = int(np.ceil(len(ids)/float(ncols))) fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=[figwd, fight], sharex=sharex) gdf = data_df.groupby(by=&#39;description&#39;) for i, (key, group) in enumerate(gdf): row = i // ncols col = i % ncols ax = axs[row][col] group.plot(ax=ax, kind=&#39;line&#39;, x=xcol, y=ycol,linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, ylim=ylim, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, title=key, legend=None, label=None, grid=True) ax.xaxis.set_tick_params(rotation=xtick_rot) return fig . ylim=(None, None) fig = plot_observations(data_df=obs_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None)) . Let&#39;s focus in on data from 2020. Notice how some of the series are almost current while others are lagging more than a month. . temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date.today() - datetime.timedelta(weeks=52)] temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date(year=2020, month=1, day=1)] ylim=(None, None) fig = plot_observations(data_df=temp_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None), xtick_rot=90) . That&#39;s all for now on FRED data. Next up, U. Michigan data and Google Trends data. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/29/fred.html",
            "relUrl": "/jupyter/2020/10/29/fred.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages - Brief How-to guide",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-4-88b922e30289&gt; in &lt;module&gt; 1 #collapse-hide 2 import pandas as pd -&gt; 3 import altair as alt ModuleNotFoundError: No module named &#39;altair&#39; . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . NameError Traceback (most recent call last) &lt;ipython-input-8-85b8363709cc&gt; in &lt;module&gt; 1 # single-value selection over [Major_Genre, MPAA_Rating] pairs 2 # use specific hard-wired values as the initial selected values -&gt; 3 selection = alt.selection_single( 4 name=&#39;Select&#39;, 5 fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], NameError: name &#39;alt&#39; is not defined . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/29/fastpages-howto.html",
            "relUrl": "/jupyter/2020/10/29/fastpages-howto.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Part 1 - Introduction and Overview",
            "content": "This is the first of a series of posts in which I plan to talk about a real world prediction problem, predicting the change in one of the monthly government macro economic values. In particular, I will look at predicting the change in the number of people working for US private employers in a given month. The number of people working in the US is compiled by the U.S Bureau of Labor Statistics, BLS, and is published monthly on the first Friday of each month. Of interest is not so much the value, or level as it is often call, but the change in the value. . The value published is an estimate of people employed in the prior month. For example, the value published on Friday 2020-10-02 gives an estimate of the number of people employed in September 2020. The number are published in a report currently called the Employment Situation. . There are actually many numbers in the report the BLS publishes with two main types, unemployment and jobs. The unemployment numbers come from a survey of households and the jobs numbers come from a survey of businesses. For the jobs number there is an aggregate value, i.e. for the entire US, and then breakdowns into private versus government and then many further breakdowns by industry type and other things. Suffice it to say that there is a lot of data in the report but I am only going to focus on the number of private jobs added in a month. The reason to focus on private and not the sum of private and government is becuase of some other data I want to use and it only covers private jobs. Why do this? For me it&#39;s to practice data science with real data on an important problem. The jobs number is one of the measures of the health of the economy. I&#39;m writing this in October 2020 with a presidential election coming next month. The report released on October 2nd got a lot of attention in the press. Both the stock and bond markets pay attention to the releases too. Back in 2007 I worked in the investment group of a large bank and I recall that when the numbers came out, usually around 8:30 AM New York time, they were announced for all the traders to hear. As evidence of the potential impact on the markets see here, here, here, or better still just google it. . You might ask WHy not just predict the S&amp;P 500 or something like that if we are interested in the markets. Well, we might get to that later. Some potential methods are described in the papers listed below. . Generative Adversarial Network for Stock Market price Prediction | EMPIRICAL ASSET PRICING VIA MACHINE LEARNING | . I plan to use a variety of techniques including Time Series Regression, Ensemble methods such as Random Forests, and hopefully Neural Networks. I&#39;m interested not only in how the perform out of sample but also how hard it is to build the models and how interpretable the model is. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/29/Intro_Overview.html",
            "relUrl": "/jupyter/2020/10/29/Intro_Overview.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Part 2 - Data Sources",
            "content": "Fred - the one stop shop . You can get the current value and some historical values from the BLS site, but I am going to use a different source, the St. Federal Reserve Economic Data site referred to as FRED . FRED is a great resource for all kinds of economic data and they have an API you can use to get the data. So I will get both the target variable, i.e. the private jobs number, as well as many of the predictor variables from FRED. . So what to use for predictor variables. One obvious choice is past values of the private jobs number. Past values of the unemployment numbers might also be helpful. There is another publication from the BLS called the JOLTS report for Job Openings and Labor Turnover Survey. This should get you to the latest release. I will use 3 values from that report, job openings, hirings and separations. We have to be careful with this one since it is published with a longer lag than the main jobs report. For example, the JOLTS report for August comes out in October whereas the Employment Situation report for August comes out in September. We can get the JOLTS data from FRED. . The BLS is part of the US Department of Labor. Another division of the Department of Labor produces weekly reports on initial and continued unemployment insurance claims. The claims data is published weekly so we will have to deal with data at different frequencies. And guess what ... we can get those from FRED. . There is a payroll processing company called ADP that produces something similar to the jobs numbers based on their proprietary data, but they don&#39;t have estimates for government jobs which is why I am only targeting the private number. More about the ADP estimates here. The ADP report comes out near the end of the month often just days before the BLS report. It is intended to be a predictor of the BLS values ... so we will see. And you can get the ADP values from FRED. . University of Michigan Consumer Sentiment . Note that most of the predictor variables I have listed so far are backward looking. They tell us about the sate of the world at some point in the past. It would be nice to have some forward looking indicators, like expectations. One source of that is the University of Michigan survey of consumer sentiment. The Michigan survey includes 3 questions that are forward looking, see the survey methodology document here. I think questions 2 through 4 look like they might be good to use so I will. Now we can get the aggregate value on Fred although with a one month lag. Anyway, I don&#39;t think the components are on FRED so I will download them from the Michigan website here. . Google Trends . And now for something completely different, Google Trends Part of the reason my interest in this project was revived was that I found a paper titled In Search of a Job: Forecasting Employment Growth Using Google Trends about using Google Trends to predict the employment numbers and I wanted to try it out myself. The SSRN version of the paper is here . The basic idea is that the relative demand for search terms such as &quot;jobs&quot; might be an indicator of how many people are looking for work. But what other terms should we use other than &quot;jobs&quot;. The papers suggests using Google&#39;s Keyword Planner to find other related terms. It should be fun. . Other Sources . I&#39;m sure there are many other variables I could use. A researcher from the The St. Louis Fed maintains a special set of over 100 monthly economic variables mainly for researchers to use. Read about it here. I don&#39;t want to use that since I believe it has the updated values of releases, not the original values. I want to use the values that first came out in case there were revisions. . The Conference Board is the source of some widely used economic data including the Leading Economic Indicators. I am not going to use those since I belive you have to subscribe to get them. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/29/DataSources.html",
            "relUrl": "/jupyter/2020/10/29/DataSources.html",
            "date": " • Oct 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jhmuller.github.io/job-forecasting/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jhmuller.github.io/job-forecasting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}