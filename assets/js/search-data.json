{
  
    
        "post0": {
            "title": "Part 4 - New data on Weekly Claims",
            "content": "I was scanning a WSJ article today on jobless claims, https://www.wsj.com/articles/weekly-jobless-claims-coronavirus-10-29-2020-11603921724?modtag=djemBestOfTheWeb The article started by claiming that initial claims filings had fallen to their lowest level since the pandemic began. This didn&#39;t seem interesting because according to FRED you could have been saying that since May. Seems like saying I am as old as I&#39;ve ever been. But then I noticed a chart showing claims under Regular state programs plus claims under special pandemic programs ... and for continued claims the special pandemic numbers were about the same as the regular ones. Uh oh, so the claims could be 2x what I would get from FRED and the normal BLS data. I could not find any data on these new plans on FRED but did find a spreadsheet on the Department of Labor website, see https://oui.doleta.gov/unemploy/DataDashboard.asp . The data might be available on the DOL or BLS site via some API. Not sure yet. In any case, in this post I will read in that data and plot it. Later I will figure out how to integreate it with the rest. . import os import sys import inspect import datetime import pandas as pd import numpy as np from plotnine import ggplot import matplotlib as mpl import xlrd import matplotlib.pyplot as plt . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd version 1.19.1 of np version 3.3.1 of mpl version 1.2.0 of xlrd . Read . Read in the spreadsheet and get rid of rows with no data for Rptdate . states_df = pd.read_excel(&quot;weekly_pandemic_claims.xlsx&quot;) states_df = states_df[states_df[&#39;Rptdate&#39;].notna()] states_df.tail(3) . State Rptdate PUA IC Reflect Date PUA CC PEUC CC . 1534 WV | 2020-10-17 | 4821.0 | NaT | NaN | NaN | . 1535 WI | 2020-10-17 | 2254.0 | NaT | NaN | NaN | . 1536 WY | 2020-10-17 | 519.0 | NaT | NaN | NaN | . Plot . For now I&#39;m not interested in the state breakdown so I&#39;ll aggregate to US level and also separte out Initial Claims, IC, from Continued Claims, CC. Partly because the CC values are about 10x the IC values so easier to see and check the IC data if plotted alone. Also because there is data on 2 different special programs for CC. . us_df = states_df.drop(labels=[&quot;State&quot;], axis=1).groupby(by=&#39;Rptdate&#39;) us_df = us_df.sum(min_count=1) us_df.index = pd.to_datetime(us_df.index).rename(&quot;date&quot;) print(us_df.head()) ic_df = us_df[&#39;PUA IC&#39;] cc_df = us_df.drop(&#39;PUA IC&#39;, axis=1) cc_df[&#39;CC Tot&#39;] = cc_df.sum(axis=1, skipna=False) cc_df.tail() dfs = {&quot;IC&quot;:ic_df, &quot;CC&quot;:cc_df} . PUA IC PUA CC PEUC CC date 2020-04-04 31949.0 52494.0 0.0 2020-04-11 52126.0 68897.0 3802.0 2020-04-18 224990.0 210939.0 31392.0 2020-04-25 833083.0 1088281.0 59760.0 2020-05-02 1051345.0 3498790.0 86972.0 . figwd = 12 fight= 6 fig, axs = plt.subplots(nrows=2, ncols=1, figsize=[figwd, fight], sharex=True) for i, key in enumerate(dfs.keys()): ax = axs[i] dfs[key].plot(ax=ax, kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, title=key, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, legend=True, label=None, grid=True) . The regular IC values have been about 750K so this new program is about half that. The regular CC values have been about 7.5 million so this new data is about the same. Later I might combine the regular and special data, but for now it&#39;s enough just to have the special data. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/04/weekly_claims.html",
            "relUrl": "/jupyter/2020/11/04/weekly_claims.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Part 5 - University of Michigan Consumer Sentiment data",
            "content": "I think they do a great job of documentation at the site. . Also have a look at the siki page about the survey and the data. In particular the references to this and indirectly this about Thomson Reuters and others giving investors early access for a fee. . In any case, you can see the question asking sheet and more. I am interested in the responses about expectations. There seems like a lot of great information there but I am going to focus on the data on the components of the index and in particular the 3 questions related to expectations . x2= PEXP_R = &quot;Now looking ahead--do you think that a year from now you (and your family living there) will be better off financially, or worse off, or just about the same as now?&quot; | x3= BUS12_R = &quot;Now turning to business conditions in the country as a whole--do you think that during the next twelve months we&#39;ll have good times financially, or bad times, or what?&quot; | x4= BUS5_R = &quot;Looking ahead, which would you say is more likely--that in the country as a whole we&#39;ll have continuous good times during the next five years or so, or that we will have periods of widespread unemployment or depression, or what?&quot; | . They compute a weighted average called the Index of Consumer Expectations, or ICE, as follows . ICE = ((x2 + x3 + x4)/4.1134) + 2 [1](#myfootnote1) . I&#39;m sure somewhere in the documentation they explain why they divide by 4.1134 and add 2 but I&#39;ll probably just use the 3 individual variables. We&#39;ll see. . 1: Notice I put in the parentesis to avoid issues like this. If the link doesn&#39;t work search for &quot;The Math equation that stumped the internet&quot;) . First the bolerplate Python I use for most notebooks. It&#39;s evolving. . import os import sys import datetime import time import re import inspect import pandas as pd from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt import selenium from selenium import webdriver from selenium.webdriver.support.ui import Select import pathlib . I like to see version numbers for modules ... and for Python. . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 2.2.1 of re version 1.1.3 of pd version 3.3.2 of mpl version 3.141.0 of selenium version 3.14.1 of webdriver . Selenium to automate downloading . It&#39;s easy to get the data from the site, can be done via the following steps. . Navigate to https://data.sca.isr.umich.edu/data-archive/mine.php [2](#myfootnote1) | For Table select Table 5: Components of the Index of Consumer Sentiment. | Click Comma-Separated(CSV) under format and it should start downloading. | But I want to automate the process and we will using Selenium. Selenium allows us to control a web browser from Pyhon so I can use Python to execute all the steps above. . 2: Note that if you start at the main site you might need to click on Data and select Time Series in the dropdown. . Download Directory . I want to have the data downloaded into a subdirectory below here rather than the default Downloads directory. We can arrange that using ChromeOptions. . temp_dir = &quot;./data/temp&quot; if not os.path.isdir(temp_dir): os.mkdir(temp_dir) prefs = {&quot;download.default_directory&quot; : os.path.abspath(temp_dir)} options = webdriver.ChromeOptions() options.add_experimental_option(&quot;prefs&quot;,prefs) options.add_argument(&quot;download.default_directory=&quot;+os.path.abspath(temp_dir)) . Step 1, going to the site . You can run Selenium in what is called headless mode where you don&#39;t get a window for the browser. But I still find it cool to see a new browser pop up so I won&#39;t use that now. And it&#39;s nice in the early stages to see what is actually happening. Probably will change once the novelty wears off. . chromedriver . With Selenium you have a choice of which browser to use, e.g. Chrome, Firefox et cetera. I&#39;ll use Chrome. Whatever the choice we&#39;ll need a driver. Google it if you are interested. I downloaded the chromedriver and put it in a sibling directory under chromedriver_win32 so it is easy to find for others projects using Selenium. . Here I am doing step 1 from above. . chromedriver_path = os.path.join(&#39;../../chromedriver_win32/chromedriver.exe&#39;) driver = webdriver.Chrome(executable_path=chromedriver_path, options=options) url = &quot;https://data.sca.isr.umich.edu/data-archive/mine.php&quot; driver.get(url) . Step 2: selecting Table 5 . There are plenty of tutorials on using Selenium to make selections and click buttons. I can&#39;t remember which ones I used. Most likely I just googled what I wanted to do. . In any case, here I am finding the selection section on the page and choosing option 5. . select_element = Select(driver.find_element_by_css_selector(&quot;select&quot;)) # this will print out strings available for selection on select_element, used in visible text below select_element.select_by_index(5) . Identifying the newly downloaded file . In the final step I will download a file with the data into the ./data directory. But I don&#39;t know exactly what the file name will be yet, and also if I pull it down multiple times the file will get names lik &quot;file.csv&quot;, &quot;file(2).csv&quot; et cetera. I want to make sure I can idendify the the latest one I downloaded. I wrote the function below to help with that. It is pobably way more complicated than it needs to be. . In any case, the idea is to start with a list of all the files in the directory before I asked for the download. Then, I look for new files with the appropriate extension, in this case &quot;.csv&quot;. Once I find a file that was not there before, that is the one that just got donloaded. In case the file takes some time to download I will loop looking for a new file, but I have a limit on how long I will do this looping, My default wait limit is 10 seconds. . def get_downloaded_fpath(dir=None, files_before=None, file_ext=&quot;.csv&quot;, max_wait = 10, verbose=True): import time done = False start_time = datetime.datetime.now() while not done: files_after = set(os.listdir(dir)) new_files = files_after.difference(set(files_before)) print(new_files) for fname in new_files: if os.path.splitext(fname)[1] == file_ext: return (os.path.join(dir, fname)) cur_time = datetime.datetime.now() if (cur_time - start_time).seconds &gt; 10: return None time.sleep(0.5) . Step 3: Selecting Comma-Separated(CSV) . Now I look for the &quot;Comma-Separated(CSV)&quot; button on the web page and click it. Then I use the above function to find the path to the new file. . elements = driver.find_elements_by_name(&quot;format&quot;) button = None for e in elements: if e.get_property(&quot;value&quot;) == &#39;Comma-Separated (CSV)&#39;: button = e break if not button: raise RuntimeError(&quot;Error downloading Consumer Sentiment data from {0}&quot;.format(url)) files_before = set(os.listdir(temp_dir)) button.click() fpath = get_downloaded_fpath(dir=temp_dir, files_before=files_before, file_ext=&quot;.csv&quot;, max_wait=10) if not fpath: raise Exception(&quot;No downloaded file found!&quot;) print(&quot;new file: {0}&quot;.format(fpath)) . set() set() set() {&#39;sca-table5-on-2020-Nov-04.csv.crdownload&#39;} {&#39;sca-table5-on-2020-Nov-04.csv&#39;} new file: ./data/temp sca-table5-on-2020-Nov-04.csv . Shotdown the browser . driver.quit() . Read in the new file | print the volumns | Drop the last column, don&#39;t know what it is | Make a datetime column from the month and year and drop the month and year columns. | make the datetime column the index | pick out th expectation columns as the ones that have either &quot;expected&quot; or &quot;Business Condition&quot; in the name. drop all but the expected columns | . | . df = pd.read_csv(fpath, skiprows=1) print(df.columns) df.drop(df.columns[-1], inplace=True, axis=1) df[&#39;Datetime&#39;] = (100*100*df[&#39;Year&#39;] + 100*df[&quot;Month&quot;] + 15).astype(str) df[&#39;Datetime&#39;] = pd.to_datetime(df[&#39;Datetime&#39;]) df.drop([&quot;Month&quot;, &quot;Year&quot;], inplace=True, axis=1) df.set_index(&quot;Datetime&quot;, inplace=True) exp_columns = [c for c in df.columns if re.search(&quot;Expected|Business Condition&quot;, c)] df = df[exp_columns] df.head() . Index([&#39;Month&#39;, &#39;Year&#39;, &#39;Personal Finance Current&#39;, &#39;Personal Finance Expected&#39;, &#39;Business Condition 12 Months&#39;, &#39;Business Condition 5 Years&#39;, &#39;Buying Conditions&#39;, &#39;Current Index&#39;, &#39;Expected Index&#39;, &#39;Unnamed: 9&#39;], dtype=&#39;object&#39;) . Personal Finance Expected Business Condition 12 Months Business Condition 5 Years Expected Index . Datetime . 2008-01-15 116 | 68 | 88 | 68.1 | . 2008-02-15 112 | 54 | 83 | 62.4 | . 2008-03-15 112 | 46 | 81 | 60.1 | . 2008-04-15 100 | 40 | 71 | 53.3 | . 2008-05-15 98 | 36 | 68 | 51.1 | . Plot all the expected series . df.plot(figsize=[16,4], marker=&#39;.&#39;, grid=True) plt.gca().set_ylim(bottom=0) . (0.0, 142.3) . Save the dataframe as &quot;umich_exp.csv&quot; . try: out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) df.to_csv(os.path.join(out_dir, &quot;umich_exp.csv&quot;)) except Exception as exc: print(exc) raise Exception(exc) . delete all the files in the temp directory | delete the temp directory | . try: if os.path.isdir(temp_dir): temp_files = os.listdir(temp_dir) # remove all the temp files for tfile in temp_files: os.remove(os.path.join(temp_dir, tfile)) # remove the temp_dir if os.path.isdir(temp_dir): pathlib.Path.rmdir(pathlib.Path(temp_dir)) except Exception as exc: print(exc) raise Exception(exc) . import datetime print(datetime.datetime.now()) . 2020-11-04 23:02:25.934200 .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/04/umich_cs.html",
            "relUrl": "/jupyter/2020/11/04/umich_cs.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Part 6 - Google Trends data",
            "content": "import os import sys import datetime import inspect import pandas as pd import pytrends from pytrends.request import TrendReq . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd . pytrends = TrendReq(hl=&#39;en-US&#39;, tz=360) kw_list = [ &quot;work from home&quot;, &quot;unemployment&quot;, &quot;indeed jobs&quot;, &quot;work&quot;] # &quot;unemployment&quot;, &quot;job&quot; &quot; ui online&quot; #,&#39;hire&#39;, &quot;Jobs&quot;, &quot;vacancy&quot;, &quot;employment&quot;, &quot;unemployment&quot;, &quot;benefits&quot;] &quot;njuifile&quot; nweeks = 200 end_date = datetime.date.today().strftime(&quot;%Y-%m-%d&quot;) start_date = (datetime.date.today() - datetime.timedelta(weeks=nweeks)).strftime(&quot;%Y-%m-%d&quot;) timeframe = start_date+ &quot; &quot; + end_date print(timeframe) geo = &#39;US&#39; pytrends.build_payload(kw_list, cat=60, timeframe=timeframe, geo=geo, gprop=&#39;&#39;) . 2017-01-04 2020-11-04 . df = pytrends.interest_over_time() . df.plot(figsize=[12,6], grid=True, marker=&#39;o&#39;) . &lt;AxesSubplot:xlabel=&#39;date&#39;&gt; . pytrends.suggestions(&quot;job&quot;) . [{&#39;mid&#39;: &#39;/m/04115t2&#39;, &#39;title&#39;: &#39;Job&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/m/07s_c&#39;, &#39;title&#39;: &#39;Unemployment&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/m/01rbb&#39;, &#39;title&#39;: &#39;Crime&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/m/019323&#39;, &#39;title&#39;: &#39;Duty&#39;, &#39;type&#39;: &#39;Topic&#39;}, {&#39;mid&#39;: &#39;/g/1211q_qf&#39;, &#39;title&#39;: &#39;job&#39;, &#39;type&#39;: &#39;Topic&#39;}] . dir(pytrends) related = pytrends.related_queries() related.keys() . dict_keys([&#39;work from home&#39;, &#39;unemployment&#39;, &#39;indeed jobs&#39;, &#39;work&#39;]) . from collections import deque import time def build_payload(pytrends, kw, nweeks=100, geo=&#39;US&#39;, cat=60): end_date = datetime.date.today() start_date = datetime.date.today() - datetime.timedelta(weeks=nweeks) timeframe = start_date.strftime(&quot;%Y-%m-%d&quot;) + &quot; &quot; + end_date.strftime(&quot;%Y-%m-%d&quot;) pytrends.build_payload([kw], cat=cat, timeframe=timeframe, geo=geo, gprop=&#39;&#39;) return pytrends sleeptime = 0.05 verbosity = 1 max_keywords = 100 min_score = 70 keywords2try = deque() keyword_tups = deque() keyword_set = set() # seed the queue with a few terms keywords2try.appendleft((&quot;jobs&quot;, 101, 0, &#39;root&#39;, 101)) keywords2try.appendleft((&quot;unemployment&quot;, 101, 0, &#39;root&#39;, 101)) print(&quot;Start {0}&quot;.format(datetime.datetime.now())) while len(keywords2try) &gt; 0: if len(keyword_tups) &gt; max_keywords: break kw, score, level, parent, parent_score = keywords2try.pop() if kw in keyword_set: if verbosity &gt; 1: print(&quot;{0} already in set&quot;.format(kw)) continue if verbosity &gt; 0: print(&quot;&lt;kw#{1}=&#39;{0}&#39;&#39;&gt;, &quot;.format(kw, len(keyword_tups)+1), end=&#39;&#39;) # add to keyword_tups and keyword_set keyword_tups.appendleft((kw, score, level, parent, parent_score)) keyword_set.add(kw) pytrends = TrendReq(hl=&#39;en-US&#39;, tz=360) pytrends = build_payload(pytrends, kw) time.sleep(.5) related = pytrends.related_queries() if not related: if verbosity &gt; 1: print(&quot; t No related&quot;) continue if &#39;top&#39; not in related[kw].keys(): if verbosity &gt; 1: print(&quot; tNo top&quot;) continue df = related[kw][&#39;top&#39;] if df is None: if verbosity &gt; 1: print(&quot; ttop is empty&quot;) continue for tup in df.itertuples(): child_score = score * tup.value/float(100) if child_score &gt; min_score and tup.query not in keyword_set: if verbosity &gt; 1: print(&quot; tpushing {0}&quot;.format(tup.query)) keywords2try.appendleft((tup.query, child_score, level+1, kw, score)) print(&quot;Done {0}&quot;.format(datetime.datetime.now())) kw_df = pd.DataFrame(data=list(keyword_tups), columns=[&#39;keyword&#39;, &#39;score&#39;, &#39;level&#39;,&#39;parent&#39;, &#39;parent_score&#39;]).sort_values(by=&quot;score&quot;, ascending=False) kw_df.head() . Start 2020-11-04 23:01:31.677058 &lt;kw#1=&#39;jobs&#39;&#39;&gt;, &lt;kw#2=&#39;unemployment&#39;&#39;&gt;, &lt;kw#3=&#39;jobs near me&#39;&#39;&gt;, &lt;kw#4=&#39;jobs indeed&#39;&#39;&gt;, &lt;kw#5=&#39;unemployment jobs&#39;&#39;&gt;, &lt;kw#6=&#39;part time unemployment&#39;&#39;&gt;, &lt;kw#7=&#39;jobs hiring near me&#39;&#39;&gt;, &lt;kw#8=&#39;indeed jobs near me&#39;&#39;&gt;, &lt;kw#9=&#39;unemployment login&#39;&#39;&gt;, &lt;kw#10=&#39;unemployment office&#39;&#39;&gt;, &lt;kw#11=&#39;unemployment benefits&#39;&#39;&gt;, &lt;kw#12=&#39;file for unemployment&#39;&#39;&gt;, &lt;kw#13=&#39;florida unemployment&#39;&#39;&gt;, &lt;kw#14=&#39;texas unemployment&#39;&#39;&gt;, &lt;kw#15=&#39;unemployment rate&#39;&#39;&gt;, &lt;kw#16=&#39;tn.gov jobs&#39;&#39;&gt;, &lt;kw#17=&#39;unemployment for part time&#39;&#39;&gt;, &lt;kw#18=&#39;unemployment part time work&#39;&#39;&gt;, &lt;kw#19=&#39;hiring jobs near me part time&#39;&#39;&gt;, &lt;kw#20=&#39;jobs near me hiring part time&#39;&#39;&gt;, &lt;kw#21=&#39;jobs hiring near me indeed&#39;&#39;&gt;, &lt;kw#22=&#39;unemployment wi&#39;&#39;&gt;, &lt;kw#23=&#39;wi unemployment login&#39;&#39;&gt;, &lt;kw#24=&#39;wisconsin unemployment login&#39;&#39;&gt;, &lt;kw#25=&#39;unemployment office near me&#39;&#39;&gt;, &lt;kw#26=&#39;part time unemployment benefits&#39;&#39;&gt;, &lt;kw#27=&#39;can part time file for unemployment&#39;&#39;&gt;, &lt;kw#28=&#39;how to file for unemployment&#39;&#39;&gt;, &lt;kw#29=&#39;careersource&#39;&#39;&gt;, &lt;kw#30=&#39;texas unemployment benefits&#39;&#39;&gt;, &lt;kw#31=&#39;tn.gov jobs unemployment&#39;&#39;&gt;, &lt;kw#32=&#39;unemployment for part time employees&#39;&#39;&gt;, &lt;kw#33=&#39;unemployment for part time workers&#39;&#39;&gt;, &lt;kw#34=&#39;can you get unemployment if you work part time&#39;&#39;&gt;, &lt;kw#35=&#39;places hiring near me&#39;&#39;&gt;, &lt;kw#36=&#39;places hiring near me part time&#39;&#39;&gt;, &lt;kw#37=&#39;jobs near me hiring now&#39;&#39;&gt;, &lt;kw#38=&#39;now hiring near me&#39;&#39;&gt;, &lt;kw#39=&#39;part time jobs near me hiring now&#39;&#39;&gt;, &lt;kw#40=&#39;jobs hiring near me part time no experience&#39;&#39;&gt;, &lt;kw#41=&#39;job hiring near me&#39;&#39;&gt;, &lt;kw#42=&#39;part time jobs hiring near me&#39;&#39;&gt;, &lt;kw#43=&#39;unemployment wisconsin&#39;&#39;&gt;, &lt;kw#44=&#39;wi dwd unemployment&#39;&#39;&gt;, &lt;kw#45=&#39;dwd unemployment wi login&#39;&#39;&gt;, &lt;kw#46=&#39;can you file for unemployment if you work part time&#39;&#39;&gt;, &lt;kw#47=&#39;can part time employees file for unemployment&#39;&#39;&gt;, &lt;kw#48=&#39;careersource florida&#39;&#39;&gt;, &lt;kw#49=&#39;are part time employees eligible for unemployment&#39;&#39;&gt;, &lt;kw#50=&#39;unemployment benefits for part time workers&#39;&#39;&gt;, &lt;kw#51=&#39;can you still get unemployment if you work part time&#39;&#39;&gt;, &lt;kw#52=&#39;places hiring part time near me&#39;&#39;&gt;, &lt;kw#53=&#39;places near me that are hiring&#39;&#39;&gt;, &lt;kw#54=&#39;places that are hiring&#39;&#39;&gt;, &lt;kw#55=&#39;part time jobs near me&#39;&#39;&gt;, &lt;kw#56=&#39;part time jobs hiring now near me&#39;&#39;&gt;, &lt;kw#57=&#39;jobs near me hiring&#39;&#39;&gt;, &lt;kw#58=&#39;jobs now hiring&#39;&#39;&gt;, &lt;kw#59=&#39;now hiring jobs near me&#39;&#39;&gt;, &lt;kw#60=&#39;jobs hiring near me now&#39;&#39;&gt;, &lt;kw#61=&#39;careersource central florida&#39;&#39;&gt;, &lt;kw#62=&#39;are part time employees eligible for unemployment benefits&#39;&#39;&gt;, &lt;kw#63=&#39;jobs hiring near me part time&#39;&#39;&gt;, &lt;kw#64=&#39;jobs hiring part time&#39;&#39;&gt;, &lt;kw#65=&#39;hiring near me part time&#39;&#39;&gt;, &lt;kw#66=&#39;jobs near me part time hiring&#39;&#39;&gt;, &lt;kw#67=&#39;hiring now near me&#39;&#39;&gt;, &lt;kw#68=&#39;careersource north central florida&#39;&#39;&gt;, &lt;kw#69=&#39;jobs near me part time&#39;&#39;&gt;, &lt;kw#70=&#39;part time hiring near me&#39;&#39;&gt;, &lt;kw#71=&#39;hiring near me&#39;&#39;&gt;, &lt;kw#72=&#39;hiring part time near me&#39;&#39;&gt;, &lt;kw#73=&#39;jobs hiring part time near me&#39;&#39;&gt;, &lt;kw#74=&#39;part time jobs hiring&#39;&#39;&gt;, &lt;kw#75=&#39;hiring jobs near me&#39;&#39;&gt;, &lt;kw#76=&#39;part time jobs&#39;&#39;&gt;, &lt;kw#77=&#39;part time jobs near me hiring&#39;&#39;&gt;, &lt;kw#78=&#39;jobs hiring now near me&#39;&#39;&gt;, &lt;kw#79=&#39;hiring part time jobs near me&#39;&#39;&gt;, Done 2020-11-04 23:03:20.569412 . keyword score level parent parent_score . 78 jobs | 101.0 | 0 | root | 101.0 | . 77 unemployment | 101.0 | 0 | root | 101.0 | . 34 dwd unemployment wi login | 101.0 | 4 | wi unemployment login | 101.0 | . 44 places hiring near me | 101.0 | 4 | hiring jobs near me part time | 101.0 | . 56 wi unemployment login | 101.0 | 3 | unemployment login | 101.0 | . print(&quot;Found {0} queries&quot;.format(kw_df.shape[0])) kw_df.sort_values(by=&quot;score&quot;, inplace=True) out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) try: kw_df.to_csv(os.path.join(out_dir, &quot;google_keywords.csv&quot;)) except Exception as exc: print(exc) raise(exc) . Found 79 queries . import datetime print(datetime.datetime.now()) . 2020-11-04 23:03:20.812273 .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/04/google.html",
            "relUrl": "/jupyter/2020/11/04/google.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Part 3 - Predicting the Jobs number - Course correction",
            "content": "I am making some changes in course due to new discoveries about the data and also refocusing effort. . The first changes should be clear, I am going to start using Jupyter notebooks as the format for my posts. The whole point is to show some of the methods and Jupyter notebooks provide a great way to do that. Thanks to the folks at fastai for their tools and tutorials to help me get started with this. . As with any project, I learn new things as I go along, especiall about the data. . I had said in an earlier post that I would pull the U. of Michigan survey data from the U. Mich site because there was a delay of a month for the data to be available on FRED. Well, it&#39;s not just FRED but overall. I believe there is some arrnagement with Bloomberg to not publish the data elsewhere for a month. I also believe that the overall consumer score can be scrapped from the U. Mich site, but not the results for the 5 questions that make up the overall score. So, the forward looking question results will have a delay of 1 month. Later I will give some notebook code to pull the data from the U. Michigan site automatically using Selenium with Python. And I guess I will use the overall score and get that from the U. Michigan site. . Another change is that I will add the OECD Consumer and Business confidence indexes to the predictor variables. I found how to get them via FRED. There are API&#39;s that should work directly with the OECD but if I can get it from FRED then that is much easier for me since I already know how to use one of the Python APIs for that. . Speaking of data in general, I found this note from the SF Fed, https://www.frbsf.org/education/publications/doctor-econ/2013/october/labor-market-indicators-monetary-policy-unemployment-rate/ Seems that I am using a lot of the data sources that the fed uses. I am going to add a few of the series mentioned in that document. . One other change not so much related to data but to priorities. I will spend less time on developing a standard time series regression model. I am more interested in what I can do with a machine learning ensemble method as well as with a neural net model. . I still have to deal with data at different frequencies. A big shout out to Tom Stark, https://www.philadelphiafed.org/research-and-data/research-contacts/stark, at the Philadelphia Fed for sending me some advice on how to deal with this. He says to use a Kalman filter. And Tom was nice enough to send me his notes on the subject, comprising of 4 PDF files and over 350 pages. Very nice Tom, what a real gem you are. . With that as an intro to Part 3, let&#39;s get started getting data from FRED. . import os import sys import datetime import inspect import pandas as pd import numpy as np from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd version 1.19.2 of np version 3.3.2 of mpl . Getting data from FRED using a Python API . The FRED website itself has a pretty nice GUI for plotting data. But I need to download the data so I can use Python or R tools to do the forecasting. . The FRED website gives links to projects you can use to access the FRED data from a variety of languages. See https://fred.stlouisfed.org/docs/api/fred/ I have used the python one called fredapi, https://github.com/mortada/fredapi, before and will use it here with one modification. I will add arguments to limit the series start and end dates. . To use any of the API tools you will need an API Key. See the instructions here, https://fred.stlouisfed.org/docs/api/api_key.html, to get one and get started. I keep mine in a file called &quot;fred_api_key&quot;. The fredapi module is in a sibling directory so I add that to my path before importing the module. . sys.path.append(&#39;../../fredapi&#39;) from fredapi import fred import fredapi Fred = fred.Fred(api_key_file=&quot;fred_api.key&quot;) print(fredapi.__version__) . 0.4.2 . How to identify the data on FRED . To acces the data for a series from FRED you need to know the series id. You can use the fredapi to search for a series given keywords. I already know the series ids I want to use and have them in a csv file, so let&#39;s read that in and see the ids and a brief description of the data for each one. . fred_ids = pd.read_csv(&#39;fred_ids.csv&#39;, index_col=None, sep=&#39;|&#39;) fred_ids . series_id description . 0 USPRIV | BLS private | . 1 NPPTTL | ADP | . 2 ICSA | Initial Claims | . 3 CCSA | Continued Claims | . 4 JTS1000JOL | Job Openings: Total Private | . 5 JTS1000HIL | Hires: Total Private | . 6 JTS1000TSL | Total Separations: Total Private | . 7 CSCICP03USM665S | OECD US Consumer Confidence | . 8 BSCICP03USM665S | OECD Business Confidence | . 9 UNRATE | Unemployment Rate | . 10 UNEMPLOY | Unemployment Level | . 11 CLF16OV | Civilian Labor Force | . 12 UEMP27OV | Unemployed for 27 Weeks | . 13 U6Rate | Unemployed plus Marginally Attached | . 14 CIVPART | Civilian Participation Rate | . 15 LNS12032194 | Part-Time for Econ Reasons | . Now we call the fredapi to get the observations for each series. I&#39;m only getting the data back to the start of 2007. I&#39;ll print out the number of observations we get of reach series and we will notice the different frequencies. . FRED and ALFRED . Have I not mentioned ALFRED yet? The goverment releases, like Nonfarm payrolls, usually get updated in subsequent months, that is, the initial estimates might be revised up or down. FRED just has the latest update. You probably want to get the value as released or perhaps the value that was known on a given date. You get that from ALFRED. Notice in the call below I get all_releases and then manipulate the data to only keep the earliest one, or the original. The API is the same for FRED and ALFRED, but if you get other than the most current data you will be getting it from ALFRED. . dfs = [] obs_start = &quot;2007-01-01&quot; obs_end = datetime.date.today() for ser in fred_ids.itertuples(): print(ser.series_id, end=&#39;, &#39;) try: all_df = Fred.get_series_all_releases(series_id=ser.series_id, observation_start=obs_start, observation_end=obs_end) tdf = all_df.sort_values(by=&quot;realtime_start&quot;, ascending=True).groupby(by=&quot;date&quot;).head(1) tdf[&#39;series_id&#39;] = ser.series_id tdf[&#39;date&#39;] = tdf[&#39;date&#39;].dt.date print(&quot;rows= {0}&quot;.format(tdf.shape[0])) dfs.append(tdf) except Exception as exc: print(exc) obs_df = pd.concat(dfs) obs_df = obs_df.merge(fred_ids, on=&#39;series_id&#39;) . USPRIV, rows= 165 NPPTTL, rows= 166 ICSA, rows= 721 CCSA, rows= 720 JTS1000JOL, rows= 164 JTS1000HIL, rows= 164 JTS1000TSL, rows= 164 CSCICP03USM665S, rows= 165 BSCICP03USM665S, rows= 165 UNRATE, rows= 165 UNEMPLOY, rows= 165 CLF16OV, rows= 165 UEMP27OV, rows= 165 U6Rate, rows= 165 CIVPART, rows= 165 LNS12032194, rows= 165 . Merge in the descriptions and have a look at the last few rows. . obs_df.sort_values(by=&quot;date&quot;, inplace=True) obs_df[&#39;dtime&#39;] = pd.to_datetime(obs_df[&#39;date&#39;]) print(obs_df.tail(4)) . realtime_start date value series_id description 1049 2020-10-15 2020-10-10 898000 ICSA Initial Claims 1771 2020-10-29 2020-10-17 7.756e+06 CCSA Continued Claims 1050 2020-10-22 2020-10-17 787000 ICSA Initial Claims 1051 2020-10-29 2020-10-24 751000 ICSA Initial Claims dtime 1049 2020-10-10 1771 2020-10-17 1050 2020-10-17 1051 2020-10-24 . Not for HFT . A quick aside about how quickly the data is available on FRED. The Claims data was released at 8:30 ET on Thursday and I first saw it on FRED and in my download about 10-15 minutes later. While not fast enough for high frequency trading, still very good. Also let me say how helpful the folks at FRED have been. I&#39;ve emailed them a number of questions and they typically get back to me in a day or so. FRED and ALFRED are great resources. . Plot of all the data . Let&#39;s have a look at what the series look like over time. What a shock to the employment situation we&#39;ve had this year. Note that in many ways it dwarfs the impact from the 2008 financial crisis. Note how the business confidence index has bounced back. Not so much for consumer confidence. Our target is BLS private. Looks like the ADP data is tracking it really well and it comes out the week before. . def plot_observations(data_df, xcol, ycol, idcol, ncols, figwd, fight, ylim=(None, None), xtick_rot=0, sharex=True): ids = list(data_df[idcol].unique()) nrows = int(np.ceil(len(ids)/float(ncols))) fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=[figwd, fight], sharex=sharex) gdf = data_df.groupby(by=&#39;description&#39;) for i, (key, group) in enumerate(gdf): row = i // ncols col = i % ncols ax = axs[row][col] group.plot(ax=ax, kind=&#39;line&#39;, x=xcol, y=ycol,linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, ylim=ylim, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, title=key, legend=None, label=None, grid=True) ax.xaxis.set_tick_params(rotation=xtick_rot) return fig . ylim=(None, None) fig = plot_observations(data_df=obs_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None)) . Plot of the last year . Let&#39;s focus in on data from 2020. Notice how some of the series are almost current while others are lagging more than a month. . temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date.today() - datetime.timedelta(weeks=52)] temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date(year=2020, month=1, day=1)] ylim=(None, None) fig = plot_observations(data_df=temp_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None), xtick_rot=90) . save the dataframe into the output directory . out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) obs_df.to_csv(os.path.join(out_dir, &quot;fred.csv&quot;)) . That&#39;s all for now on FRED data. Next up, U. Michigan data and Google Trends data. . import datetime print(datetime.datetime.now()) . 2020-11-04 23:01:51.517899 .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/04/fred.html",
            "relUrl": "/jupyter/2020/11/04/fred.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages - Brief How-to guide",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-4-88b922e30289&gt; in &lt;module&gt; 1 #collapse-hide 2 import pandas as pd -&gt; 3 import altair as alt ModuleNotFoundError: No module named &#39;altair&#39; . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . NameError Traceback (most recent call last) &lt;ipython-input-8-85b8363709cc&gt; in &lt;module&gt; 1 # single-value selection over [Major_Genre, MPAA_Rating] pairs 2 # use specific hard-wired values as the initial selected values -&gt; 3 selection = alt.selection_single( 4 name=&#39;Select&#39;, 5 fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], NameError: name &#39;alt&#39; is not defined . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/04/fastpages-howto.html",
            "relUrl": "/jupyter/2020/11/04/fastpages-howto.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Part 1 - Introduction and Overview",
            "content": "This is the first of a series of posts in which I plan to talk about a real world prediction problem, predicting the change in one of the monthly government macro economic values. In particular, I will look at predicting the change in the number of people working for US private employers in a given month. The number of people working in the US is compiled by the U.S Bureau of Labor Statistics, BLS, and is published monthly on the first Friday of each month. Of interest is not so much the value, or level as it is often call, but the change in the value. . The value published is an estimate of people employed in the prior month. For example, the value published on Friday 2020-10-02 gives an estimate of the number of people employed in September 2020. The number are published in a report currently called the Employment Situation. . There are actually many numbers in the report the BLS publishes with two main types, unemployment and jobs. The unemployment numbers come from a survey of households and the jobs numbers come from a survey of businesses. For the jobs number there is an aggregate value, i.e. for the entire US, and then breakdowns into private versus government and then many further breakdowns by industry type and other things. Suffice it to say that there is a lot of data in the report but I am only going to focus on the number of private jobs added in a month. The reason to focus on private and not the sum of private and government is becuase of some other data I want to use and it only covers private jobs. Why do this? For me it&#39;s to practice data science with real data on an important problem. The jobs number is one of the measures of the health of the economy. I&#39;m writing this in October 2020 with a presidential election coming next month. The report released on October 2nd got a lot of attention in the press. Both the stock and bond markets pay attention to the releases too. Back in 2007 I worked in the investment group of a large bank and I recall that when the numbers came out, usually around 8:30 AM New York time, they were announced for all the traders to hear. As evidence of the potential impact on the markets see here, here, here, or better still just google it. . You might ask WHy not just predict the S&amp;P 500 or something like that if we are interested in the markets. Well, we might get to that later. Some potential methods are described in the papers listed below. . Generative Adversarial Network for Stock Market price Prediction | EMPIRICAL ASSET PRICING VIA MACHINE LEARNING | . I plan to use a variety of techniques including Time Series Regression, Ensemble methods such as Random Forests, and hopefully Neural Networks. I&#39;m interested not only in how the perform out of sample but also how hard it is to build the models and how interpretable the model is. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/04/Intro_Overview.html",
            "relUrl": "/jupyter/2020/11/04/Intro_Overview.html",
            "date": " • Nov 4, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Part 2 - Data Sources",
            "content": "Fred - the one stop shop . You can get the current value and some historical values from the BLS site, but I am going to use a different source, the St. Federal Reserve Economic Data site referred to as FRED . FRED is a great resource for all kinds of economic data and they have an API you can use to get the data. So I will get both the target variable, i.e. the private jobs number, as well as many of the predictor variables from FRED. . So what to use for predictor variables. One obvious choice is past values of the private jobs number. Past values of the unemployment numbers might also be helpful. There is another publication from the BLS called the JOLTS report for Job Openings and Labor Turnover Survey. This should get you to the latest release. I will use 3 values from that report, job openings, hirings and separations. We have to be careful with this one since it is published with a longer lag than the main jobs report. For example, the JOLTS report for August comes out in October whereas the Employment Situation report for August comes out in September. We can get the JOLTS data from FRED. . The BLS is part of the US Department of Labor. Another division of the Department of Labor produces weekly reports on initial and continued unemployment insurance claims. The claims data is published weekly so we will have to deal with data at different frequencies. And guess what ... we can get those from FRED. . There is a payroll processing company called ADP that produces something similar to the jobs numbers based on their proprietary data, but they don&#39;t have estimates for government jobs which is why I am only targeting the private number. More about the ADP estimates here. The ADP report comes out near the end of the month often just days before the BLS report. It is intended to be a predictor of the BLS values ... so we will see. And you can get the ADP values from FRED. . University of Michigan Consumer Sentiment . Note that most of the predictor variables I have listed so far are backward looking. They tell us about the sate of the world at some point in the past. It would be nice to have some forward looking indicators, like expectations. One source of that is the University of Michigan survey of consumer sentiment. The Michigan survey includes 3 questions that are forward looking, see the survey methodology document here. I think questions 2 through 4 look like they might be good to use so I will. Now we can get the aggregate value on Fred although with a one month lag. Anyway, I don&#39;t think the components are on FRED so I will download them from the Michigan website here. . Google Trends . And now for something completely different, Google Trends Part of the reason my interest in this project was revived was that I found a paper titled In Search of a Job: Forecasting Employment Growth Using Google Trends about using Google Trends to predict the employment numbers and I wanted to try it out myself. The SSRN version of the paper is here . The basic idea is that the relative demand for search terms such as &quot;jobs&quot; might be an indicator of how many people are looking for work. But what other terms should we use other than &quot;jobs&quot;. The papers suggests using Google&#39;s Keyword Planner to find other related terms. It should be fun. . Other Sources . I&#39;m sure there are many other variables I could use. A researcher from the The St. Louis Fed maintains a special set of over 100 monthly economic variables mainly for researchers to use. Read about it here. I don&#39;t want to use that since I believe it has the updated values of releases, not the original values. I want to use the values that first came out in case there were revisions. . The Conference Board is the source of some widely used economic data including the Leading Economic Indicators. I am not going to use those since I belive you have to subscribe to get them. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/04/DataSources.html",
            "relUrl": "/jupyter/2020/11/04/DataSources.html",
            "date": " • Nov 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a computer scientist, a data scientist and a finance quant. . I like to solve practical problems and create useful applications. I love doing analytics, machine learning, algorithms and data science, so I am thrilled that these disciplines have all become very popular and that just about every industry is using their methods to solve problems. . This website is powered by fastpages [^1]. [^1]:a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://jhmuller.github.io/job-forecasting/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jhmuller.github.io/job-forecasting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}