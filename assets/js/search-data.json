{
  
    
        "post0": {
            "title": "Part 4 - New data on Weekly Claims",
            "content": "I was scanning a WSJ article today on jobless claims, https://www.wsj.com/articles/weekly-jobless-claims-coronavirus-10-29-2020-11603921724?modtag=djemBestOfTheWeb The article started by claiming that initial claims filings had fallen to their lowest level since the pandemic began. This didn&#39;t seem interesting because according to FRED you could have been saying that since May. Seems like saying I am as old as I&#39;ve ever been. But then I noticed a chart showing claims under Regular state programs plus claims under special pandemic programs ... and for continued claims the special pandemic numbers were about the same as the regular ones. Uh oh, so the claims could be 2x what I would get from FRED and the normal BLS data. I could not find any data on these new plans on FRED but did find a spreadsheet on the Department of Labor website, see https://oui.doleta.gov/unemploy/DataDashboard.asp . The data might be available on the DOL or BLS site via some API. Not sure yet. In any case, in this post I will read in that data and plot it. Later I will figure out how to integreate it with the rest. . import os import sys import inspect import datetime import pandas as pd import numpy as np import time from plotnine import ggplot import matplotlib as mpl import xlrd import matplotlib.pyplot as plt import selenium from selenium import webdriver from selenium.webdriver.support.ui import Select import pathlib import shutil . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd version 1.19.4 of np version 3.2.2 of mpl version 1.2.0 of xlrd version 3.141.0 of selenium version 3.14.1 of webdriver . Selenium . Setup temp directory for download . temp_dir = &quot;./data/temp&quot; if not os.path.isdir(temp_dir): os.mkdir(temp_dir) prefs = {&quot;download.default_directory&quot; : os.path.abspath(temp_dir)} options = webdriver.ChromeOptions() options.add_experimental_option(&quot;prefs&quot;,prefs) options.add_argument(&quot;download.default_directory=&quot;+os.path.abspath(temp_dir)) . chromedriver_path = os.path.join(&#39;../../chromedriver_win32/chromedriver.exe&#39;) driver = webdriver.Chrome(executable_path=chromedriver_path, options=options) url = &quot;https://oui.doleta.gov/unemploy/DataDashboard.asp&quot; driver.get(url) . def get_downloaded_fpath(data_dir=None, files_before=None, file_ext=&quot;.csv&quot;, max_wait = 10, verbosity=0): import time done = False start_time = datetime.datetime.now() while not done: files_after = set(os.listdir(data_dir)) new_files = files_after.difference(set(files_before)) if verbosity &gt; 0: print(&quot;Files Before: {0}&quot;.format(files_before)) print(&quot;Files After: {0}&quot;.format(files_after)) print(&quot;New Files: {0}&quot;.format(new_files)) if verbosity &gt; 0: print(new_files) for fname in new_files: if os.path.splitext(fname)[1] == file_ext: return (os.path.join(data_dir, fname)) cur_time = datetime.datetime.now() if (cur_time - start_time).seconds &gt; max_wait: return None time.sleep(0.5) . files_before = set(os.listdir(temp_dir)) time.sleep(1) el = driver.find_element_by_link_text(&quot;NEW: Weekly Pandemic Claims Data&quot;) el.click() downloaded_fpath = get_downloaded_fpath(data_dir=temp_dir, files_before=files_before, file_ext=&quot;.xlsx&quot;, max_wait=3, verbosity=0) if not downloaded_fpath: raise Exception(&quot;No downloaded file found!&quot;) print(&quot;new file: {0}&quot;.format(downloaded_fpath)) . new file: ./data/temp weekly_pandemic_claims (1).xlsx . Clean up . def clean_filename(fpath): import re head, tail = os.path.split(downloaded_fpath) base, ext = os.path.splitext(tail) match = re.match(&quot;(.*)( (.* ))&quot;, base) new_base = match.group(1).strip() new_fname = new_base + ext return new_fname temp_df = pd.read_excel(downloaded_fpath) data_dir = &quot;./data&quot; new_fname = clean_filename(downloaded_fpath) new_fpath = os.path.join(data_dir, new_fname) temp_df.to_csv(new_fpath, index=False) if os.path.isfile(new_fpath): # stop driver and remove old directory driver.quit() shutil.rmtree(temp_dir) . Read . Read in the spreadsheet and get rid of rows with no data for Rptdate . data_dir = &quot;./data&quot; new_fpath = os.path.join(data_dir, new_fname) states_df = pd.read_csv(new_fpath) states_df = states_df[states_df[&#39;Rptdate&#39;].notna()] states_df.rename(columns={&quot;Rptdate&quot;:&quot;date&quot;}, inplace=True) states_df.tail(3) . State date PUA IC Reflect Date PUA CC PEUC CC . 1693 WV | 2020-11-07 | 2469.0 | NaN | NaN | NaN | . 1694 WI | 2020-11-07 | 1799.0 | NaN | NaN | NaN | . 1695 WY | 2020-11-07 | 209.0 | NaN | NaN | NaN | . Plot . For now I&#39;m not interested in the state breakdown so I&#39;ll aggregate to US level and also separte out Initial Claims, IC, from Continued Claims, CC. Partly because the CC values are about 10x the IC values so easier to see and check the IC data if plotted alone. Also because there is data on 2 different special programs for CC. . us_df = states_df.drop(labels=[&quot;State&quot;], axis=1).groupby(by=&#39;date&#39;) us_df = us_df.sum(min_count=1) us_df.reset_index(inplace=True) print(us_df.tail()) us_df.to_csv(os.path.join(&quot;./data&quot;, &quot;us_pau_claims.csv&quot;), index=False) . date PUA IC PUA CC PEUC CC 27 2020-10-10 337228.0 10152753.0 3308937.0 28 2020-10-17 345440.0 10324779.0 3691720.0 29 2020-10-24 359044.0 9332610.0 3983613.0 30 2020-10-31 361959.0 9433127.0 4143389.0 31 2020-11-07 298154.0 NaN NaN . ic_df = us_df[[&quot;date&quot;,&#39;PUA IC&#39;]] cc_df = us_df.drop(&#39;PUA IC&#39;, axis=1) cc_df[&#39;CC Tot&#39;] = cc_df[[&quot;PUA CC&quot;,&quot;PEUC CC&quot;]].sum(axis=1, skipna=False) cc_df.tail() dfs = {&quot;IC&quot;:ic_df, &quot;CC&quot;:cc_df} . figwd = 12 fight= 6 fig, axs = plt.subplots(nrows=2, ncols=1, figsize=[figwd, fight], sharex=True) for i, key in enumerate(dfs.keys()): ax = axs[i] dfs[key].plot(ax=ax, x=&quot;date&quot;, kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, title=key, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, legend=True, label=None, grid=True) . The regular IC values have been about 750K so this new program is about half that. The regular CC values have been about 7.5 million so this new data is about the same. Later I might combine the regular and special data, but for now it&#39;s enough just to have the special data. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/weekly_claims.html",
            "relUrl": "/jupyter/2020/11/20/weekly_claims.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Part 5 - University of Michigan Consumer Sentiment data",
            "content": "I think they do a great job of documentation at the site. . Also have a look at the siki page about the survey and the data. In particular the references to this and indirectly this about Thomson Reuters and others giving investors early access for a fee. . In any case, you can see the question asking sheet and more. I am interested in the responses about expectations. There seems like a lot of great information there but I am going to focus on the data on the components of the index and in particular the 3 questions related to expectations . x2= PEXP_R = &quot;Now looking ahead--do you think that a year from now you (and your family living there) will be better off financially, or worse off, or just about the same as now?&quot; | x3= BUS12_R = &quot;Now turning to business conditions in the country as a whole--do you think that during the next twelve months we&#39;ll have good times financially, or bad times, or what?&quot; | x4= BUS5_R = &quot;Looking ahead, which would you say is more likely--that in the country as a whole we&#39;ll have continuous good times during the next five years or so, or that we will have periods of widespread unemployment or depression, or what?&quot; | . They compute a weighted average called the Index of Consumer Expectations, or ICE, as follows . ICE = ((x2 + x3 + x4)/4.1134) + 2 [1](#myfootnote1) . I&#39;m sure somewhere in the documentation they explain why they divide by 4.1134 and add 2 but I&#39;ll probably just use the 3 individual variables. We&#39;ll see. . 1: Notice I put in the parentesis to avoid issues like this. If the link doesn&#39;t work search for &quot;The Math equation that stumped the internet&quot;) . First the bolerplate Python I use for most notebooks. It&#39;s evolving. . import os import sys import datetime import time import re import inspect import pandas as pd from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt import selenium from selenium import webdriver from selenium.webdriver.support.ui import Select import pathlib . I like to see version numbers for modules ... and for Python. . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 2.2.1 of re version 1.1.3 of pd version 3.2.2 of mpl version 3.141.0 of selenium version 3.14.1 of webdriver . Selenium to automate downloading . It&#39;s easy to get the data from the site, can be done via the following steps. . Navigate to https://data.sca.isr.umich.edu/data-archive/mine.php [2](#myfootnote1) | For Table select Table 5: Components of the Index of Consumer Sentiment. | Click Comma-Separated(CSV) under format and it should start downloading. | But I want to automate the process and we will using Selenium. Selenium allows us to control a web browser from Pyhon so I can use Python to execute all the steps above. . 2: Note that if you start at the main site you might need to click on Data and select Time Series in the dropdown. . Download Directory . I want to have the data downloaded into a subdirectory below here rather than the default Downloads directory. We can arrange that using ChromeOptions. . temp_dir = &quot;./data/temp&quot; if not os.path.isdir(temp_dir): os.mkdir(temp_dir) prefs = {&quot;download.default_directory&quot; : os.path.abspath(temp_dir)} options = webdriver.ChromeOptions() options.add_experimental_option(&quot;prefs&quot;,prefs) options.add_argument(&quot;download.default_directory=&quot;+os.path.abspath(temp_dir)) . Step 1, going to the site . You can run Selenium in what is called headless mode where you don&#39;t get a window for the browser. But I still find it cool to see a new browser pop up so I won&#39;t use that now. And it&#39;s nice in the early stages to see what is actually happening. Probably will change once the novelty wears off. . chromedriver . With Selenium you have a choice of which browser to use, e.g. Chrome, Firefox et cetera. I&#39;ll use Chrome. Whatever the choice we&#39;ll need a driver. Google it if you are interested. I downloaded the chromedriver and put it in a sibling directory under chromedriver_win32 so it is easy to find for others projects using Selenium. . Here I am doing step 1 from above. . chromedriver_path = os.path.join(&#39;../../chromedriver_win32/chromedriver.exe&#39;) driver = webdriver.Chrome(executable_path=chromedriver_path, options=options) url = &quot;https://data.sca.isr.umich.edu/data-archive/mine.php&quot; driver.get(url) . Step 2: selecting Table 5 . There are plenty of tutorials on using Selenium to make selections and click buttons. I can&#39;t remember which ones I used. Most likely I just googled what I wanted to do. . In any case, here I am finding the selection section on the page and choosing option 5. . select_element = Select(driver.find_element_by_css_selector(&quot;select&quot;)) # this will print out strings available for selection on select_element, used in visible text below select_element.select_by_index(5) . Identifying the newly downloaded file . In the final step I will download a file with the data into the ./data directory. But I don&#39;t know exactly what the file name will be yet, and also if I pull it down multiple times the file will get names lik &quot;file.csv&quot;, &quot;file(2).csv&quot; et cetera. I want to make sure I can idendify the the latest one I downloaded. I wrote the function below to help with that. It is pobably way more complicated than it needs to be. . In any case, the idea is to start with a list of all the files in the directory before I asked for the download. Then, I look for new files with the appropriate extension, in this case &quot;.csv&quot;. Once I find a file that was not there before, that is the one that just got donloaded. In case the file takes some time to download I will loop looking for a new file, but I have a limit on how long I will do this looping, My default wait limit is 10 seconds. . def get_downloaded_fpath(dir=None, files_before=None, file_ext=&quot;.csv&quot;, max_wait = 10, verbose=True): import time done = False start_time = datetime.datetime.now() while not done: files_after = set(os.listdir(dir)) new_files = files_after.difference(set(files_before)) print(new_files) for fname in new_files: if os.path.splitext(fname)[1] == file_ext: return (os.path.join(dir, fname)) cur_time = datetime.datetime.now() if (cur_time - start_time).seconds &gt; 10: return None time.sleep(0.5) . Step 3: Selecting Comma-Separated(CSV) . Now I look for the &quot;Comma-Separated(CSV)&quot; button on the web page and click it. Then I use the above function to find the path to the new file. . elements = driver.find_elements_by_name(&quot;format&quot;) button = None for e in elements: if e.get_property(&quot;value&quot;) == &#39;Comma-Separated (CSV)&#39;: button = e break if not button: raise RuntimeError(&quot;Error downloading Consumer Sentiment data from {0}&quot;.format(url)) files_before = set(os.listdir(temp_dir)) button.click() fpath = get_downloaded_fpath(dir=temp_dir, files_before=files_before, file_ext=&quot;.csv&quot;, max_wait=10) if not fpath: raise Exception(&quot;No downloaded file found!&quot;) print(&quot;new file: {0}&quot;.format(fpath)) . set() set() {&#39;6cdd26b7-050f-47a5-b417-90b22b5c0fe5.tmp&#39;} {&#39;sca-table5-on-2020-Nov-12.csv&#39;} new file: ./data/temp sca-table5-on-2020-Nov-12.csv . Shotdown the browser . driver.quit() . Read in the new file | print the volumns | Drop the last column, don&#39;t know what it is | Make a datetime column from the month and year and drop the month and year columns. | make the datetime column the index | pick out th expectation columns as the ones that have either &quot;expected&quot; or &quot;Business Condition&quot; in the name. drop all but the expected columns | . | . df = pd.read_csv(fpath, skiprows=1) print(df.columns) df.drop(df.columns[-1], inplace=True, axis=1) df[&#39;Datetime&#39;] = (100*100*df[&#39;Year&#39;] + 100*df[&quot;Month&quot;] + 15).astype(str) df[&#39;Datetime&#39;] = pd.to_datetime(df[&#39;Datetime&#39;]) df.drop([&quot;Month&quot;, &quot;Year&quot;], inplace=True, axis=1) df.set_index(&quot;Datetime&quot;, inplace=True) exp_columns = [c for c in df.columns if re.search(&quot;Expected|Business Condition&quot;, c)] df = df[exp_columns] df.head() . Index([&#39;Month&#39;, &#39;Year&#39;, &#39;Personal Finance Current&#39;, &#39;Personal Finance Expected&#39;, &#39;Business Condition 12 Months&#39;, &#39;Business Condition 5 Years&#39;, &#39;Buying Conditions&#39;, &#39;Current Index&#39;, &#39;Expected Index&#39;, &#39;Unnamed: 9&#39;], dtype=&#39;object&#39;) . Personal Finance Expected Business Condition 12 Months Business Condition 5 Years Expected Index . Datetime . 2008-01-15 116 | 68 | 88 | 68.1 | . 2008-02-15 112 | 54 | 83 | 62.4 | . 2008-03-15 112 | 46 | 81 | 60.1 | . 2008-04-15 100 | 40 | 71 | 53.3 | . 2008-05-15 98 | 36 | 68 | 51.1 | . Plot all the expected series . df.plot(figsize=[16,4], marker=&#39;.&#39;, grid=True) plt.gca().set_ylim(bottom=0) . (0.0, 142.3) . Save the dataframe as &quot;umich_exp.csv&quot; . try: out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) df.to_csv(os.path.join(out_dir, &quot;umich_exp.csv&quot;)) except Exception as exc: print(exc) raise Exception(exc) . delete all the files in the temp directory | delete the temp directory | . try: if os.path.isdir(temp_dir): temp_files = os.listdir(temp_dir) # remove all the temp files for tfile in temp_files: os.remove(os.path.join(temp_dir, tfile)) # remove the temp_dir if os.path.isdir(temp_dir): pathlib.Path.rmdir(pathlib.Path(temp_dir)) except Exception as exc: print(exc) raise Exception(exc) . import datetime print(datetime.datetime.now()) . 2020-11-12 07:08:31.238145 .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/umich_cs.html",
            "relUrl": "/jupyter/2020/11/20/umich_cs.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Part 6 - Google Trends data",
            "content": "In this notebook I will describe my code to download data from Google trends as well as code to pull keyword suggestions&quot; based on some seed* keywords. . I start in the usual way I do with all my notebooks with . imports | print version for Python and non standar modules | . This boilerplate code is evolving and hopefully getting better . import os import sys import datetime import inspect import pandas as pd import plotnine as pn import pytrends as pt from pytrends.request import TrendReq . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): mname = name if hasattr(mod, &quot;__file__&quot;): mname = os.path.split(mod.__path__[0])[1] print(&quot;version {1} of {0} as {1} &quot;.format(mname, name, mod.__version__)) elif hasattr(mod, &quot;__file__&quot;) and &quot;site-packages&quot; in mod.__file__: print(&quot;No __version__ for {0} as {1}&quot;.format(mname, name)) del mod del name . version 3.8.5 of Python version pd of pandas as pd version pn of plotnine as pn No __version__ for plotnine as pt . Pytrends api . I&#39;m new to pytrends but I&#39;ll make a go of using it. . Seems like the steps are . create a TrendReq object which is usually called pytrends. | configure the TrendReq object, called pytrends, by calling build_payload with pytrends with parameters for a keyword list | a timeframe | a geography reference | . | call interest_over_time to get the data, returns a dataframe | . It will be useful to wrap some of the code that sets up the paramaters for build_payload into a function . I&#39;ll call the function set_payload . def set_payload(trend_req, kw_list, nweeks=100, geo=&#39;US&#39;, cat=60, grop=&#39;&#39;): end_date = datetime.date.today() start_date = datetime.date.today() - datetime.timedelta(weeks=nweeks) timeframe = start_date.strftime(&quot;%Y-%m-%d&quot;) + &quot; &quot; + end_date.strftime(&quot;%Y-%m-%d&quot;) trend_req.build_payload(kw_list, cat=cat, timeframe=timeframe, geo=geo, gprop=grop) return trend_req . tr = TrendReq(hl=&#39;en-US&#39;, tz=360) tr = set_payload(trend_req=tr, kw_list=[&#39;work&#39;], cat=60, nweeks=20, geo=&#39;US&#39;, grop=&#39;&#39;) df = tr.interest_over_time() print(df.head(2)) print(df.tail(2)) . work isPartial date 2020-06-24 86 False 2020-06-25 83 False work isPartial date 2020-11-02 83 False 2020-11-03 73 False . I don&#39;t know what isPartial means yet. The module documentation does not say much about it. I read somewere You can ignore isPartial for now: that field lets you know if the data point is complete for that particular date, so I plan to remove an ignore it. Now let&#39;s try with more keywords and plot the results . kw_list = [ &quot;work from home&quot;, &quot;unemployment&quot;, &quot;indeed jobs&quot;, &quot;work&quot;] nweeks = 52 tr = set_payload(trend_req=tr, kw_list=kw_list, nweeks=nweeks, ) df = tr.interest_over_time().drop(&quot;isPartial&quot;, axis=1) df.plot(figsize=[12,6], grid=True, marker=&#39;o&#39;) . timeout Traceback (most recent call last) ~ anaconda3 envs payems lib site-packages urllib3 connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw) 425 # Otherwise it looks like a bug in the code. --&gt; 426 six.raise_from(e, None) 427 except (SocketTimeout, BaseSSLError, SocketError) as e: ~ anaconda3 envs payems lib site-packages urllib3 packages six.py in raise_from(value, from_value) ~ anaconda3 envs payems lib site-packages urllib3 connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw) 420 try: --&gt; 421 httplib_response = conn.getresponse() 422 except BaseException as e: ~ anaconda3 envs payems lib http client.py in getresponse(self) 1346 try: -&gt; 1347 response.begin() 1348 except ConnectionError: ~ anaconda3 envs payems lib http client.py in begin(self) 306 while True: --&gt; 307 version, status, reason = self._read_status() 308 if status != CONTINUE: ~ anaconda3 envs payems lib http client.py in _read_status(self) 267 def _read_status(self): --&gt; 268 line = str(self.fp.readline(_MAXLINE + 1), &#34;iso-8859-1&#34;) 269 if len(line) &gt; _MAXLINE: ~ anaconda3 envs payems lib socket.py in readinto(self, b) 668 try: --&gt; 669 return self._sock.recv_into(b) 670 except timeout: ~ anaconda3 envs payems lib ssl.py in recv_into(self, buffer, nbytes, flags) 1240 self.__class__) -&gt; 1241 return self.read(nbytes, buffer) 1242 else: ~ anaconda3 envs payems lib ssl.py in read(self, len, buffer) 1098 if buffer is not None: -&gt; 1099 return self._sslobj.read(len, buffer) 1100 else: timeout: The read operation timed out During handling of the above exception, another exception occurred: ReadTimeoutError Traceback (most recent call last) ~ anaconda3 envs payems lib site-packages requests adapters.py in send(self, request, stream, timeout, verify, cert, proxies) 438 if not chunked: --&gt; 439 resp = conn.urlopen( 440 method=request.method, ~ anaconda3 envs payems lib site-packages urllib3 connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw) 725 --&gt; 726 retries = retries.increment( 727 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2] ~ anaconda3 envs payems lib site-packages urllib3 util retry.py in increment(self, method, url, response, error, _pool, _stacktrace) 402 if read is False or not self._is_method_retryable(method): --&gt; 403 raise six.reraise(type(error), error, _stacktrace) 404 elif read is not None: ~ anaconda3 envs payems lib site-packages urllib3 packages six.py in reraise(tp, value, tb) 734 raise value.with_traceback(tb) --&gt; 735 raise value 736 finally: ~ anaconda3 envs payems lib site-packages urllib3 connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw) 669 # Make the request on the httplib connection object. --&gt; 670 httplib_response = self._make_request( 671 conn, ~ anaconda3 envs payems lib site-packages urllib3 connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw) 427 except (SocketTimeout, BaseSSLError, SocketError) as e: --&gt; 428 self._raise_timeout(err=e, url=url, timeout_value=read_timeout) 429 raise ~ anaconda3 envs payems lib site-packages urllib3 connectionpool.py in _raise_timeout(self, err, url, timeout_value) 334 if isinstance(err, SocketTimeout): --&gt; 335 raise ReadTimeoutError( 336 self, url, &#34;Read timed out. (read timeout=%s)&#34; % timeout_value ReadTimeoutError: HTTPSConnectionPool(host=&#39;trends.google.com&#39;, port=443): Read timed out. (read timeout=5) During handling of the above exception, another exception occurred: ReadTimeout Traceback (most recent call last) &lt;ipython-input-5-84f8a26bdd5c&gt; in &lt;module&gt; 2 nweeks = 52 3 tr = set_payload(trend_req=tr, kw_list=kw_list, nweeks=nweeks, ) -&gt; 4 df = tr.interest_over_time().drop(&#34;isPartial&#34;, axis=1) 5 df.plot(figsize=[12,6], grid=True, marker=&#39;o&#39;) ~ anaconda3 envs payems lib site-packages pytrends request.py in interest_over_time(self) 210 211 # make the request and parse the returned json --&gt; 212 req_json = self._get_data( 213 url=TrendReq.INTEREST_OVER_TIME_URL, 214 method=TrendReq.GET_METHOD, ~ anaconda3 envs payems lib site-packages pytrends request.py in _get_data(self, url, method, trim_chars, **kwargs) 124 cookies=self.cookies, **kwargs, **self.requests_args) # DO NOT USE retries or backoff_factor here 125 else: --&gt; 126 response = s.get(url, timeout=self.timeout, cookies=self.cookies, 127 **kwargs, **self.requests_args) # DO NOT USE retries or backoff_factor here 128 # check if the response contains json and throw an exception otherwise ~ anaconda3 envs payems lib site-packages requests sessions.py in get(self, url, **kwargs) 541 542 kwargs.setdefault(&#39;allow_redirects&#39;, True) --&gt; 543 return self.request(&#39;GET&#39;, url, **kwargs) 544 545 def options(self, url, **kwargs): ~ anaconda3 envs payems lib site-packages requests sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json) 528 } 529 send_kwargs.update(settings) --&gt; 530 resp = self.send(prep, **send_kwargs) 531 532 return resp ~ anaconda3 envs payems lib site-packages requests sessions.py in send(self, request, **kwargs) 641 642 # Send the request --&gt; 643 r = adapter.send(request, **kwargs) 644 645 # Total elapsed time of the request (approximately) ~ anaconda3 envs payems lib site-packages requests adapters.py in send(self, request, stream, timeout, verify, cert, proxies) 527 raise SSLError(e, request=request) 528 elif isinstance(e, ReadTimeoutError): --&gt; 529 raise ReadTimeout(e, request=request) 530 else: 531 raise ReadTimeout: HTTPSConnectionPool(host=&#39;trends.google.com&#39;, port=443): Read timed out. (read timeout=5) . Notes . the resulting data is weekly. I have read that that is the standard when the time span is over 90 days. | There is a get_daily_data method available in the pytrends module but I ok with weekly for now. | . Getting More keywords . OK, so far so good. Maybe using more keywords would give us better preictor variables, but what other keywords might be useful. The trend_requst object gives 2 methods that seem like they might help here . relate_queries | suggestions I&#39;m not sure the difference but for now I am usig related_queries. It returns a score which I belive is some measure of how related the other query is. Below I have some code to do the following | start with some seed queries, push then into a queue | pop an item off the queue, an item includes a keyword and a score | if the score is above a threshold add the keyword to the keyword list | call related queries on the keyword and push all results above a cutoff into the queue | . | . from collections import deque import time sleeptime = 0.0001 verbosity = 1 max_keywords = 100 min_score = 98 keywords2try = deque() keyword_tups = deque() keyword_set = set() # seed the queue with a few terms keywords2try.appendleft((&quot;jobs&quot;, 101, 0, &#39;root&#39;, 101)) keywords2try.appendleft((&quot;unemployment&quot;, 101, 0, &#39;root&#39;, 101)) print(&quot;Start {0}&quot;.format(datetime.datetime.now())) while len(keywords2try) &gt; 0: if len(keyword_tups) &gt; max_keywords: break kw, score, level, parent, parent_score = keywords2try.pop() if kw in keyword_set: if verbosity &gt; 1: print(&quot;{0} already in set&quot;.format(kw)) continue if verbosity &gt; 0: print(&quot;&lt;kw#{1}=&#39;{0}&#39;&#39;&gt;, &quot;.format(kw, len(keyword_tups)+1), end=&#39;&#39;) # add to keyword_tups and keyword_set keyword_tups.appendleft((kw, score, level, parent, parent_score)) keyword_set.add(kw) trend_req = TrendReq(hl=&#39;en-US&#39;, tz=360) trend_req = set_payload(trend_req, kw_list=[kw]) # do I need to insert some wait time here? time.sleep(sleeptime) related = trend_req.related_queries() if not related: if verbosity &gt; 1: print(&quot; t No related&quot;) continue if &#39;top&#39; not in related[kw].keys(): if verbosity &gt; 1: print(&quot; tNo top&quot;) continue df = related[kw][&#39;top&#39;] if df is None: if verbosity &gt; 1: print(&quot; ttop is empty&quot;) continue for tup in df.itertuples(): child_score = score * tup.value/float(100) if child_score &gt; min_score and tup.query not in keyword_set: if verbosity &gt; 1: print(&quot; tpushing {0}&quot;.format(tup.query)) keywords2try.appendleft((tup.query, child_score, level+1, kw, score)) print(&quot;Done {0}&quot;.format(datetime.datetime.now())) kw_df = pd.DataFrame(data=list(keyword_tups), columns=[&#39;keyword&#39;, &#39;score&#39;, &#39;level&#39;,&#39;parent&#39;, &#39;parent_score&#39;]).sort_values(by=&quot;score&quot;, ascending=False) kw_df.sort_values(by=&#39;score&#39;, ascending=True) kw_df.tail() . print(&quot;Found {0} queries&quot;.format(kw_df.shape[0])) kw_df.sort_values(by=&quot;score&quot;, inplace=True) out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) try: kw_df.to_csv(os.path.join(out_dir, &quot;google_keywords.csv&quot;)) except Exception as exc: print(exc) raise(exc) . trend_req = TrendReq(hl=&#39;en-US&#39;, tz=360) nyears = 2 trends_df = pd.DataFrame() for tup in kw_df.itertuples(): kw_list = [tup.keyword] nweeks = 52*nyears trend_req = set_payload(trend_req, kw_list, nweeks=nweeks, ) temp_df = trend_req.interest_over_time() temp_df.drop(&#39;isPartial&#39;, axis=1, inplace=True) if trends_df.shape[0] == 0: trends_df = temp_df else: trends_df = trends_df.join(temp_df) print(trends_df.columns) . trends_df.columns trends_df.head() . import matplotlib.pyplot as plt trends_df.plot(figsize=(20, 10)) plt.legend(loc=&#39;lower left&#39;) plt.show() . import datetime print(datetime.datetime.now()) .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/google.html",
            "relUrl": "/jupyter/2020/11/20/google.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Part 3 - Predicting the Jobs number - Course correction",
            "content": "I am making some changes in course due to new discoveries about the data and also refocusing effort. . The first changes should be clear, I am going to start using Jupyter notebooks as the format for my posts. The whole point is to show some of the methods and Jupyter notebooks provide a great way to do that. Thanks to the folks at fastai for their tools and tutorials to help me get started with this. . As with any project, I learn new things as I go along, especiall about the data. . I had said in an earlier post that I would pull the U. of Michigan survey data from the U. Mich site because there was a delay of a month for the data to be available on FRED. Well, it&#39;s not just FRED but overall. I believe there is some arrnagement with Bloomberg to not publish the data elsewhere for a month. I also believe that the overall consumer score can be scrapped from the U. Mich site, but not the results for the 5 questions that make up the overall score. So, the forward looking question results will have a delay of 1 month. Later I will give some notebook code to pull the data from the U. Michigan site automatically using Selenium with Python. And I guess I will use the overall score and get that from the U. Michigan site. . Another change is that I will add the OECD Consumer and Business confidence indexes to the predictor variables. I found how to get them via FRED. There are API&#39;s that should work directly with the OECD but if I can get it from FRED then that is much easier for me since I already know how to use one of the Python APIs for that. . Speaking of data in general, I found this note from the SF Fed, https://www.frbsf.org/education/publications/doctor-econ/2013/october/labor-market-indicators-monetary-policy-unemployment-rate/ Seems that I am using a lot of the data sources that the fed uses. I am going to add a few of the series mentioned in that document. . One other change not so much related to data but to priorities. I will spend less time on developing a standard time series regression model. I am more interested in what I can do with a machine learning ensemble method as well as with a neural net model. . I still have to deal with data at different frequencies. A big shout out to Tom Stark, https://www.philadelphiafed.org/research-and-data/research-contacts/stark, at the Philadelphia Fed for sending me some advice on how to deal with this. He says to use a Kalman filter. And Tom was nice enough to send me his notes on the subject, comprising of 4 PDF files and over 350 pages. Very nice Tom, what a real gem you are. . With that as an intro to Part 3, let&#39;s get started getting data from FRED. . import os import sys import datetime import inspect import pandas as pd import numpy as np from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt . Print out the versions for Python and non standard library modules . mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items())) vi = sys.version_info print(&quot;version {0}.{1}.{2} of Python&quot;.format(vi.major, vi.minor, vi.micro)) for name, mod in mlist: if name.startswith(&quot;__&quot;): continue if hasattr(mod, &quot;__version__&quot;): print(&quot;version {1} of {0}&quot;.format(name, mod.__version__)) del mod del name . version 3.8.5 of Python version 1.1.3 of pd version 1.19.4 of np version 3.2.2 of mpl . Getting data from FRED using a Python API . The FRED website itself has a pretty nice GUI for plotting data. But I need to download the data so I can use Python or R tools to do the forecasting. . The FRED website gives links to projects you can use to access the FRED data from a variety of languages. See https://fred.stlouisfed.org/docs/api/fred/ I have used the python one called fredapi, https://github.com/mortada/fredapi, before and will use it here with one modification. I will add arguments to limit the series start and end dates. . To use any of the API tools you will need an API Key. See the instructions here, https://fred.stlouisfed.org/docs/api/api_key.html, to get one and get started. I keep mine in a file called &quot;fred_api_key&quot;. The fredapi module is in a sibling directory so I add that to my path before importing the module. . sys.path.append(&#39;../../fredapi&#39;) from fredapi import fred import fredapi Fred = fred.Fred(api_key_file=&quot;fred_api.key&quot;) print(fredapi.__version__) . 0.4.2.2 . How to identify the data on FRED . To acces the data for a series from FRED you need to know the series id. You can use the fredapi to search for a series given keywords. I already know the series ids I want to use and have them in a csv file, so let&#39;s read that in and see the ids and a brief description of the data for each one. . fred_ids = pd.read_csv(&#39;fred_ids.csv&#39;, index_col=None, sep=&#39;|&#39;) fred_ids . series_id description . 0 USPRIV | BLS private | . 1 NPPTTL | ADP | . 2 ICSA | Initial Claims | . 3 CCSA | Continued Claims | . 4 JTS1000JOL | Job Openings: Total Private | . 5 JTS1000HIL | Hires: Total Private | . 6 JTS1000TSL | Total Separations: Total Private | . 7 CSCICP03USM665S | OECD US Consumer Confidence | . 8 BSCICP03USM665S | OECD Business Confidence | . 9 UNRATE | Unemployment Rate | . 10 UNEMPLOY | Unemployment Level | . 11 CLF16OV | Civilian Labor Force | . 12 UEMP27OV | Unemployed for 27 Weeks | . 13 U6Rate | Unemployed plus Marginally Attached | . 14 CIVPART | Civilian Participation Rate | . 15 LNS12032194 | Part-Time for Econ Reasons | . Now we call the fredapi to get the observations for each series. I&#39;m only getting the data back to the start of 2007. I&#39;ll print out the number of observations we get of reach series and we will notice the different frequencies. . FRED and ALFRED . Have I not mentioned ALFRED yet? The goverment releases, like Nonfarm payrolls, usually get updated in subsequent months, that is, the initial estimates might be revised up or down. FRED just has the latest update. You probably want to get the value as released or perhaps the value that was known on a given date. You get that from ALFRED. Notice in the call below I get all_releases and then manipulate the data to only keep the earliest one, or the original. The API is the same for FRED and ALFRED, but if you get other than the most current data you will be getting it from ALFRED. . dfs = [] obs_start = &quot;2007-01-01&quot; obs_end = datetime.date.today() for ser in fred_ids.itertuples(): print(ser.series_id, end=&#39;, &#39;) try: all_df = Fred.get_series_all_releases(series_id=ser.series_id, observation_start=obs_start, observation_end=obs_end) tdf = all_df.sort_values(by=&quot;realtime_start&quot;, ascending=True).groupby(by=&quot;date&quot;).head(1) tdf[&#39;series_id&#39;] = ser.series_id tdf[&#39;date&#39;] = tdf[&#39;date&#39;].dt.date print(&quot;rows= {0}&quot;.format(tdf.shape[0])) dfs.append(tdf) except Exception as exc: print(exc) obs_df = pd.concat(dfs) obs_df = obs_df.merge(fred_ids, on=&#39;series_id&#39;) . USPRIV, rows= 166 NPPTTL, rows= 166 ICSA, rows= 723 CCSA, rows= 722 JTS1000JOL, rows= 165 JTS1000HIL, rows= 165 JTS1000TSL, rows= 165 CSCICP03USM665S, rows= 166 BSCICP03USM665S, rows= 166 UNRATE, rows= 166 UNEMPLOY, rows= 166 CLF16OV, rows= 166 UEMP27OV, rows= 166 U6Rate, rows= 166 CIVPART, rows= 166 LNS12032194, rows= 166 . Merge in the descriptions and have a look at the last few rows. . obs_df.sort_values(by=&quot;date&quot;, inplace=True) obs_df[&#39;dtime&#39;] = pd.to_datetime(obs_df[&#39;date&#39;]) print(obs_df.tail(4)) . realtime_start realtime_end date value series_id 1775 2020-11-05 2020-11-11 2020-10-24 7.285e+06 CCSA 1053 2020-11-05 2020-11-11 2020-10-31 751000 ICSA 1776 2020-11-12 2099-01-02 2020-10-31 6.786e+06 CCSA 1054 2020-11-12 2099-01-02 2020-11-07 709000 ICSA description dtime 1775 Continued Claims 2020-10-24 1053 Initial Claims 2020-10-31 1776 Continued Claims 2020-10-31 1054 Initial Claims 2020-11-07 . Not for HFT . A quick aside about how quickly the data is available on FRED. The Claims data was released at 8:30 ET on Thursday and I first saw it on FRED and in my download about 10-15 minutes later. While not fast enough for high frequency trading, still very good. Also let me say how helpful the folks at FRED have been. I&#39;ve emailed them a number of questions and they typically get back to me in a day or so. FRED and ALFRED are great resources. . Plot of all the data . Let&#39;s have a look at what the series look like over time. What a shock to the employment situation we&#39;ve had this year. Note that in many ways it dwarfs the impact from the 2008 financial crisis. Note how the business confidence index has bounced back. Not so much for consumer confidence. Our target is BLS private. Looks like the ADP data is tracking it really well and it comes out the week before. . def plot_observations(data_df, xcol, ycol, idcol, ncols, figwd, fight, ylim=(None, None), xtick_rot=0, sharex=True): ids = list(data_df[idcol].unique()) nrows = int(np.ceil(len(ids)/float(ncols))) fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=[figwd, fight], sharex=sharex) gdf = data_df.groupby(by=&#39;description&#39;) for i, (key, group) in enumerate(gdf): row = i // ncols col = i % ncols ax = axs[row][col] group.plot(ax=ax, kind=&#39;line&#39;, x=xcol, y=ycol,linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, ylim=ylim, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, title=key, legend=None, label=None, grid=True) ax.xaxis.set_tick_params(rotation=xtick_rot) return fig . ylim=(None, None) fig = plot_observations(data_df=obs_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None)) . Plot of the last year . Let&#39;s focus in on data from 2020. Notice how some of the series are almost current while others are lagging more than a month. . temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date.today() - datetime.timedelta(weeks=52)] temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date(year=2020, month=1, day=1)] ylim=(None, None) fig = plot_observations(data_df=temp_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None), xtick_rot=90) . save the dataframe into the output directory . out_dir = &quot;./data&quot; if not os.path.isdir(out_dir): os.mkdir(out_dir) obs_df.to_csv(os.path.join(out_dir, &quot;fred.csv&quot;)) . Tidy Fred . cols = [&quot;date&quot;, &quot;value&quot;] series_ids = obs_df[&quot;series_id&quot;].unique() dfs = [] tidy_df = pd.DataFrame() for sid in series_ids: x = obs_df.loc[obs_df[&quot;series_id&quot;]==sid,cols] x.columns = [&quot;date&quot;, sid] if tidy_df.shape[0] == 0: tidy_df = x else: tidy_df = tidy_df.merge(x, how=&quot;outer&quot;, on=&quot;date&quot;) tidy_df.sort_values(by=&quot;date&quot;, inplace=True) scols = [&quot;date&quot;, &quot;USPRIV&quot;, &quot;ICSA&quot;, &quot;JTS1000JOL&quot;] print(final_df[scols].head(10)) print(final_df[scols].tail(10)) tidy_df.to_csv(&quot;./data/tidy_fred.csv&quot;) . NameError Traceback (most recent call last) &lt;ipython-input-11-200cde2accdf&gt; in &lt;module&gt; 12 tidy_df.sort_values(by=&#34;date&#34;, inplace=True) 13 scols = [&#34;date&#34;, &#34;USPRIV&#34;, &#34;ICSA&#34;, &#34;JTS1000JOL&#34;] &gt; 14 print(final_df[scols].head(10)) 15 print(final_df[scols].tail(10)) 16 tidy_df.to_csv(&#34;./data/tidy_fred.csv&#34;) NameError: name &#39;final_df&#39; is not defined . That&#39;s all for now on FRED data. Next up, U. Michigan data and Google Trends data. . import datetime print(datetime.datetime.now()) .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/fred.html",
            "relUrl": "/jupyter/2020/11/20/fred.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages - Brief How-to guide",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-4-88b922e30289&gt; in &lt;module&gt; 1 #collapse-hide 2 import pandas as pd -&gt; 3 import altair as alt ModuleNotFoundError: No module named &#39;altair&#39; . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . NameError Traceback (most recent call last) &lt;ipython-input-8-85b8363709cc&gt; in &lt;module&gt; 1 # single-value selection over [Major_Genre, MPAA_Rating] pairs 2 # use specific hard-wired values as the initial selected values -&gt; 3 selection = alt.selection_single( 4 name=&#39;Select&#39;, 5 fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], NameError: name &#39;alt&#39; is not defined . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/fastpages-howto.html",
            "relUrl": "/jupyter/2020/11/20/fastpages-howto.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Part 7 - Combining input data",
            "content": "import os import sys import inspect import datetime import pandas as pd import numpy as np from plotnine import ggplot import matplotlib.pyplot as plt . data_dir = &quot;./data&quot; fnames = os.listdir(data_dir) dfs = {} for fname in fnames: fpath = os.path.join(data_dir, fname) head, tail = os.path.split(fpath) fbase,ext = os.path.splitext(tail) if os.path.isfile(fpath) and ext == &quot;.csv&quot;: df = pd.read_csv(fpath) print(&quot;{0} shape:{1}&quot;.format(fbase,df.shape)) dfs[fbase] = df . fred shape:(3766, 8) google_keywords shape:(33, 6) tidy_fred shape:(866, 18) umich_exp shape:(153, 5) us_pau_claims shape:(32, 4) weekly_pandemic_claims shape:(4998, 6) . fred_claims = dfs[&quot;tidy_fred&quot;][[&quot;date&quot;, &quot;ICSA&quot;, &quot;CCSA&quot;]] pau_claims = dfs[&#39;us_pau_claims&#39;] claims_df = fred_claims.merge(pau_claims, how=&quot;outer&quot;, on=&quot;date&quot;) claims_df[&quot;date&quot;] = pd.to_datetime(claims_df[&quot;date&quot;]) claims_df.set_index(keys=&quot;date&quot;, inplace=True) claims_df.sort_index( inplace=True) print(claims_df.columns) print(claims_df.tail(10)) ic_df = claims_df[[ &quot;ICSA&quot;, &#39;PUA IC&#39;]].dropna(how=&quot;all&quot;) cc_df = claims_df[[ &quot;CCSA&quot;, &#39;PUA CC&#39;, &#39;PEUC CC&#39;]].dropna(how=&quot;all&quot;) print(cc_df.tail()) print(ic_df.tail()) . Index([&#39;ICSA&#39;, &#39;CCSA&#39;, &#39;PUA IC&#39;, &#39;PUA CC&#39;, &#39;PEUC CC&#39;], dtype=&#39;object&#39;) ICSA CCSA PUA IC PUA CC PEUC CC date 2020-09-12 860000.0 12580000.0 675154.0 11510888.0 1615057.0 2020-09-19 870000.0 11767000.0 630080.0 11828338.0 1815066.0 2020-09-26 837000.0 10976000.0 450548.0 10381364.0 1969800.0 2020-10-01 NaN NaN NaN NaN NaN 2020-10-03 840000.0 10018000.0 378916.0 10658673.0 2794868.0 2020-10-10 898000.0 8373000.0 337228.0 10152753.0 3308937.0 2020-10-17 787000.0 7756000.0 345440.0 10324779.0 3691720.0 2020-10-24 751000.0 7285000.0 359044.0 9332610.0 3983613.0 2020-10-31 751000.0 6786000.0 361959.0 9433127.0 4143389.0 2020-11-07 709000.0 NaN 298154.0 NaN NaN CCSA PUA CC PEUC CC date 2020-10-03 10018000.0 10658673.0 2794868.0 2020-10-10 8373000.0 10152753.0 3308937.0 2020-10-17 7756000.0 10324779.0 3691720.0 2020-10-24 7285000.0 9332610.0 3983613.0 2020-10-31 6786000.0 9433127.0 4143389.0 ICSA PUA IC date 2020-10-10 898000.0 337228.0 2020-10-17 787000.0 345440.0 2020-10-24 751000.0 359044.0 2020-10-31 751000.0 361959.0 2020-11-07 709000.0 298154.0 . import matplotlib.dates as mdates figwd = 14 fight= 8 fig, axs = plt.subplots(nrows=2, ncols=1, figsize=[figwd, fight], sharex=&#39;row&#39;) ic_df.tail(10).plot(ax=axs[0], kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;x&#39;, lw=2, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=2.2, legend=True, label=None, grid=True) cc_df.tail(10).plot(ax=axs[1], kind=&#39;line&#39;, linestyle=&#39;-&#39;, marker=&#39;x&#39;, lw=2, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=2.2, legend=True, label=None, grid=True) loc = mdates.DayLocator([0,31]) axs[0].xaxis.set_major_locator(loc) axs[0].xaxis.set_major_formatter(mdates.AutoDateFormatter(loc)) axs[1].xaxis.set_major_locator(loc) axs[1].xaxis.set_major_formatter(mdates.AutoDateFormatter(loc)) plt.show() . from plotnine import ggplot, geom_line ggplot(data=ic_df) + geom_line() . KeyError Traceback (most recent call last) ~ anaconda3 envs payems lib site-packages IPython core formatters.py in __call__(self, obj) 700 type_pprinters=self.type_printers, 701 deferred_pprinters=self.deferred_printers) --&gt; 702 printer.pretty(obj) 703 printer.flush() 704 return stream.getvalue() ~ anaconda3 envs payems lib site-packages IPython lib pretty.py in pretty(self, obj) 392 if cls is not object 393 and callable(cls.__dict__.get(&#39;__repr__&#39;)): --&gt; 394 return _repr_pprint(obj, self, cycle) 395 396 return _default_pprint(obj, self, cycle) ~ anaconda3 envs payems lib site-packages IPython lib pretty.py in _repr_pprint(obj, p, cycle) 698 &#34;&#34;&#34;A pprint that just redirects to the normal repr function.&#34;&#34;&#34; 699 # Find newlines and replace them with p.break_() --&gt; 700 output = repr(obj) 701 lines = output.splitlines() 702 with p.group(): ~ anaconda3 envs payems lib site-packages plotnine ggplot.py in __repr__(self) 86 # in the jupyter notebook. 87 if not self.figure: &gt; 88 self.draw() 89 plt.show() 90 return &#39;&lt;ggplot: (%d)&gt;&#39; % self.__hash__() ~ anaconda3 envs payems lib site-packages plotnine ggplot.py in draw(self, return_ggplot) 179 # new frames knowing that they are separate from the original. 180 with pd.option_context(&#39;mode.chained_assignment&#39;, None): --&gt; 181 return self._draw(return_ggplot) 182 183 def _draw(self, return_ggplot=False): ~ anaconda3 envs payems lib site-packages plotnine ggplot.py in _draw(self, return_ggplot) 186 # assign a default theme 187 self = deepcopy(self) --&gt; 188 self._build() 189 190 # If no theme we use the default ~ anaconda3 envs payems lib site-packages plotnine ggplot.py in _build(self) 305 # Prepare data in geoms 306 # e.g. from y and width to ymin and ymax --&gt; 307 layers.setup_data() 308 309 # Apply position adjustments ~ anaconda3 envs payems lib site-packages plotnine layer.py in setup_data(self) 69 def setup_data(self): 70 for l in self: &gt; 71 l.setup_data() 72 73 def draw(self, layout, coord): ~ anaconda3 envs payems lib site-packages plotnine layer.py in setup_data(self) 428 return type(data)() 429 --&gt; 430 data = self.geom.setup_data(data) 431 432 check_required_aesthetics( ~ anaconda3 envs payems lib site-packages plotnine geoms geom_line.py in setup_data(self, data) 20 21 def setup_data(self, data): &gt; 22 return data.sort_values([&#39;PANEL&#39;, &#39;group&#39;, &#39;x&#39;]) ~ anaconda3 envs payems lib site-packages pandas core frame.py in sort_values(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key) 5278 from pandas.core.sorting import lexsort_indexer 5279 -&gt; 5280 keys = [self._get_label_or_level_values(x, axis=axis) for x in by] 5281 5282 # need to rewrap columns in Series to apply key function ~ anaconda3 envs payems lib site-packages pandas core frame.py in &lt;listcomp&gt;(.0) 5278 from pandas.core.sorting import lexsort_indexer 5279 -&gt; 5280 keys = [self._get_label_or_level_values(x, axis=axis) for x in by] 5281 5282 # need to rewrap columns in Series to apply key function ~ anaconda3 envs payems lib site-packages pandas core generic.py in _get_label_or_level_values(self, key, axis) 1561 values = self.axes[axis].get_level_values(key)._values 1562 else: -&gt; 1563 raise KeyError(key) 1564 1565 # Check for duplicates KeyError: &#39;x&#39; .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/combiningData.html",
            "relUrl": "/jupyter/2020/11/20/combiningData.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Part 1 - Introduction and Overview",
            "content": "This is the first of a series of posts in which I plan to talk about a real world prediction problem, predicting the change in one of the monthly government macro economic values. In particular, I will look at predicting the change in the number of people working for US private employers in a given month. The number of people working in the US is compiled by the U.S Bureau of Labor Statistics, BLS, and is published monthly on the first Friday of each month. Of interest is not so much the value, or level as it is often call, but the change in the value. . The value published is an estimate of people employed in the prior month. For example, the value published on Friday 2020-10-02 gives an estimate of the number of people employed in September 2020. The number are published in a report currently called the Employment Situation. . There are actually many numbers in the report the BLS publishes with two main types, unemployment and jobs. The unemployment numbers come from a survey of households and the jobs numbers come from a survey of businesses. For the jobs number there is an aggregate value, i.e. for the entire US, and then breakdowns into private versus government and then many further breakdowns by industry type and other things. Suffice it to say that there is a lot of data in the report but I am only going to focus on the number of private jobs added in a month. The reason to focus on private and not the sum of private and government is becuase of some other data I want to use and it only covers private jobs. Why do this? For me it&#39;s to practice data science with real data on an important problem. The jobs number is one of the measures of the health of the economy. I&#39;m writing this in October 2020 with a presidential election coming next month. The report released on October 2nd got a lot of attention in the press. Both the stock and bond markets pay attention to the releases too. Back in 2007 I worked in the investment group of a large bank and I recall that when the numbers came out, usually around 8:30 AM New York time, they were announced for all the traders to hear. As evidence of the potential impact on the markets see here, here, here, or better still just google it. . You might ask WHy not just predict the S&amp;P 500 or something like that if we are interested in the markets. Well, we might get to that later. Some potential methods are described in the papers listed below. . Generative Adversarial Network for Stock Market price Prediction | EMPIRICAL ASSET PRICING VIA MACHINE LEARNING | . I plan to use a variety of techniques including Time Series Regression, Ensemble methods such as Random Forests, and hopefully Neural Networks. I&#39;m interested not only in how the perform out of sample but also how hard it is to build the models and how interpretable the model is. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/Intro_Overview.html",
            "relUrl": "/jupyter/2020/11/20/Intro_Overview.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Part 2 - Data Sources",
            "content": "Fred - the one stop shop . You can get the current value and some historical values from the BLS site, but I am going to use a different source, the St. Federal Reserve Economic Data site referred to as FRED . FRED is a great resource for all kinds of economic data and they have an API you can use to get the data. So I will get both the target variable, i.e. the private jobs number, as well as many of the predictor variables from FRED. . So what to use for predictor variables. One obvious choice is past values of the private jobs number. Past values of the unemployment numbers might also be helpful. There is another publication from the BLS called the JOLTS report for Job Openings and Labor Turnover Survey. This should get you to the latest release. I will use 3 values from that report, job openings, hirings and separations. We have to be careful with this one since it is published with a longer lag than the main jobs report. For example, the JOLTS report for August comes out in October whereas the Employment Situation report for August comes out in September. We can get the JOLTS data from FRED. . The BLS is part of the US Department of Labor. Another division of the Department of Labor produces weekly reports on initial and continued unemployment insurance claims. The claims data is published weekly so we will have to deal with data at different frequencies. And guess what ... we can get those from FRED. . There is a payroll processing company called ADP that produces something similar to the jobs numbers based on their proprietary data, but they don&#39;t have estimates for government jobs which is why I am only targeting the private number. More about the ADP estimates here. The ADP report comes out near the end of the month often just days before the BLS report. It is intended to be a predictor of the BLS values ... so we will see. And you can get the ADP values from FRED. . University of Michigan Consumer Sentiment . Note that most of the predictor variables I have listed so far are backward looking. They tell us about the sate of the world at some point in the past. It would be nice to have some forward looking indicators, like expectations. One source of that is the University of Michigan survey of consumer sentiment. The Michigan survey includes 3 questions that are forward looking, see the survey methodology document here. I think questions 2 through 4 look like they might be good to use so I will. Now we can get the aggregate value on Fred although with a one month lag. Anyway, I don&#39;t think the components are on FRED so I will download them from the Michigan website here. . Google Trends . And now for something completely different, Google Trends Part of the reason my interest in this project was revived was that I found a paper titled In Search of a Job: Forecasting Employment Growth Using Google Trends about using Google Trends to predict the employment numbers and I wanted to try it out myself. The SSRN version of the paper is here . The basic idea is that the relative demand for search terms such as &quot;jobs&quot; might be an indicator of how many people are looking for work. But what other terms should we use other than &quot;jobs&quot;. The papers suggests using Google&#39;s Keyword Planner to find other related terms. It should be fun. . Other Sources . I&#39;m sure there are many other variables I could use. A researcher from the The St. Louis Fed maintains a special set of over 100 monthly economic variables mainly for researchers to use. Read about it here. I don&#39;t want to use that since I believe it has the updated values of releases, not the original values. I want to use the values that first came out in case there were revisions. . The Conference Board is the source of some widely used economic data including the Leading Economic Indicators. I am not going to use those since I belive you have to subscribe to get them. .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/11/20/DataSources.html",
            "relUrl": "/jupyter/2020/11/20/DataSources.html",
            "date": " • Nov 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a computer scientist, a data scientist and a finance quant. . I like to solve practical problems and create useful applications. I love doing analytics, machine learning, algorithms and data science, so I am thrilled that these disciplines have all become very popular and that just about every industry is using their methods to solve problems. . This website is powered by fastpages [^1]. [^1]:a blogging platform that natively supports Jupyter notebooks in addition to other formats. .",
          "url": "https://jhmuller.github.io/job-forecasting/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jhmuller.github.io/job-forecasting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}