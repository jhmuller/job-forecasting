{
  
    
        "post0": {
            "title": "Part 3 - Predicting the Jobs number - Course correction",
            "content": "I am making some changes in course due to new discoveries about the data and also refocusing effort. . The first changes should be clear, I am going to start using Jupyter notebooks as the format for my posts. The whole point is to show some of the methods and Jupyter notebooks provide a great way to do that. Thanks to the folks at fastai for their tools and tutorials to help me get started with this. . As with any project, I learn new things as I go along, especiall about the data. . I had said in an earlier post that I would pull the U. of Michigan survey data from the U. Mich site because there was a delay of a month for the data to be available on FRED. Well, it&#39;s not just FRED but overall. I believe there is some arrnagement with Bloomberg to not publish the data elsewhere for a month. I also believe that the overall consumer score can be scrapped from the U. Mich site, but not the results for the 5 questions that make up the overall score. So, the forward looking question results will have a delay of 1 month. Later I will give some notebook code to pull the data from the U. Michigan site automatically using Selenium with Python. And I guess I will use the overall score and get that from the U. Michigan site. . Another change is that I will add the OECD Consumer and Business confidence indexes to the predictor variables. I found how to get them via FRED. There are API&#39;s that should work directly with the OECD but if I can get it from FRED then that is much easier for me since I already know how to use one of the Python APIs for that. . Speaking of data in general, I found this note from the SF Fed, https://www.frbsf.org/education/publications/doctor-econ/2013/october/labor-market-indicators-monetary-policy-unemployment-rate/ Seems that I am using a lot of the data sources that the fed uses. I am going to add a few of the series mentioned in that document. . One other change not so much related to data but to priorities. I will spend less time on developing a standard time series regression model. I am more interested in what I can do with a machine learning ensemble method as well as with a neural net model. . I still have to deal with data at different frequencies. A big shout out to Tom Stark, https://www.philadelphiafed.org/research-and-data/research-contacts/stark, at the Philadelphia Fed for sending me some advice on how to deal with this. He says to use a Kalman filter. And Tom was nice enough to send me his notes on the subject, comprising of 4 PDF files and over 350 pages. Very nice Tom, what a real gem you are. . With that as an intro to Part 3, let&#39;s get started getting data from FRED. . import os import sys import datetime import pandas as pd import numpy as np from plotnine import ggplot import matplotlib as mpl import matplotlib.pyplot as plt print(&quot;Python version= {0}&quot;.format(sys.version_info)) print(&quot;Pandas version= {0}&quot;.format(pd.__version__)) print(&quot;Datetime= {0}&quot;.format(datetime.datetime.now())) . Python version= sys.version_info(major=3, minor=8, micro=5, releaselevel=&#39;final&#39;, serial=0) Pandas version= 1.1.3 Datetime= 2020-10-27 19:38:22.778051 . Getting data from FRED using a Python API . The FRED website itself has a pretty nice GUI for plotting data. But I need to download the data so I can use Python or R tools to do the forecasting. . The FRED website gives links to projects you can use to access the FRED data from a variety of languages. See https://fred.stlouisfed.org/docs/api/fred/ I have used the python one called fredapi, https://github.com/mortada/fredapi, before and will use it here with one modification. I will add arguments to limit the series start and end dates. . To use any of the API tools you will need an API Key. See the instructions here, https://fred.stlouisfed.org/docs/api/api_key.html, to get one and get started. I keep mine in a file called &quot;fred_api_key&quot;. The fredapi module is in a sibling directory so I add that to my path before importing the module. . sys.path.append(&#39;../../fredapi&#39;) from fredapi import fred import fredapi Fred = fred.Fred(api_key_file=&quot;fred_api.key&quot;) print(fredapi.__version__) . 0.4.2 . How to identify the data on FRED . To acces the data for a series from FRED you need to know the series id. You can use the fredapi to search for a series given keywords. I already know the series ids I want to use and have them in a csv file, so let&#39;s read that in and see the ids and a brief description of the data for each one. . fred_ids = pd.read_csv(&#39;fred_ids.csv&#39;, index_col=None, sep=&#39;|&#39;) fred_ids . series_id description . 0 USPRIV | BLS private | . 1 NPPTTL | ADP | . 2 ICSA | Initial Claims | . 3 CCSA | Continued Claims | . 4 JTS1000JOL | Job Openings: Total Private | . 5 JTS1000HIL | Hires: Total Private | . 6 JTS1000TSL | Total Separations: Total Private | . 7 CSCICP03USM665S | OECD US Consumer Confidence | . 8 BSCICP03USM665S | OECD Business Confidence | . 9 UNRATE | Unemployment Rate | . 10 UNEMPLOY | Unemployment Level | . 11 CLF16OV | Civilian Labor Force | . 12 UEMP27OV | Unemployed for 27 Weeks | . 13 U6Rate | Unemployed plus Marginally Attached | . 14 CIVPART | Civilian Participation Rate | . 15 LNS12032194 | Part-Time for Econ Reasons | . Now we call the fredapi to get the observations for each series. I&#39;m only getting the data back to the start of 2007. I&#39;ll print out the number of observations we get of reach series and we will notice the different frequencies. . dfs = [] obs_start = &quot;2007-01-01&quot; obs_end = datetime.date.today() for ser in fred_ids.itertuples(): print(ser.series_id, end=&#39;, &#39;) try: all_df = Fred.get_series_all_releases(series_id=ser.series_id, observation_start=obs_start, observation_end=obs_end) tdf = all_df.sort_values(by=&quot;realtime_start&quot;, ascending=True).groupby(by=&quot;date&quot;).head(1) tdf[&#39;series_id&#39;] = ser.series_id tdf[&#39;date&#39;] = tdf[&#39;date&#39;].dt.date print(&quot;rows= {0}&quot;.format(tdf.shape[0])) dfs.append(tdf) except Exception as exc: print(exc) obs_df = pd.concat(dfs) obs_df = obs_df.merge(fred_ids, on=&#39;series_id&#39;) . USPRIV, rows= 165 NPPTTL, rows= 165 ICSA, rows= 720 CCSA, rows= 719 JTS1000JOL, rows= 164 JTS1000HIL, rows= 164 JTS1000TSL, rows= 164 CSCICP03USM665S, rows= 165 BSCICP03USM665S, rows= 165 UNRATE, rows= 165 UNEMPLOY, rows= 165 CLF16OV, rows= 165 UEMP27OV, rows= 165 U6Rate, rows= 165 CIVPART, rows= 165 LNS12032194, rows= 165 . Merge in the descriptions and have a look at the last few rows. . obs_df.sort_values(by=&quot;date&quot;, inplace=True) obs_df[&#39;dtime&#39;] = pd.to_datetime(obs_df[&#39;date&#39;]) print(obs_df.tail(4)) . realtime_start date value series_id description 1047 2020-10-08 2020-10-03 840000 ICSA Initial Claims 1048 2020-10-15 2020-10-10 898000 ICSA Initial Claims 1768 2020-10-22 2020-10-10 8.373e+06 CCSA Continued Claims 1049 2020-10-22 2020-10-17 787000 ICSA Initial Claims dtime 1047 2020-10-03 1048 2020-10-10 1768 2020-10-10 1049 2020-10-17 . Let&#39;s have a look at what the series look like over time. What a shock to the employment situation we&#39;ve had this year. Note that in many ways it dwarfs the impact from the 2008 financial crisis. Note how the business confidence index has bounced back. Not so much for consumer confidence. Our target is BLS private. Looks like the ADP data is tracking it really well and it comes out the week before. . def plot_observations(data_df, xcol, ycol, idcol, ncols, figwd, fight, ylim=(None, None), sharex=True): ids = list(data_df[idcol].unique()) figrows = int(np.ceil(len(ids)/float(figcols))) fig, axs = plt.subplots(nrows=figrows, ncols=figcols, figsize=[figwd, fight], sharex=sharex) gdf = data_df.groupby(by=&#39;description&#39;) for i, (key, group) in enumerate(gdf): row = i // figcols col = i % figcols ax = axs[row][col] group.plot(ax=ax, kind=&#39;line&#39;, x=xcol, y=ycol,linestyle=&#39;-&#39;, marker=&#39;o&#39;, lw=1, ylim=ylim, mec=&#39;red&#39;, mfc=&#39;black&#39;, ms=0.75, title=key, legend=None, label=None, grid=True) return fig . ylim=(None, None) fig = plot_observations(data_df=obs_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None)) . Let&#39;s focus in on data from the past year. . temp_df = obs_df[obs_df[&#39;date&#39;] &gt;= datetime.date.today() - datetime.timedelta(weeks=52)] ylim=(None, None) fig = plot_observations(data_df=temp_df, xcol=&#39;date&#39;, ycol=&#39;value&#39;, idcol=&#39;description&#39;, ncols=4, figwd=20, fight=10, ylim=(None,None)) . Still looks like the ADP series is close to the BLS but not spot on. . File &#34;&lt;ipython-input-1-443003e669d8&gt;&#34;, line 1 Still looks like the ADP series is close to the BLS but not spot on. ^ SyntaxError: invalid syntax .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/10/27/fred.html",
            "relUrl": "/jupyter/2020/10/27/fred.html",
            "date": " • Oct 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Changeofstrategy",
            "content": "{ “cells”: [ { “cell_type”: “markdown”, “metadata”: {}, “source”: [ “## A few Changes in the Plan n” ] }, { “cell_type”: “code”, “execution_count”: 2, “metadata”: { “ExecuteTime”: { “end_time”: “2020-10-25T16:08:52.519780Z”, “start_time”: “2020-10-25T16:08:52.513780Z” } }, “outputs”: [ { “name”: “stdout”, “output_type”: “stream”, “text”: [ “Python version= sys.version_info(major=3, minor=8, micro=5, releaselevel=’final’, serial=0) n”, “Pandas version= 1.1.3 n”, “2020-10-25 12:08:52.516780 n” ] } ], “source”: [ “import os n”, “import sys n”, “import datetime n”, “import time n”, “import pandas as pd n”, “print(&quot;Python version= {0}&quot;.format(sys.version_info)) n”, “print(&quot;Pandas version= {0}&quot;.format(pd.version)) n”, “print(datetime.datetime.now())” ] } ], “metadata”: { “kernelspec”: { “display_name”: “Python 3”, “language”: “python”, “name”: “python3” }, “language_info”: { “codemirror_mode”: { “name”: “ipython”, “version”: 3 }, “file_extension”: “.py”, “mimetype”: “text/x-python”, “name”: “python”, “nbconvert_exporter”: “python”, “pygments_lexer”: “ipython3”, “version”: “3.8.5” } }, “nbformat”: 4, “nbformat_minor”: 4 } .",
            "url": "https://jhmuller.github.io/job-forecasting/2020/10/25/ChangeOfStrategy.html",
            "relUrl": "/2020/10/25/ChangeOfStrategy.html",
            "date": " • Oct 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Jobs_part2",
            "content": "Data Sources . You can get the current value and some historical values from the BLS site, but I am going to use a different source, the St. Federal Reserve Economic Data site referred to as FRED . FRED is a great resource for all kinds of economic data and they have an API you can use to get the data. So I will get both the target variable, i.e. the private jobs number, as well as many of the predictor variables from FRED. So what to use for predictor variables. One obvious choice is past values of the private jobs number. Past values of the unemployment numbers might also be helpful. There is another publication from the BLS called the JOLTS report for Job Openings and Labor Turnover Survey. This should get you to the latest release. I will use 3 values from that report, job openings, hirings and separations. We have to be careful with this one since it is published with a longer lag than the main jobs report. For example, the JOLTS report for August comes out in October whereas the Employment Situation report for August comes out in September. We can get the JOLTS data from FRED. The BLS is part of the US Department of Labor. Another division of the Department of Labor produces weekly reports on initial and continued unemployment insurance claims. The claims data is published weekly so we will have to deal with data at different frequencies. And guess what … we can get those from FRED. . There is a payroll processing company called ADP that produces something similar to the jobs numbers based on their proprietary data, but they don’t have estimates for government jobs which is why I am only targeting the private number. More about the ADP estimates here. The ADP report comes out near the end of the month often just days before the BLS report. It is intended to be a predictor of the BLS values … so we will see. And you can get the ADP values from FRED. Note that most of the predictor variables I have listed so far are backward looking. They tell us about the sate of the world at some point in the past. It would be nice to have some forward looking indicators, like expectations. One source of that is the University of Michigan survey of consumer sentiment. The Michigan survey includes 3 questions that are forward looking, see the survey methodology document here. I think questions 2 through 4 look like they might be good to use so I will. Now we can get the aggregate value on Fred although with a one month lag. Anyway, I don’t think the components are on FRED so I will download them from the Michigan website here. . And now for something completely different, Google Trends Part of the reason my interest in this project was revived was that I found a paper titled In Search of a Job: Forecasting Employment Growth Using Google Trends about using Google Trends to predict the employment numbers and I wanted to try it out myself. The SSRN version of the paper is here . The basic idea is that the relative demand for search terms such as “jobs” might be an indicator of how many people are looking for work. But what other terms should we use other than “jobs”. The papers suggests using Google’s Keyword Planner to find other related terms. It should be fun. . Other Sources . I’m sure there are many other variables I could use. A researcher from the The St. Louis Fed maintains a special set of over 100 monthly economic variables mainly for researchers to use. Read about it here. I don’t want to use that since I believe it has the updated values of releases, not the original values. I want to use the values that first came out in case there were revisions. . The Conference Board is the source of some widely used economic data including the Leading Economic Indicators. I am not going to use those since I belive you have to subscribe to get them. . Getting the data . As I mentioned above, much of the traditional data will come from FRED except the Michigan survey data which I will download from the survey site. So how do we get data from Fred. . FRED data via Python . Google Trends .",
            "url": "https://jhmuller.github.io/job-forecasting/2020/10/15/jobs_part2.html",
            "relUrl": "/2020/10/15/jobs_part2.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Jobs_part1",
            "content": "Forecasting US jobs . Part 1: Introduction and Overview . This is the first of a series of posts in which I plan to talk about a real world prediction problem, predicting the change in one of the monthly government macro economic values. In particular, I will look at predicting the change in the number of people working for US private employers in a given month. The number of people working in the US is compiled by the U.S Bureau of Labor Statistics, BLS, and is published monthly on the first Friday of each month. Of interest is not so much the value, or level as it is often call, but the change in the value. The value published is an estimate of people employed in the prior month. For example, the value published on Friday 2020-10-02 gives an estimate of the number of people employed in September 2020. The number are published in a report currently called the Employment Situation. There are actually many numbers in the report the BLS publishes with two main types, unemployment and jobs. The unemployment numbers come from a survey of households and the jobs numbers come from a survey of businesses. For the jobs number there is an aggregate value, i.e. for the entire US, and then breakdowns into private versus government and then many further breakdowns by industry type and other things. Suffice it to say that there is a lot of data in the report but I am only going to focus on the number of private jobs added in a month. The reason to focus on private and not the sum of private and government is becuase of some other data I want to use and it only covers private jobs. Why do this? For me it&#39;s to practice data science with real data on an important problem. The jobs number is one of the measures of the health of the economy. I&#39;m writing this in October 2020 with a presidential election coming next month. The report released on October 2nd got a lot of attention in the press. Both the stock and bond markets pay attention to the releases too. Back in 2007 I worked in the investment group of a large bank and I recall that when the numbers came out, usually around 8:30 AM New York time, they were announced for all the traders to hear. As evidence of the potential impact on the markets see here, here, here, or better still just google it. You might ask WHy not just predict the S&amp;P 500 or something like that&lt;/em&gt; if we are interested in the markets. Well, we might get to that later. Some potential methods are described in the papers listed below. Generative Adversarial Network for Stock Market price Prediction | EMPIRICAL ASSET PRICING VIA MACHINE LEARNING | . I plan to use a variety of techniques including Time Series Regression, Ensemble methods such as Random Forests, and hopefully Neural Networks. I&#39;m interested not only in how the perform out of sample but also how hard it is to build the models and how interpretable the model is.",
            "url": "https://jhmuller.github.io/job-forecasting/2020/10/15/jobs_part1.html",
            "relUrl": "/2020/10/15/jobs_part1.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jhmuller.github.io/job-forecasting/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jhmuller.github.io/job-forecasting/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jhmuller.github.io/job-forecasting/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jhmuller.github.io/job-forecasting/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}