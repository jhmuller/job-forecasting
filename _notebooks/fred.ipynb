{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Predicting the Jobs number - Course correction\n",
    "\n",
    "- toc: True\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am making some changes in course due to new discoveries about the data and also refocusing effort.\n",
    "\n",
    "The first changes should be clear, I am going to start using Jupyter notebooks as the format for my posts.  The whole point is to show some of the methods and Jupyter notebooks provide a great way to do that.  Thanks to the folks at fastai for their tools and tutorials to help me get started with this.\n",
    "\n",
    "As with any project, I learn new things as I go along, especiall about the data.\n",
    "\n",
    "I had said in an earlier post that I would pull the U. of Michigan survey data from the U. Mich site because there was a delay of a month for the data to be available on FRED.  Well, it's not just FRED but overall.  I believe there is some arrnagement with Bloomberg to not publish the data elsewhere for a month.  I also believe that the overall consumer score can be scrapped from the U. Mich site, but not the results for the 5 questions that make up the overall score.  So, the forward looking question results *will* have a delay of 1 month.  Later I will give some notebook code to pull the data from the U. Michigan site automatically using Selenium with Python.  And I guess I will use the overall score and get that from the U. Michigan site.\n",
    "\n",
    "Another change is that I will add the OECD Consumer and Business confidence indexes to the predictor variables.  I found how to get them via FRED.  There are API's that should work directly with the OECD but if I can get it from FRED then that is much easier for me since I already know how to use one of the Python APIs for that.\n",
    "\n",
    "Speaking of data in general, I found this note from the SF Fed, https://www.frbsf.org/education/publications/doctor-econ/2013/october/labor-market-indicators-monetary-policy-unemployment-rate/\n",
    "Seems that I am using a lot of the data sources that the fed uses.  \n",
    "I am going to add a few of the series mentioned in that document.\n",
    "\n",
    "One other change not so much related to data but to priorities.  I will spend less time on developing a standard time series regression model.  I am more interested in what I can do with a machine learning ensemble method as well as with a neural net model.\n",
    "\n",
    "I still have to deal with data at different frequencies. A big shout out to Tom Stark, https://www.philadelphiafed.org/research-and-data/research-contacts/stark, at the Philadelphia Fed for sending me some advice on how to deal with this.  He says to use a Kalman filter. And Tom was nice enough to send me his notes on the subject, comprising of 4 PDF files and over 350 pages.  Very nice Tom, what a real gem you are.\n",
    "\n",
    "With that as an intro to Part 3, let's get started getting data from FRED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:30.532665Z",
     "start_time": "2020-11-23T16:10:28.681826Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import ggplot\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the versions for Python and non standard library modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:30.551630Z",
     "start_time": "2020-11-23T16:10:30.535629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 3.9.2 of Python\n",
      "version 1.2.3 of pd\n",
      "version 1.20.1 of np\n",
      "version 3.3.4 of mpl\n"
     ]
    }
   ],
   "source": [
    "mlist = list(filter(lambda x: inspect.ismodule(x[1]), locals().items()))\n",
    "vi = sys.version_info\n",
    "print(\"version {0}.{1}.{2} of Python\".format(vi.major, vi.minor, vi.micro))\n",
    "for name, mod in mlist:\n",
    "    if name.startswith(\"__\"):\n",
    "        continue\n",
    "    if hasattr(mod, \"__version__\"):\n",
    "        print(\"version {1} of {0}\".format(name, mod.__version__))\n",
    "del mod\n",
    "del name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from FRED using a Python API\n",
    "\n",
    "The FRED website itself has a pretty nice GUI for plotting data. But I need to download the data so I can use Python or R tools to do the forecasting.\n",
    "\n",
    "The FRED website gives links to projects you can use to access the FRED data from a variety of languages.\n",
    "See https://fred.stlouisfed.org/docs/api/fred/\n",
    "I have used the python one called fredapi, https://github.com/mortada/fredapi, before and will use it here with one modification.  \n",
    "I will add arguments to limit the series start and end dates.\n",
    "\n",
    "To use any of the API tools you will need an API Key.\n",
    "See the instructions here, https://fred.stlouisfed.org/docs/api/api_key.html, to get one and get started.\n",
    "I keep mine in a file called \"fred_api_key\".\n",
    "The fredapi module is in a sibling directory so I add that to my path before importing the module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:30.570627Z",
     "start_time": "2020-11-23T16:10:30.558631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.2\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../fredapi')\n",
    "from fredapi import fred\n",
    "import fredapi\n",
    "Fred = fred.Fred(api_key_file=\"fred_api.key\")\n",
    "print(fredapi.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to identify the data on FRED\n",
    "To acces the data for a series from FRED you need to know the series id.\n",
    "You can use the fredapi to search for a series given keywords. \n",
    "I already know the series ids I want to use and have them in a csv file, so let's read that in and see the ids and a brief description of the data for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:30.632628Z",
     "start_time": "2020-11-23T16:10:30.573628Z"
    }
   },
   "outputs": [],
   "source": [
    "fred_ids = pd.read_csv('fred_ids.csv', index_col=None, sep='|')\n",
    "fred_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the fredapi to get the observations for each series.  I'm only getting the data back to the start of 2007.  I'll print out the number of observations we get of reach series and we will notice the different frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T12:50:31.399149Z",
     "start_time": "2020-10-30T12:50:31.391148Z"
    }
   },
   "source": [
    "## FRED and ALFRED\n",
    "Have I not mentioned ALFRED yet?  The goverment releases, like Nonfarm payrolls, usually get updated in subsequent months,\n",
    "that is, the initial estimates might be revised up or down.  \n",
    "FRED just has the latest update.  \n",
    "You probably want to get the value as released or perhaps the value that was known on a given date.  \n",
    "You get that from ALFRED.  \n",
    "Notice in the call below I get *all_releases* and then manipulate the data to only keep the earliest one, or the original.  \n",
    "The API is the same for FRED and ALFRED, but if you get other than the most current data you will be getting it from ALFRED.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:49.365228Z",
     "start_time": "2020-11-23T16:10:30.634632Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "obs_start = \"2007-01-01\"\n",
    "obs_end = datetime.date.today()\n",
    "for ser in fred_ids.itertuples():\n",
    "    print(ser.series_id, end=', ')\n",
    "    try:\n",
    "        all_df = Fred.get_series_all_releases(series_id=ser.series_id, observation_start=obs_start, observation_end=obs_end)\n",
    "        tdf = all_df.sort_values(by=\"realtime_start\", ascending=True).groupby(by=\"date\").head(1)   \n",
    "        tdf['series_id'] = ser.series_id\n",
    "        tdf['date'] = tdf['date'].dt.date  \n",
    "        print(\"rows= {0}\".format(tdf.shape[0]))\n",
    "        dfs.append(tdf)\n",
    "    except Exception as exc:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "obs_df = pd.concat(dfs)\n",
    "obs_df = obs_df.merge(fred_ids, on='series_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge in the descriptions and have a look at the last few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:49.383271Z",
     "start_time": "2020-11-23T16:10:49.367227Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_df.sort_values(by=\"date\", inplace=True)\n",
    "obs_df['dtime'] = pd.to_datetime(obs_df['date'])  \n",
    "print(obs_df.tail(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not for HFT\n",
    "A quick aside about how quickly the data is available on FRED.  The Claims data was released at 8:30 ET on Thursday and I first saw it on FRED and in my download about 10-15 minutes later.  While not fast enough for high frequency trading, still very good.  Also let me say how helpful the folks at FRED have been.  I've emailed them a number of questions and they typically get back to me in a day or so. FRED and ALFRED are great resources.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of all the data\n",
    "Let's have a look at what the series look like over time.\n",
    "What a shock to the employment situation we've had this year.\n",
    "Note that in many ways it dwarfs the impact from the 2008 financial crisis.\n",
    "Note how the business confidence index has bounced back.  Not so much for consumer confidence.\n",
    "Our target is BLS private.  Looks like the ADP data is tracking it really well and it comes out the week before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:49.401274Z",
     "start_time": "2020-11-23T16:10:49.388225Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_observations(data_df, xcol, ycol, idcol, ncols, figwd, fight, ylim=(None, None), \n",
    "                      xtick_rot=0, sharex=True):\n",
    "    fig = None\n",
    "    ids = list(data_df[idcol].unique())\n",
    "    nrows = int(np.ceil(len(ids)/float(ncols)))\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=[figwd, fight], sharex=sharex)\n",
    "    gdf = data_df.groupby(by='description')\n",
    "    for i, (key, group) in enumerate(gdf):\n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        ax = axs[row][col]\n",
    "        group.plot(ax=ax, kind='line', x=xcol, y=ycol,linestyle='-', marker='o', lw=1, ylim=ylim,\n",
    "                   mec='red', mfc='black', ms=0.75,  title=key, legend=None, label=None, grid=True)\n",
    "        ax.xaxis.set_tick_params(rotation=xtick_rot)         \n",
    "    return fig  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T18:59:41.758824Z",
     "start_time": "2020-11-23T18:59:38.762826Z"
    }
   },
   "outputs": [],
   "source": [
    "ylim=(None, None)\n",
    "fig = plot_observations(data_df=obs_df, xcol='date', ycol='value', idcol='description', ncols=4,\n",
    "                  figwd=20, fight=10, ylim=(None,None))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the last year\n",
    "Let's focus in on data from 2020.  Notice how some of the series are almost current while others are lagging more than a month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:56.891166Z",
     "start_time": "2020-11-23T16:10:52.698166Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df = obs_df[obs_df['date'] >= datetime.date.today() - datetime.timedelta(weeks=52)]\n",
    "temp_df = obs_df[obs_df['date'] >= datetime.date(year=2020, month=1, day=1)]\n",
    "ylim=(None, None)\n",
    "fig = plot_observations(data_df=temp_df, xcol='date', ycol='value', idcol='description', ncols=4,\n",
    "                  figwd=20, fight=10, ylim=(None,None), xtick_rot=90) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    save the dataframe into the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:56.978198Z",
     "start_time": "2020-11-23T16:10:56.894166Z"
    }
   },
   "outputs": [],
   "source": [
    "out_dir = \"./data\"\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "obs_df.to_csv(os.path.join(out_dir, \"fred.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Fred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:57.077165Z",
     "start_time": "2020-11-23T16:10:56.980166Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"date\", \"value\"]\n",
    "series_ids = obs_df[\"series_id\"].unique()\n",
    "dfs = []\n",
    "tidy_df = pd.DataFrame()\n",
    "for sid in series_ids:\n",
    "    x = obs_df.loc[obs_df[\"series_id\"]==sid,cols]\n",
    "    x.columns = [\"date\", sid]\n",
    "    if tidy_df.shape[0] == 0:\n",
    "        tidy_df = x\n",
    "    else:\n",
    "        tidy_df = tidy_df.merge(x, how=\"outer\", on=\"date\")\n",
    "tidy_df.sort_values(by=\"date\", inplace=True)\n",
    "scols = [\"date\", \"USPRIV\", \"ICSA\", \"JTS1000JOL\"]\n",
    "print(tidy_df[scols].head(10))\n",
    "print(tidy_df[scols].tail(10))\n",
    "tidy_df.to_csv(\"./data/tidy_fred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for now on FRED data.\n",
    "Next up, U. Michigan data and Google Trends data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T16:10:57.083174Z",
     "start_time": "2020-11-23T16:10:57.079165Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "payems",
   "language": "python",
   "name": "payems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
